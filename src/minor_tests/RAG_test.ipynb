{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to evaluate a RAG application\n",
    "\n",
    "This example uses [Langchain](https://www.langchain.com) and [Giskard](https://github.com/Giskard-AI/giskard) to evaluate the quality of a RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the Website and Split the Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 0}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220720964\\nIntelligent scissors for image composition\\nConference Paper · January 1995\\nDOI: 10.1145/218380.218442\\xa0·\\xa0Source: DBLP\\nCITATIONS\\n812\\nREADS\\n2,545\\n2 authors, including:\\nEric N. Mortensen\\nLucidyne Technologies, Inc.\\n34 PUBLICATIONS\\xa0\\xa0\\xa03,134 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Eric N. Mortensen on 01 June 2014.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='Abstract\\nWe present a new, interactive tool called Intelligent Scissors\\nwhich we use for image segmentation and composition.  Fully auto-\\nmated segmentation is an unsolved problem, while manual tracing\\nis inaccurate and laboriously unacceptable.  However, Intelligent\\nScissors allow objects within digital images to be extracted quickly\\nand accurately using simple gesture motions with a mouse.  When\\nthe gestured mouse position comes in proximity to an object edge,\\nalive-wire boundary  “snaps” to, and wraps around the object of\\ninterest.\\nLive-wire boundary detection formulates discrete dynamic pro-\\ngramming (DP) as a two-dimensional graph searching problem.  DP\\nprovides mathematically optimal boundaries while greatly reducing\\nsensitivity to local noise or other intervening structures.  Robust-\\nness is further enhanced withon-the-ﬂy training  which causes the\\nboundary to adhere to the speciﬁc type of edge currently being fol-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='lowed, rather than simply the strongest edge in the neighborhood.\\nBoundary cooling automatically freezes unchanging segments and\\nautomates input of additional seed points. Cooling also allows the\\nuser to be much more free with the gesture path, thereby increasing\\nthe efﬁciency and ﬁnesse with which boundaries can be extracted.\\nExtracted objects can be scaled, rotated, and composited using\\nlive-wire masks and spatial frequency equivalencing .  Frequency\\nequivalencing is performed by applying a Butterworth ﬁlter which\\nmatches the lowest frequency spectra to all other image compo-\\nnents.  Intelligent Scissors allow creation of convincing composi-\\ntions from existing images while dramatically increasing the speed\\nand precision with which objects can be extracted.\\n1. Introduction\\nDigital image composition has recently received much attention\\nfor special effects in movies and in a variety of desktop applica-\\ntions.  In movies, image composition, combined with other digital'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='manipulation techniques, has also been used to realistically blend\\nold ﬁlm into a new script.  The goal of image composition is to com-\\nbine objects or regions from various still photographs or movie\\nframes to create a seamless, believable, image or image sequence\\nwhich appears convincing and real.  Fig. 9(d) shows a believable\\ncomposition created by combining objects extracted from three\\nimages, Fig. 9(a-c).  These objects were digitally extracted and\\ncombined in a few minutes using a new, interactive tool calledIntel-\\nligent Scissors .\\nWhen using existing images, objects of interest must be extracted\\nand segmented from a surrounding background of unpredictable\\ncomplexity.  Manual segmentation is tedious and time consuming,\\nlacking in precision, and impractical when applied to long image\\nsequences. Further, due to the wide variety of image types and con-\\ntent, most current computer based segmentation techniques are\\nslow, inaccurate, and require signiﬁcant user input to initialize or'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='control the segmentation process.\\nThis paper describes a new, interactive, digital image segmenta-\\ntion tool called “Intelligent Scissors” which allows rapid object\\nextraction from arbitrarily complex backgrounds. Intelligent Scis-\\nsors boundary detection formulates discrete dynamic programming\\n(DP) as a two-dimensional graph searching problem.  Presented as\\npart of this tool areboundary cooling  and on-the-ﬂy training , which\\nreduce user input and dynamically adapt the tool to speciﬁc types of\\nedges.  Finally, we presentlive-wire masking  and spatial frequency\\nequivalencing  for convincing image compositions.\\n2. Background\\nDigital image segmentation techniques are used to extract image\\ncomponents from their surrounding natural background.  However,\\ncurrently available computer based segmentation tools are typically\\nprimitive and often offer little more advantage than manual tracing.\\nRegion based magic wands, provided in many desktop applica-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='tions, use an interactively selected seed point to “grow” a region by\\nadding adjacent neighboring pixels.  Since this type of region grow-\\ning does not provide interactive visual feedback, resulting region\\nboundaries must usually be edited or modiﬁed.\\nOther popular boundary deﬁnition methods use active contours\\nor snakes[1, 5, 8, 15] to improve a manually entered rough approx-\\nimation.  After being initialized with a rough boundary approxima-\\ntion, snakes iteratively adjust the boundary points in parallel in an\\nattempt to minimize an energy functional and achieve an optimal\\nboundary.  The energy functional is a combination of internal\\nforces, such as boundary curvature, and external forces, like image\\ngradient magnitude.  Snakes can track frame-to-frame boundary\\nmotion provided the boundary hasn’t moved drastically.  However,\\nactive contours follow a pattern of initialization followed by energy\\nminimization; as a result, the user does not know what the ﬁnal'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='boundary will look like when the rough approximation is input. If\\nthe resulting boundary is not satisfactory, the process must be\\nrepeated or the boundary must be manually edited.  W e provide a\\ndetailed comparison of snakes and Intelligent Scissors in section\\n3.6.\\nAnother class of image segmentation techniques use a graph\\nsearching formulation of DP (or similar concepts) to ﬁnd globally\\noptimal boundaries [2, 4, 10, 11, 14].  These techniques differ from\\nsnakes in that boundary points are generated in a stage-wise optimal\\ncost fashion whereas snakes iteratively minimize an energy func-\\ntional for all points on a contour in parallel (giving the appearance\\nof wiggling).  However, like snakes, these graph searching tech-\\nniques typically require a boundary template--in the form of a man-\\nually entered rough approximation, a ﬁgure of merit, etc.--which is\\nused to impose directional sampling and/or searching constraints.\\nThis limits these techniques to  a boundary search with one degree'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='of freedom within a window about the two-dimensional boundary\\ntemplate.  Thus, boundary extraction using previous graph search-\\ning techniques is non-interactive (beyond template speciﬁcation),\\nlosing the beneﬁts of further human guidance and expertise.\\nIntelligent Scissors for Image Composition\\nEric N. Mortensen 1       William A. Barrett 2\\nBrigham Y oung University\\n1 enm @ cs.byu.edu, Dept. of Comp. Sci., BYU, Provo, UT  84602 (801)378-7605\\n2 barrett @ cs.byu.edu, Dept. of Comp. Sci., BYU, Provo, UT  84602 (801)378-7430\\nPermission to make digital/hard copy of part or all of this work \\nfor personal or classroom use is granted without fee provided  \\nthat copies are not made or distributed for profit or commercial  \\nadvantage, the copyright notice, the title of the publication and \\nits date appear, and notice is given that copying is by permission \\nof ACM, Inc.  To copy otherwise, to republish, to post on \\nservers, or to redistribute to lists, requires prior specific'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='permission and/or a fee. \\n©1995  ACM-0-89791-701-4/95/008\\x85$3.50'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content='The most important difference between previous boundary ﬁnd-\\ning techniques and Intelligent Scissors presented here lies not in the\\nboundary deﬁning criteria per se´, but in themethod  of interaction.\\nNamely, previous methods exhibit a pattern of boundary approxi-\\nmation followed by boundary reﬁnement, whereas Intelligent Scis-\\nsors  allow the user tointeractively select  the most suitable\\nboundary from a set of all  optimal boundaries emanating from a\\nseed point.  In addition, previous approaches do not incorporate on-\\nthe-ﬂy training or cooling, and are not as computationally efﬁcient.\\nFinally, it appears that the problem of automated matching of spa-\\ntial frequencies for digital image composition has not been\\naddressed previously.\\n3. Intelligent Scissors\\nBoundary deﬁnition via dynamic programming can be formu-\\nlated as a graph searching problem [10] where the goal is to ﬁnd the\\noptimal path between a start node and a set of goal nodes.  As'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content='applied to image boundary ﬁnding, the graph search consists of\\nﬁnding the globally optimal path from a start pixel to a goal pixel--\\nin particular, pixels represent nodes and edges are created between\\neach pixel and its 8 neighbors.  For this paper, optimality is deﬁned\\nas the minimum cumulative cost path from a start pixel to a goal\\npixel where the cumulative cost of a path is the sum of the local\\nedge (or link) costs on the path.\\n3.1. Local Costs\\nSince a minimum cost path should correspond to an image com-\\nponent boundary, pixels (or more accurately, links between neigh-\\nboring pixels) that exhibit strong edge features should have low\\nlocal costs and vice-versa.  Thus, local component costs are created\\nfrom the various edge features:\\nThe local costs are computed as a weighted sum of these component\\nfunctionals.  Letting l( p ,q ) represents the local cost on the directed\\nlink from pixel p  to a neighboring pixel q , the local cost function is\\n(1)'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content='(1)\\nwhere each ω  is the weight of the corresponding feature function.\\n(Empirically, weights of ω Z  = 0.43, ω D  = 0.43, and ω G  = 0.14 seem\\nto work well in a wide range of images.)\\nThe laplacian zero-crossing is a binary edge feature used for edge\\nlocalization [7, 9]. Convolution of an image with a laplacian kernel\\napproximates the 2nd  partial derivative of the image.  The laplacian\\nimage zero-crossing corresponds to points of maximal (or minimal)\\ngradient magnitude.  Thus, laplacian zero-crossings represent\\n“good” edge properties and should therefore have a low local cost.\\nIfI L ( q ) is the laplacian of an image I  at pixel q , then\\n(2)\\nHowever, application of a discrete laplacian kernel to a digital\\nimage produces very few zero-valued pixels.  Rather, a zero-cross-\\ning is represented by two neighboring pixels that change from pos-\\nitive to negative.  Of the two pixels, the one closest to zero is used\\nto represent the zero-crossing.  The resulting feature cost contains'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content='single-pixel wide cost “canyons” used for boundary localization.\\nImage Feature Formulation\\nLaplacian Zero-Crossing fZ\\nGradient Magnitude fG\\nGradient Direction fD\\nl pq ,()ω Z f Z q()⋅ω D f D pq ,()⋅ω G f G q()⋅++=\\nf Z q()\\n0 ;\\x11\\x11\\x11if\\x11 I L q() 0=\\n1 ;\\x11\\x11\\x11if\\x11 I L q() 0≠{=\\nSince the laplacian zero-crossing creates a binary feature, fZ ( q )\\ndoes not distinguish between strong, high gradient edges and weak,\\nlow gradient edges.  However, gradient magnitude provides a direct\\ncorrelation between edge strength and local cost.  IfI x  and I y  repre-\\nsent the partials of an image I  in x  and y respectively, then the gra-\\ndient magnitude G  is approximated with\\n.\\nThe gradient is scaled and inverted so high gradients produce low\\ncosts and vice-versa.  Thus, the gradient component function is\\n(3)\\ngiving an inverse linear ramp function.  Finally, gradient magnitude\\ncosts are scaled by Euclidean distance.  To keep the resulting max-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content='imum gradient at unity,fG ( q ) is scaled by 1 if q  is a diagonal neigh-\\nbor to p  and by 1/ √ 2 if q  is a horizontal or vertical neighbor.\\nThe gradient direction adds a smoothness constraint to the\\nboundary by associating a high cost for sharp changes in boundary\\ndirection.  The gradient direction is the unit vector deﬁned byI x  and\\nI y .  Letting D ( p ) be the unit vector perpendicular (rotated 90 degrees\\nclockwise) to the gradient direction at point p  (i.e., for D ( p ) = ( I y ( p ),\\n-I x ( p ))), the formulation of the gradient direction feature cost is\\n(4)\\nwhere\\nare vector dot products and\\n(5)\\nis the bidirectional link or edge vector between pixels p  and q .\\nLinks are either horizontal, vertical, or diagonal (relative to the\\nposition ofq  in p ’s neighborhood) and point such that the dot prod-\\nuct of D ( p ) and L ( p , q ) is positive, as noted in (5).  The neighbor-\\nhood link direction associates a high cost to an edge or link between'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content='two pixels that have similar gradient directions but are perpendicu-\\nlar, or near perpendicular, to the link between them.  Therefore, the\\ndirection feature cost is low when the gradient direction of the two\\npixels are similar to each other and the link between them.\\n3.2. Two-Dimensional Dynamic Programming\\nAs mentioned, dynamic programming can be formulated as a\\ndirected graph search for an optimal path.  This paper utilizes an\\noptimal graph search similar to that presented by Dijkstra [6] and\\nextended by Nilsson [13]; further, this technique builds on and\\nextends previous boundary tracking methods in 4 important ways:\\n1. It imposes no directional sampling or searching constraints.\\n2. It utilizes a new set of edge features and costs: laplacian\\nzero-crossing, multiple gradient kernels.\\n3. The active list is sorted with an O( N ) sort for N  nodes/pixels.\\n4. No a priori goal nodes/pixels are speciﬁed.\\nFirst, formulation of boundary ﬁnding as a 2-D graph search elimi-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 2}, page_content=\"nates the directed sampling and searching restrictions of previous\\nimplementations, thereby allowing boundaries of arbitrary com-\\nGI x\\n2 I y\\n2+=\\nf G\\nma x G() G−\\nma x G()\\n1\\nG\\nma x G()−==\\nf D pq ,()\\n1\\nπ\\nd p pq ,()[]cos 1− d q pq ,()[]cos 1−+{}=\\nd p pq ,() D ' p() Lp q ,()⋅=\\nd q pq ,() Lp q ,() D ' q()⋅=\\nLp q ,()\\nqp \\x11\\x11\\x11if\\x11 D ' p() qp −() 0≥⋅;−\\npq \\x11\\x11\\x11if\\x11 D ' p() qp −() 0<⋅;−{=\"),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 3}, page_content='plexity to be extracted.  Second, the edge features used here are\\nmore robust and comprehensive than previous implementations: we\\nmaximize over different gradient kernels sizes to encompass the\\nvarious edge types and scales while simultaneously attempting to\\nbalance edge detail with noise suppression [7], and we use the lapla-\\ncian zero-crossing for boundary localization and ﬁne detail live-\\nwire “snapping”.  Third, the discrete, bounded nature of the local\\nedge costs permit the use of a specialized sorting algorithm that\\ninserts points into a sorted list (called the active list) in constant\\ntime.  Fourth, the live-wire tool is free to deﬁne a goal pixel inter-\\nactively, at any “free” point in the image, after minimum cost paths\\nare computed toall  pixels.  The latter happens fast enough that the\\nfree point almost always falls within an expanding cost wavefront\\nand interactivity is not impeded.\\nThe Live-Wire 2-D dynamic programming (DP) graph search\\nalgorithm is as follows:'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 3}, page_content='Algorithm:  Live-Wire 2-D DP graph search.\\nInput:\\ns {Start (or seed) pixel.}\\nl( q , r ) {Local cost function for link between pixels q and r.}\\nData Structures:\\nL {List of active pixels sorted by total cost (initially empty).}\\nN( q ) {Neighborhood set of q (contains 8 neighbors of pixel).}\\ne( q ) {Boolean function indicating if q has been expanded/processed.}\\ng( q ) {Total cost function from seed point to q.}\\nOutput:\\np {Pointers from each pixel indicating the minimum cost path.}\\nAlgorithm:\\ng( s ) ← 0;  L ← s ; {Initialize active list with zero cost seed pixel.}\\nwhile  L ≠∅ do begin {While still points to expand:}\\nq ← min(L); {Remove minimum cost pixel q from active list.}\\ne( q ) ← TRUE; {Mark q as expanded (i.e., processed).}\\nfor each r ∈ N( q ) such that  not e( r ) do begin\\ng tmp ← g( q )+l( q , r ); {Compute total cost to neighbor.}\\nif r ∈ L and  g tmp <g( r ) then {Remove higher cost neighbor’s }\\nr ← L; {  from list.}\\nif r ∉ L then begin {If neighbor not on list, }'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 3}, page_content='g( r ) ← g tmp ; {  assign neighbor’s total cost, }\\np( r ) ← q ; {  set (or reset) back pointer, }\\nL ← r ; {  and place on (or return to) }\\nend {  active list.}\\nend\\nend\\nNotice that since the active list is sorted, when a new , lower cumu-\\nlative cost is computed for a pixel already on the list then that point\\nmust be removed from the list in order to be added back to the list\\nwith the new lower cost.  Similar to adding a point to the sorted list,\\nthis operation is also performed in constant time.\\nFigure 1 demonstrates the use of the 2-D DP graph search algo-\\nrithm to create a minimum cumulative cost path map (with corre-\\nsponding optimal path pointers).  Figure 1(a) is the initial local cost\\nmap with the seed point circled.  For simplicity of demonstration\\nthe local costs in this example are pixel based rather than link based\\nand can be thought of as representing the gradient magnitude cost\\nfeature.  Figure 1(b) shows a portion of the cumulative cost and'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 3}, page_content='pointer map after expanding the seed point (with a cumulative cost\\nof zero).  Notice how the diagonal local costs have been scaled by\\nEuclidean distance (consistent with the gradient magnitude cost\\nfeature described previously).  Though complicating the example,\\nweighing by Euclidean distance is necessary to demonstrate that the\\ncumulative costs to points currently on the active list can change if\\neven lower cumulative costs are computed from as yet unexpanded\\nneighbors.  This is demonstrated in Figure 1(c) where two points\\nhave now been expanded--the seed point and the next lowest cumu-\\nlative cost point on the active list.  Notice how the points diagonal\\nto the seed point have changed cumulative cost and direction point-\\ners.  The Euclidean weighting between the seed and diagonal points\\nmakes them more costly than non-diagonal paths.  Figures 1(d),\\n1(e), and 1(f) show the cumulative cost/direction pointer map at\\nvarious stages of completion.  Note how the algorithm produces a'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 3}, page_content='“wavefront” of active points emanating from the initial start point,\\ncalled the seed point, and that the wavefront grows out faster where\\nthere are lower costs.\\n3.3. Interactive “Live-Wire” Segmentation T ool\\nOnce the optimal path pointers are generated, a desired boundary\\nsegment can be chosen dynamically via a “free” point.  Interactive\\nmovement of the free point by the mouse cursor causes the bound-\\nary to behave like a live-wire as it adapts to the new minimum cost\\npath by following the optimal path pointers from the free point back\\n45 41 35 31 29 35 33 34 36 40 50\\n38 29 23 22 24 29 37 38 42 39 43\\n28 18 16 21 28 37 46 49 47 40 35\\n18 12 16 27 38 53 59 53 39 33 31\\n1 4 8 1 32 02 93 54 95 43 52 83 2\\n14 6 6 12 14 22 28 35 27 25 31\\n1 8 72959 1 4 2 1 1 8 2 3 3 2\\n1 6 4016 1 2 1 3 1 5 1 9 2 7 3 9\\n18 13 7 6 14 17 18 17 24 30 45\\n1 1 1 3 1 2 9583124 1 0\\n1 4 1 1 742584638\\n1 1 63579 1 2 1 1 1 0 74\\n7 4 6 11 13 18 17 14 8 5 2\\n6 2 7 10 15 15 21 19 8 3 5\\n83479 1 3 1 4 1 5 956\\n1 1 5283457259'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 3}, page_content='1 1 5283457259\\n1 2 421563248 1 2\\n1 0 975985378 1 5\\n(a)\\n(e)\\n(f)\\nFigure 1:  (a) Initial local cost matrix.  (b) Seed point (shaded)\\nexpanded.  (c) 2 points (shaded) expanded.  (d) 5 points (shaded)\\nexpanded.  (e) 47 points expanded.  (f) Finished total cost and path\\nmatrix with two of many paths (free points shaded) indicated.\\n(c) (d)\\n41 35 31 29 35\\n38 29 23 22 24 29\\n28 18 16 21 28 37\\n18 12 16 27 38\\n1 4 8 1 32 02 93 55 2 3 52 83 2\\n14 6 6 12 14 22 28 35 27 25 31\\n1 8 72959 1 4 2 1 1 8 2 3 3 2\\n1 6 4016 1 2 1 3 1 5 1 9 2 7 4 0\\n18 13 7 6 14 17 18 17 24 30\\n(b)\\n7295\\n4016\\n13 7 6 14\\n72 1 1\\n401\\n13 7 7\\n6 6 12 14 23\\n2 0 72959\\n1 6 4016 1 3\\n18 13 7 6 14'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 4}, page_content='to the seed point.  By constraining the seed point and free points to\\nlie near a given edge, the user is able to interactively “snap” and\\n“wrap” the live-wire boundary around the object of interest.  Figure\\n2 demonstrates how a live-wire boundary segment adapts to\\nchanges in the free point (cursor position) by latching onto more\\nand more of an object boundary.  Speciﬁcally, note the live-wire\\nsegments corresponding to user-speciﬁed free point positions at\\ntimest0 , t1 , and t2 .  Although Fig. 2 only shows live-wire segments\\nfor three discrete time instances, live-wire segments are actually\\nupdated dynamically and interactively (on-the-ﬂy) with each move-\\nment of the free point.\\nWhen movement of the free point causes the boundary to digress\\nfrom the desired object edge, interactive input of a new seed point\\nprior to the point of departure reinitiates the 2-D DP boundary\\ndetection.  This causes potential paths to be recomputed from the'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 4}, page_content='new seed point while effectively “tieing off” the boundary com-\\nputed up to the new seed point.\\nNote again that optimal paths are computed from the seed point\\nto all  points in the image (since the 2-D DP graph search produces\\na minimum cost spanning tree of the image [6]).  Thus, by selecting\\na free point with the mouse cursor, the interactive live-wire tool is\\nsimply selecting an optimal boundary segment from a large collec-\\ntion of optimal paths.\\nSince each pixel (or free point) deﬁnes only one optimal path to\\na seed point, a minimum of two seed points must be placed to\\nensure a closed object boundary.  The path map from the ﬁrst seed\\npoint of every object is maintained during the course of an object’ s\\nboundary deﬁnition to provide a closing boundary path from the\\nfree point.  The closing boundary segment from the free point to the\\nﬁrst seed point expedites boundary closure.\\nPlacing seed points directly on an object’s edge is often difﬁcult'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 4}, page_content='and tedious.  If a seed point is not localized to an object edge then\\nspikes results on the segmented boundary at those seed points (since\\nFigure 2:  Image demonstrating how the live-wire segment adapts and\\nsnaps to an object boundary as the free point moves (via cursor move-\\nment).  The path of the free point is shown in white.  Live-wire segments\\nfrom previous free point positions (t0 , t 1 , and t 2 ) are shown in green.\\n(a) (b)\\nFigure 3:  Comparison of live-wire without (a) and with (b) cooling.\\nWithot cooling (a), all seed points must be placed manually on the\\nobject edge.  With cooling (b), seed points are generated automatically\\nas the live-wire segment freezes.\\nthe boundary is forced to pass through the seed points).  T o facilitate\\nseed point placement, a cursor snap is available which forces the\\nmouse pointer to the maximum gradient magnitude pixel within a\\nuser speciﬁed neighborhood.  The neighborhood can be anywhere'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 4}, page_content='from 1× 1 (resulting in no cursor snap) to 15 × 15 (where the cursor\\ncan snap as much as 7 pixels in both x  and y ).  Thus, as the mouse\\ncursor is moved by the user, it snaps or jumps to a neighborhood\\npixel representing a “good” static edge point.\\n3.4. Path Cooling\\nGenerating closed boundaries around objects of interest can\\nrequire as few as two seed points (for reasons given previously).\\nSimple objects typically require two to ﬁve seed points but complex\\nobjects may require many more.  Even with cursor snap, manual\\nplacement of seed points can be tedious and often requires a large\\nportion of the overall boundary deﬁnition time.\\n(a)\\n(b)\\nFigure 4:  Comparison of live-wire (a) without and (b) with dynamic\\ntraining.  (a) Without training, the live-wire segment snaps to nearby\\nstrong edges.  (b) With training, it favors edges with similar characteris-\\ntics as those just learned. (c) The static gradient magnitude cost map'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 4}, page_content='shows that without training, high gradients are favored since they map\\nto low costs.  However , with training, the dynamic cost map (d) favors\\ngradients similar to those sampled from the previous boundary segment.\\nM G\\nn G\\n0\\n0 Cost\\nGradient Magnitude\\nM G\\nn G\\n0\\n0 Cost\\nGradient Magnitude\\nStatic Cost Map Dynamic Cost Map\\n(c) (d)'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='Automatic seed point generation relieves the user from precise\\nmanual placement of seed points by automatically selecting a pixel\\non the current active boundary segment to be a new seed point.\\nSelection is based on “path cooling” which in turn relies on path\\ncoalescence. Though a single minimum cost path exists from each\\npixel to a given seed point, many paths “coalesce” and share por-\\ntions of their optimal path with paths from other pixels.  Due to\\nBellman’s Principle of Optimality [3], if any two optimal paths\\nfrom two distinct pixels share a common point or pixel, then the two\\npaths are identical from that pixel back to the seed point. This is par-\\nticularly noticeable if the seed point is placed near an object edge\\nand the free point is moved away from the seed point but remains\\nin the vicinity of the object edge.  Though a new optimal path is\\nselected and displayed every time the mouse cursor moves, the\\npaths are typically identical near the seed point and object edges'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='and only change local to the free point.  As the free point moves far-\\nther and farther away from the seed point, the portion of the active\\nlive-wire boundary segment that does not change becomes longer.\\nNew seed points are generated at the end of a stable segment (i.e.,\\nthat has not changed recently).  Stability is measured by time (in\\nmilliseconds) on the active boundary and path coalescence (number\\nof times the path has been redrawn from distinct free points).\\nThis measure of stability provides the live-wire segment with a\\nsense of “cooling”.  The longer a pixel is on a stable section of the\\nlive-wire boundary, the cooler it becomes until it eventually freezes\\nand automatically produces a new seed point.\\nFigure 3 illustrates the beneﬁt of path cooling.  In Fig. 3(a), the\\nuser must place each seed point manually on the object boundary .\\nHowever, with cooling (Fig. 3(b)), only the ﬁrst seed point (and last\\nfree point) need to be speciﬁed manually; the other seed points were'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='generated automatically via cooling.\\n3.5. Interactive Dynamic Training\\nOn occasion, a section of the desired object boundary may have\\na weak gradient magnitude relative to a nearby strong gradient\\nedge.  Since the nearby strong edge has a relatively lower cost, the\\nlive-wire segment snaps to the strong edge rather than the desired\\nweaker edge.  This can be seen in Fig. 4(a).  The desired boundary\\nis the woman’s (Harriet’s) cheek.  However, since part of it is so\\nclose to the high contrast shoulder of the man (Ozzie), the live-wire\\nsnaps to the shoulder.\\nTraining allows dynamic adaptation of the cost function based on\\na sample boundary segment.  Training exploits an object’s bound-\\nary segment that is already considered to be good and is performed\\ndynamically as part of the boundary segmentation process.  As a\\nresult, trained features are updated interactively as an object bound-\\nary is being deﬁned.  On-the-ﬂy training eliminates the need for a'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content=\"separate training phase and allows the trained feature cost functions\\nto adaptwithin  the object being segmented as well as between\\nobjects in the image.  Fig. 4(b) demonstrates how a trained live-wire\\nsegment latches onto the edge that is similar to the previous training\\nsegment rather that the nearby stronger edge.\\nTo facilitate training and trained cost computation, a gradient\\nmagnitude feature map or image is precomputed by scaling the min-\\nimized gradient magnitude image,G ', into an integer range of size\\nn G  (i.e., from 0 to n G  - 1).  The actual feature cost is determined by\\nmapping these feature values through a look-up table which con-\\ntains the scaled (weighted) cost for each value.  Fig 4(c) illustrates\\nedge cost based on gradient magnitude without training.  Note that\\nwith training (Fig. 4(d)) edge cost plummets for gradients that are\\nspeciﬁc to the object of interest’s edges.\\nSelection of a “good” boundary segment for training is made\"),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='interactively using the live-wire tool.  To allow training to adapt to\\nslow (or smooth) changes in edge characteristics, the trained gradi-\\nent magnitude cost function is based only on the most recent or\\nclosest portion of the current deﬁned object boundary.  A training\\nlength, t, speciﬁes how many of the most recent boundary pixels are\\nused to generate the training statistics.  A monotonically decreasing\\nweight function (either linearly or Gaussian based) determines the\\ncontribution from each of the closestt pixels.  This permits adaptive\\ntraining with local dependence to prevent trained feature from\\nbeing too subject to old edge characteristics.  The closest pixel (i.e.,\\nthe current active boundary segment endpoint) gets a weight of 1\\nand the point that ist pixels away, along the boundary from the cur-\\nrent active endpoint, gets a minimal weight (which can be deter-\\nmined by the user).  The training algorithm samples the'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='precomputed feature maps along the closestt pixels of the edge seg-\\nment and increments the feature histogram element by the corre-\\nsponding pixel weight to generate a histogram for each feature\\ninvolved in training.\\nAfter sampling and smoothing, each feature histogram is then\\nscaled and inverted (by subtracting the scaled histogram values\\nfrom its maximum value) to create the feature cost map needed to\\nconvert feature values to trained cost functions.\\nSince training is based on learned edge characteristics from the\\nmost recent portion of an object’s boundary, training is most effec-\\ntive for those objects with edge properties that are relatively consis-\\ntent along the object boundary (or, if changing, at least change\\nsmoothly enough for the training algorithm to adapt).  In fact, train-\\ning can be counter-productive for objects with sudden and/or dra-\\nmatic changes in edge features.  However, training can be turned on\\nand off interactively throughout the deﬁnition of an object bound-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='ary so that it can be used (if needed) in a section of the boundary\\nwith similar edge characteristics and then turned off before a drastic\\nchange occurs.\\n3.6 Comparison with Snakes\\nDue to the recent popularity of snakes and other active contours\\nmodels and since the interactive boundary wrapping of the live-\\nwire may seem similar to the “wiggling” of snakes, we highlight\\nwhat we feel are the similarities and their corresponding differences\\nbetween snakes and Intelligent Scissors.\\nSimilarities (compare with corresponding dif ferences below):\\n1. The gradient magnitude cost in Intelligent Scissors is similar to\\nthe edge energy functional used in snakes.\\n2. Both methods employ a smoothing term to minimize the effects\\nof noise in the boundary.\\n3. Snakes and live-wire boundaries are both attracted towards\\nstrong edge features.\\n4. Both techniques attempt to ﬁnd globally optimal boundaries to\\ntry to overcome the effects of noise and edge dropout.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 5}, page_content='5. Snakes and Intelligent Scissors both require interaction as part of\\nthe boundary segmentation process.\\nDif ferences (compare with corresponding similarities above):\\n1. The laplacian zero-crossing binary cost feature seems to have not\\nbeen used previously in active contours models 1  (or DP bound-\\nary tracking methods for that matter).\\n2. The active contour smoothing term is internal (i.e., based on the\\ncontour’s point positions) whereas the smoothing term for live-\\nwire boundaries is computed from external image gradient direc-\\ntions2(next page) .\\n1. Kass et al. [8] did use a squared laplacian energy functional to show the rela-\\ntionship of scale-space continuation to the Marr-Hildreth edge detection theory.  How-\\never, the squared laplacian does not represent a binary condition, nor could it since the\\nvariational calculus minimization used in [8] required that all functionals be differen-\\ntiable.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='3. Snakes are typically attracted to edge features only within the\\ngravity of an edge’s gradient energy valley whereas the live-wire\\nboundary can snap to strong edge features from arbitrary dis-\\ntances (since the 2-D DP’s search window is the entire image).\\n4. Snakes are globally optimal over the entire contour whereas live-\\nwire boundaries are piece-wise optimal (i.e., optimal between\\nseed points).  We feel this creates a desirable balance between\\nglobal optimality and local control.  This piece-wise optimality\\nalso allows for path cooling and intra-object on-the-ﬂy training.\\n5. Finally, snakes reﬁne (and interactively “nudge” by placing\\nsprings, etc.) a single rough boundary approximation where the\\nlive-wire tool interactivelyselects  an optimal boundary segment\\nfrom potentially all  possible minimum cost paths.\\nInteractive optimal 2-D path selection is what makes Intelligent\\nScissors work and is the key difference between Intelligent Scissors'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='and all previous techniques.  Snakes are interactively initialized\\nwith an approximate boundary contour (often requiring several\\nmanually placed points); this single contour is then iteratively\\nadjusted in an attempt to minimize an energy functional.  The live-\\nwire tool, on the other hand, is interactively initialized with just a\\nsingle seed point and it then generates, at interactive speeds, all pos-\\nsible optimal paths from the seed point toevery  other point in the\\nimage, thus, allowing the user to interactively select the desired\\noptimal boundary segment.  As a result, Intelligent Scissors typi-\\ncally require less time and effort to segment an object than it takes\\nto manually input an initial approximation to the object boundary .\\nActually, the live-wire tool is much more similar to previous\\nstage-wise optimal boundary tracking approaches than it is to\\nsnakes, since Intelligent Scissors were developed as an interactive'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='2-D extension to previous optimal edge tracking methods rather\\nthan an improvement on active contours.\\n4. Image Composition with Intelligent Scissors\\nAs mentioned, composition artists need an intelligent, interactive\\ntool to facilitate image component boundary deﬁnition.  Since Intel-\\nligent Scissors can quickly segment object from an image, it serves\\nas a tool for cut and paste operations.  After object boundaries have\\nbeen extracted, object can be copied from the image and placed into\\na buffer (i.e., clipboard) for future processing and placement into\\nanother image, or the same image if desired.\\nThe cut object can be transformed--i.e., rotated, scaled, and trans-\\nlated, (RST)--before combination with the destination image.  This\\nis done using an interactive graphical tool with “handles” for RST\\ncontrol.  The tool speciﬁes a 2-D RST transformation matrix,M .\\nThe source image is then bilinearly interpolated through the matrix\\nto paste the cut object into the destination image.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='Image composition often requires blending an object from one\\nimage into another image such that the cut-out object is not in the\\nforeground.  This requires the composition artist to “slip” the cut-\\nout object behind some scene components while leaving it in front\\nof other components. This operation can again be performed using\\nthe live-wire tool to create a composition mask1 . Scene components\\ncan be cut out of an image to create a mask such that any additions\\nor changes to the scene will not affect masked pixels.\\n4.1. Edge Filtering\\nAs described, live-wire boundaries are limited by the pixel reso-\\n2. Admittedly, the gradient direction cost used in Intelligent Scissors is more\\nsusceptible to noise in areas of low contrast (since it computes a smoothness cost\\nbased only on two points and one link).  However, it is possible to extend the gradient\\ndirection term to include 3 pixels and 2 links without signiﬁcant loss of computational\\nefﬁciency.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='efﬁciency.\\n1. Similar in concept to an optical mask used in motion picture special effects.\\nlution of the image.  This may produce jaggies along object bound-\\naries in a resulting composition.  However, subpixel accuracy can\\nbe obtained by exploiting the signed output of the laplacian opera-\\ntor.  That is, the position of the object edge can be estimated to sub-\\npixel accuracy by using a (linearly) weighted combination of the\\nlaplacian pixel values on either side of the zero-crossings.\\nSince the live-wire boundary will not always correspond to a\\nzero-crossing, jaggies can also be reduced by appropriate edge ﬁl-\\ntering, similar to anti-aliasing.  Edge ﬁltering is also desirable\\nbecause real world images are acquired using ﬁnite image detectors\\nand, as a result, pixels on or near an object boundary share informa-\\ntion (i.e., color) from the object and the background.\\nOne approach to edge ﬁltering is to perform a local post-smooth-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='ing of the image around the pasted object boundary .  However, this\\ndoes not account for the fact that the edge pixels of the cut object\\nvery likely contain some background information from the original\\nimage. This is most noticeable when an object is composited into a\\nscene with a different background color than the object’s original\\nbackground color.  A more general solution would determine how\\nmuch of each edge pixel corresponds to the actual object color and\\nweight them accordingly when combining into another image.\\n4.2. Spatial Frequency and Contrast Matching\\nOnce the object of interest has been segmented from the sur-\\nrounding background we can scale it, rotate it, color it, or paste it\\nonto another (destination) image.  When pasting, it is desirable to\\nperform image composition “seamlessly” in order to make it believ-\\nable.  That is, we should not be able to detect where the paste\\noccurred.  However, the source and the destination images will'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='often have differing spatial frequencies or contrast due to differ-\\nences in focus or lighting when the images were acquired.  Thus,\\nequivalencing of spatial frequencies and normalization of contrast\\nis sometimes desirable in order to produce a convincing composi-\\ntion.\\nEquivalencing of spatial frequencies is performed by matching\\nthe spectral content of the cut piece and the destination image in the\\nvicinity where it is to be pasted.  Convincing composition often\\nrequires the spectra of the object and the destination image to\\nmatch.  This is accomplished by low-pass ﬁltering the spectrum\\nwith the higher frequency content to match that of the other.  The\\nspectrum with the higher frequency content is determined by\\nparameter ﬁtting of a Butterworth low-pass ﬁlter (BLPF) to both\\nspectra.  Parameters corresponding to the spectrum with the lower\\nfrequency content are used to low-pass ﬁlter the spectrum of the\\nother image.\\nThe BLPF B ( u , v , d 0 , n ) is given by'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content=\"where d 0  is the distance of the cutoff frequency from the origin and\\nn  is the ﬁlter order.\\nEquivalencing of spatial frequencies is performed by ﬁrst com-\\nputing the fourier transforms, S ( u , v ) and I ( u , v ) of the source image\\nS ( x , y ) and the destination image I ( x , y ).  We then compute the log\\npower spectra s ( u , v ) and i( u , v ):\\nBy varying the two parameters d 0  and n , a least squares ﬁt can be\\nused to create a normalized Butterworth ﬁlter B ( u , v , d 0 ', n ') (where\\nBu v d 0 n,, ,()\\n1\\n1\\nu 2 v 2+\\nd 0\\n2 n\\n+\\n=\\nsu v ,() Su v ,()[]log 2=\\niu v ,() Iu v ,()[]log 2=\"),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content=\"d 0 ' and n ' are the ﬁt parameters) matched to the spatial frequency\\ncharacteristics of i( u , v ).  If i( u , v ) demonstrates lower spatial fre-\\nquency content than s ( u , v ), the spatial frequencies between the two\\nimages can be equivalenced by ﬁtting B ( u , v , d 0 ', n ') to s ( u , v ).  The\\nequivalenced result using the inverse Fourier transform\\nis then pasted onto I ( x , y ).  Prior to pasting, the colors in the source\\nimage are scaled to the range of I ( x , y ) to account for differences in\\ncontrast.\\n5. Results\\nFigures 5, 6, and 7 show the boundaries deﬁned using Intelligent\\nScissors on a variety of image types.  Fig. 5 is an artiﬁcial test image\\nthat exhibits gaussian edge blurring and point noise typical of some\\nimaging hardware.   Fig. 6 is the desktop scene used in Figures 2\\nand 3.  Fig. 7 (a CT image of a lumbar spine) demonstrates the live-\\nwire’s application to medical imaging.  The boundary deﬁnition\"),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content='times (for a trained user) for each displayed object boundary are\\ngiven in the caption of each respective ﬁgure.\\nFigure 8 graphically compares the live-wire boundary deﬁnition\\ntimes and boundary accuracy with manual tracing.  These results\\nshow the average time and accuracy from a study where 8 untrained\\nusers1  were asked to deﬁne the boundaries of ﬁve objects (the two\\nobjects in Fig. 5, the paper clip holder and pocket knife in Fig. 6,\\nand the outer boundary of the spinal vertebrae in Fig. 7).\\nFigures 9(a-c) demonstrates Intelligent Scissors application to\\ncolor images and show the boundaries deﬁned using Intelligent\\nScissors for the image composition in Fig. 9(d).  Objects were\\nscaled, rotated, and (in the case of Fig. 9(a)) ﬂipped to produce the\\nﬁnal composition in Fig. 9(d).  Note also that live-wire masking was\\nperformed on some of the foreground (grass).\\nPreprocessing requires 36 convolutions for color images (from'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content='3 × 3, 5 × 5, 7 × 7, and 9 × 9 kernels), a gradient orientation calculation,\\na maximum gradient neighborhood search, and creation of a local\\n1.   Each user spent a few minutes becoming familiar with the live-wire tool as\\nwell as a manual tracing tool and then were asked to deﬁne the boundary of 5 objects\\nEach boundary was deﬁned multiple times by each user with both Intelligent Scissors\\nand manual tracing (to also measure intra- and inter-user reproducibility--not shown).\\nS ′ xy ,() F 1− Bu v d 0 ′ n ′,,,() Su v ,()⋅[]=\\nFigure 8:  Average timing and accuracy comparison between manually\\ntraced and live-wire boundaries for 8 users.  Boundary times are for\\nindividual objects where accuracy measurements are over all objects.\\n0\\n5\\n10\\n15\\n20\\n25Average Time (in seconds)\\n30\\n40\\n50\\n60\\n35\\n45\\n55\\n65\\nPolygon Curve Holder Knife Spine\\nObject\\nBoundary Deﬁnition Time\\nLegend\\nLive-Wire\\nHand Traced\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60\\n65\\n70\\n75\\n80\\n85\\n90\\n95\\n100\\n01234\\nDistance (in Pixels)\\nLegend\\nLive-Wire\\nHand Traced'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content='Hand Traced\\nPercentage (of Pixels ≤  Distance from “Ideal” Boundary)\\nAccuracy\\ncost map.  For color images, we maximize feature values over the\\nthree color bands rather than averaging.\\nPreviously, dynamic programming approaches to boundary\\ndetection were typically computationally expensive.  However, by\\nformulating DP as a graph search and restricting the local costs to\\ninteger values within a range, the 2-D DP algorithm can take advan-\\ntage of an O(N ) sort for N  points.  As mentioned, adding points to\\nthe sorted active list and removing points from it requires constant\\ntime.  As a result, the algorithm’s computational complexity forN\\nimage pixels is O( N ).  This can be seen by examining the algorithm\\nin a worst case situation. As a pixel is removed from the active list,\\nit is expanded by computing the cumulative cost to all of its neigh-\\nbors that have not already been expanded.  In the worst case, a pixel\\nhas its cumulative cost computed by all of its 8 neighbors, resulting'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content='in 8N  cumulative cost computations for N  pixels.  Obviously, not\\nevery point can be expanded after all of its neighbors have. Except\\nfor the seed point, every point that has a cumulative cost must have\\nat least one neighboring point that has already been expanded.  Thus\\nthe cumulative cost is not recomputed for those neighbors.  In short,\\nit can be shown that at most only 4N  cumulative cost computations\\nare performed, resulting in an O( N ) algorithm.\\n6.Conclusions and Future Work\\nIntelligent Scissors provide an accurate and efﬁcient interactive\\ntool for object extraction and image composition. In fact, and in\\nsharp contrast to tedious manual boundary deﬁnition, object extrac-\\ntion using the live-wire is almost as much fun as the ﬁnal result (the\\ncomposition).  Intelligent Scissors are intuitive to use and can be\\napplied to existing black and white or color images of arbitrary\\ncomplexity. There are many rich extensions of this work, including:'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content='(1) making use of the weighted zero-crossings in the Laplacian to\\nperform subpixel edge ﬁltering and anti-aliasing,  (2) use of multi-\\nple layered (multiplane) masks,  (3) making spatial frequency\\nequivalencing locally adaptive,  (4) varying the light source over\\nthe object using directional gradient shading (artiﬁcial or borrowed)\\nto provide consistent lighting in the composition, and, most impor-\\ntantly  (5) extension of the 2-D DP graph search and application of\\nthe live-wire snap and training tools to moving objects and moving,\\nmultiplane masks for composition of image sequences.\\nReferences\\n[1] A. A. Amini, T. E. Weymouth, and R. C. Jain, “Using Dynamic\\nProgramming for Solving V ariational Problems in Vision,”\\nIEEE Transactions on Pattern Analysis and Machine Intelli-\\ngence, vol. 12, no. 2, pp. 855-866, Sept. 1990.\\n[2] D. H. Ballard, and C. M. Brown, Computer Vision .  Engle-\\nwood Cliffs, NJ: Prentice Hall, 1982.\\n[3] R. Bellman and S. Dreyfus, Applied Dynamic Programming .'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 7}, page_content='Princeton, NJ: Princeton University Press, 1962.\\n[4]  Y . P . Chien and K. S. Fu, “A Decision Function Method for\\nBoundary Detection,” Computer Graphics and Image Pro-\\ncessing , vol. 3, no. 2, pp. 125-140, June 1974.\\n[5] D. Daneels, et al., “Interactive Outlining: An Improved\\nApproach Using Active Contours,” in SPIE Proceedings of\\nStorage and Retrieval for Image and V ideo Databases , vol\\n1908, pp. 226-233, Feb. 1993.\\n[6] E. W. Dijkstra, “A Note on Two Problems in Connexion with\\nGraphs,” Numerische Mathematik , vol. 1, pp. 269-270, 1959.\\n[7] M. M. Fleck, “Multiple Widths Yield Reliable Finite Differ-\\nences,” IEEE Transactions on Pattern Analysis and Machine\\nIntelligence , vol. 14, no. 4, pp. 412-429, April 1992.\\n[8] M. Kass, A. Witkin, and D. Terzopoulos, “Snakes: Active'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 8}, page_content='Contour Models,” in Proceedings of the First International\\nConference on Computer Vision , London, England, pp. 259-\\n68, June 1987.\\n[9] D. Marr and E. Hildreth, “A Theory of Edge Detection,” in\\nProceedings of the Royal Society of London--Series B: Biolog-\\nical Sciences, vol. 207, no. 1167, pp. 187-217, Feb. 1980.\\n[10] A. Martelli, “An Application of Heuristic Search Methods to\\nEdge and Contour Detection,” Communications of the ACM ,\\nvol. 19, no. 2, pp. 73-83, Feb. 1976.\\n[11] U. Montanari, “On the Optimal Detection of Curves in Noisy\\nPictures,” Communications of the ACM , vol. 14, no. 5, pp.\\n335-45, May 1971.\\n[12] E. N. Mortensen, B. S. Morse, W . A. Barrett, and J. K. Udupa,\\n“Adaptive Boundary Dectection Using ‘Live-Wire’ Two-\\nDimensional Dynamic Programming,” inIEEE Proceedings\\nof Computers in Cardiology , pp. 635-638, Oct. 1992.\\n[13] N. J. Nilsson, Principles of Artiﬁcial Intelligence .  Palo Alto,\\nCA: Tioga,  1980.\\n[14] D. L. Pope, D. L. Parker, D. E. Gustafson, and P . D. Clayton,'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 8}, page_content='\"Dynamic Search Algorithms in Left V entricular Border Rec-\\nognition and Analysis of Coronary Arteries,\" inIEEE Pro-\\nceedings of Computers in Cardiology , pp. 71-75, Sept. 1984.\\n[15] D. J. Williams and M. Shah, “A Fast Algorithm for Active\\nContours and Curvature Estimation,” CVGIP: Image Under-\\nstanding , vol. 55, no. 1, pp. 14-26, Jan. 1992.\\nFigure 5:  Test image exhibiting edge blurring and point noise.  Bound-\\nary deﬁnition times--polygon: 4.3 sec and curve: 8.3 sec.\\nFigure 6:  Desktop scene.  Boundary deﬁnition times--block: 2.4 sec;\\npaper clip holder: 3.6 sec; pocket knife: 4.6 sec; liqud paper bottle: 5.1\\nsec; and spoon: 9.8 sec.\\nFigure 7:  Spinal vertebrae.  Boundary deﬁnition time--5.9 sec.\\n(a)\\n(b)\\n(c)\\n(d)\\nFigure 9:  Images used in a composition. (a-c) Live-wire boundaries\\nused for segmentation.  (d) Final composition: “Millennium.”\\nView publication stats')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "\n",
    "loader = PyPDFLoader(\"intelligent_scissor.pdf\")\n",
    "documents = loader.load_and_split(text_splitter)\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Content in a Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
      "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_documents(\n",
    "    documents, embedding=OpenAIEmbeddings()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Knowledge Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the content in a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>See discussions, stats, and author profiles fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abstract\\nWe present a new, interactive tool c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lowed, rather than simply the strongest edge i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>manipulation techniques, has also been used to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>control the segmentation process.\\nThis paper ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tions, use an interactively selected seed poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>boundary will look like when the rough approxi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>of freedom within a window about the two-dimen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>permission and/or a fee. \\n©1995  ACM-0-89791-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The most important difference between previous...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  See discussions, stats, and author profiles fo...\n",
       "1  Abstract\\nWe present a new, interactive tool c...\n",
       "2  lowed, rather than simply the strongest edge i...\n",
       "3  manipulation techniques, has also been used to...\n",
       "4  control the segmentation process.\\nThis paper ...\n",
       "5  tions, use an interactively selected seed poin...\n",
       "6  boundary will look like when the rough approxi...\n",
       "7  of freedom within a window about the two-dimen...\n",
       "8  permission and/or a fee. \\n©1995  ACM-0-89791-...\n",
       "9  The most important difference between previous..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame([d.page_content for d in documents], columns=[\"text\"])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a Knowledge Base using the DataFrame we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import KnowledgeBase\n",
    "\n",
    "knowledge_base = KnowledgeBase(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c359f3d6d24c43b48d0f812015473b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating questions:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-17 13:50:31,051 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:50:31,051 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 80, in generate_questions\n",
      "    yield self._modify_question(question, knowledge_base, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\conversational_questions.py\", line 57, in _modify_question\n",
      "    out = self._llm_complete(messages=messages)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2024-11-17 13:50:37,268 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:50:37,269 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 80, in generate_questions\n",
      "    yield self._modify_question(question, knowledge_base, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\conversational_questions.py\", line 57, in _modify_question\n",
      "    out = self._llm_complete(messages=messages)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2024-11-17 13:50:38,936 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:50:38,936 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 59, in generate_questions\n",
      "    yield self.generate_single_question(knowledge_base, *args, **kwargs, seed_document=doc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\simple_questions.py\", line 103, in generate_single_question\n",
      "    generated_qa = self._llm_complete(messages=messages)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2024-11-17 13:50:49,463 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:50:49,463 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 59, in generate_questions\n",
      "    yield self.generate_single_question(knowledge_base, *args, **kwargs, seed_document=doc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\simple_questions.py\", line 103, in generate_single_question\n",
      "    generated_qa = self._llm_complete(messages=messages)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2024-11-17 13:50:55,618 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:50:55,619 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 80, in generate_questions\n",
      "    yield self._modify_question(question, knowledge_base, *args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\conversational_questions.py\", line 57, in _modify_question\n",
      "    out = self._llm_complete(messages=messages)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2024-11-17 13:51:03,385 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:51:03,385 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 59, in generate_questions\n",
      "    yield self.generate_single_question(knowledge_base, *args, **kwargs, seed_document=doc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\simple_questions.py\", line 103, in generate_single_question\n",
      "    generated_qa = self._llm_complete(messages=messages)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "2024-11-17 13:51:05,101 pid:3908 MainThread giskard.rag  ERROR    Encountered error in question generation: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}. Skipping.\n",
      "2024-11-17 13:51:05,116 pid:3908 MainThread giskard.rag  ERROR    Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 59, in generate_questions\n",
      "    yield self.generate_single_question(knowledge_base, *args, **kwargs, seed_document=doc)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\simple_questions.py\", line 103, in generate_single_question\n",
      "    generated_qa = self._llm_complete(messages=messages)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\rag\\question_generators\\base.py\", line 42, in _llm_complete\n",
      "    out = self._llm_client.complete(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\llm\\client\\openai.py\", line 63, in complete\n",
      "    completion = self._client.chat.completions.create(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_utils\\_utils.py\", line 275, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\resources\\chat\\completions.py\", line 829, in create\n",
      "    return self._post(\n",
      "           ^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1278, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 955, in request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1044, in _request\n",
      "    return self._retry_request(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1093, in _retry_request\n",
      "    return self._request(\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\openai\\_base_client.py\", line 1059, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n"
     ]
    }
   ],
   "source": [
    "from giskard.rag import generate_testset\n",
    "\n",
    "testset = generate_testset(\n",
    "    knowledge_base,\n",
    "    num_questions=60,\n",
    "    agent_description=\"A chatbot answering questions about the intelligent scissor paper\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display a few samples from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1: What are the limitations of the training based on learned edge characteristics?\n",
      "Reference answer: Training is most effective for those objects with edge properties that are relatively consistent along the object boundary. Training can be counter-productive for objects with sudden and/or dramatic changes in edge features. However, training can be turned on and off interactively throughout the definition of an object boundary.\n",
      "Reference context:\n",
      "Document 32: precomputed feature maps along the closestt pixels of the edge seg-\n",
      "ment and increments the feature histogram element by the corre-\n",
      "sponding pixel weight to generate a histogram for each feature\n",
      "involved in training.\n",
      "After sampling and smoothing, each feature histogram is then\n",
      "scaled and inverted (by subtracting the scaled histogram values\n",
      "from its maximum value) to create the feature cost map needed to\n",
      "convert feature values to trained cost functions.\n",
      "Since training is based on learned edge characteristics from the\n",
      "most recent portion of an object’s boundary, training is most effec-\n",
      "tive for those objects with edge properties that are relatively consis-\n",
      "tent along the object boundary (or, if changing, at least change\n",
      "smoothly enough for the training algorithm to adapt).  In fact, train-\n",
      "ing can be counter-productive for objects with sudden and/or dra-\n",
      "matic changes in edge features.  However, training can be turned on\n",
      "and off interactively throughout the deﬁnition of an object bound-\n",
      "******************\n",
      "\n",
      "Question 2: What is the role of the seed points in the intelligent scissor tool?\n",
      "Reference answer: Seed points are used to compute optimal paths from the seed point to all points in the image. A minimum of two seed points must be placed to ensure a closed object boundary. The path map from the first seed point of every object is maintained during the course of an object’s boundary definition to provide a closing boundary path from the free point. If a new seed point is placed, it reinitiates the 2-D DP boundary detection, causing potential paths to be recomputed.\n",
      "Reference context:\n",
      "Document 23: new seed point while effectively “tieing off” the boundary com-\n",
      "puted up to the new seed point.\n",
      "Note again that optimal paths are computed from the seed point\n",
      "to all  points in the image (since the 2-D DP graph search produces\n",
      "a minimum cost spanning tree of the image [6]).  Thus, by selecting\n",
      "a free point with the mouse cursor, the interactive live-wire tool is\n",
      "simply selecting an optimal boundary segment from a large collec-\n",
      "tion of optimal paths.\n",
      "Since each pixel (or free point) deﬁnes only one optimal path to\n",
      "a seed point, a minimum of two seed points must be placed to\n",
      "ensure a closed object boundary.  The path map from the ﬁrst seed\n",
      "point of every object is maintained during the course of an object’ s\n",
      "boundary deﬁnition to provide a closing boundary path from the\n",
      "free point.  The closing boundary segment from the free point to the\n",
      "ﬁrst seed point expedites boundary closure.\n",
      "Placing seed points directly on an object’s edge is often difﬁcult\n",
      "\n",
      "Document 22: to the seed point.  By constraining the seed point and free points to\n",
      "lie near a given edge, the user is able to interactively “snap” and\n",
      "“wrap” the live-wire boundary around the object of interest.  Figure\n",
      "2 demonstrates how a live-wire boundary segment adapts to\n",
      "changes in the free point (cursor position) by latching onto more\n",
      "and more of an object boundary.  Speciﬁcally, note the live-wire\n",
      "segments corresponding to user-speciﬁed free point positions at\n",
      "timest0 , t1 , and t2 .  Although Fig. 2 only shows live-wire segments\n",
      "for three discrete time instances, live-wire segments are actually\n",
      "updated dynamically and interactively (on-the-ﬂy) with each move-\n",
      "ment of the free point.\n",
      "When movement of the free point causes the boundary to digress\n",
      "from the desired object edge, interactive input of a new seed point\n",
      "prior to the point of departure reinitiates the 2-D DP boundary\n",
      "detection.  This causes potential paths to be recomputed from the\n",
      "\n",
      "Document 24: and tedious.  If a seed point is not localized to an object edge then\n",
      "spikes results on the segmented boundary at those seed points (since\n",
      "Figure 2:  Image demonstrating how the live-wire segment adapts and\n",
      "snaps to an object boundary as the free point moves (via cursor move-\n",
      "ment).  The path of the free point is shown in white.  Live-wire segments\n",
      "from previous free point positions (t0 , t 1 , and t 2 ) are shown in green.\n",
      "(a) (b)\n",
      "Figure 3:  Comparison of live-wire without (a) and with (b) cooling.\n",
      "Withot cooling (a), all seed points must be placed manually on the\n",
      "object edge.  With cooling (b), seed points are generated automatically\n",
      "as the live-wire segment freezes.\n",
      "the boundary is forced to pass through the seed points).  T o facilitate\n",
      "seed point placement, a cursor snap is available which forces the\n",
      "mouse pointer to the maximum gradient magnitude pixel within a\n",
      "user speciﬁed neighborhood.  The neighborhood can be anywhere\n",
      "******************\n",
      "\n",
      "Question 3: What is the key difference between Intelligent Scissors and previous boundary finding techniques?\n",
      "Reference answer: The key difference between Intelligent Scissors and previous boundary finding techniques lies in the method of interaction. Previous methods exhibit a pattern of boundary approximation followed by boundary refinement, whereas Intelligent Scissors allow the user to interactively select the most suitable boundary from a set of all optimal boundaries emanating from a seed point.\n",
      "Reference context:\n",
      "Document 6: boundary will look like when the rough approximation is input. If\n",
      "the resulting boundary is not satisfactory, the process must be\n",
      "repeated or the boundary must be manually edited.  W e provide a\n",
      "detailed comparison of snakes and Intelligent Scissors in section\n",
      "3.6.\n",
      "Another class of image segmentation techniques use a graph\n",
      "searching formulation of DP (or similar concepts) to ﬁnd globally\n",
      "optimal boundaries [2, 4, 10, 11, 14].  These techniques differ from\n",
      "snakes in that boundary points are generated in a stage-wise optimal\n",
      "cost fashion whereas snakes iteratively minimize an energy func-\n",
      "tional for all points on a contour in parallel (giving the appearance\n",
      "of wiggling).  However, like snakes, these graph searching tech-\n",
      "niques typically require a boundary template--in the form of a man-\n",
      "ually entered rough approximation, a ﬁgure of merit, etc.--which is\n",
      "used to impose directional sampling and/or searching constraints.\n",
      "This limits these techniques to  a boundary search with one degree\n",
      "\n",
      "Document 35: 3. Snakes are typically attracted to edge features only within the\n",
      "gravity of an edge’s gradient energy valley whereas the live-wire\n",
      "boundary can snap to strong edge features from arbitrary dis-\n",
      "tances (since the 2-D DP’s search window is the entire image).\n",
      "4. Snakes are globally optimal over the entire contour whereas live-\n",
      "wire boundaries are piece-wise optimal (i.e., optimal between\n",
      "seed points).  We feel this creates a desirable balance between\n",
      "global optimality and local control.  This piece-wise optimality\n",
      "also allows for path cooling and intra-object on-the-ﬂy training.\n",
      "5. Finally, snakes reﬁne (and interactively “nudge” by placing\n",
      "springs, etc.) a single rough boundary approximation where the\n",
      "live-wire tool interactivelyselects  an optimal boundary segment\n",
      "from potentially all  possible minimum cost paths.\n",
      "Interactive optimal 2-D path selection is what makes Intelligent\n",
      "Scissors work and is the key difference between Intelligent Scissors\n",
      "\n",
      "Document 9: The most important difference between previous boundary ﬁnd-\n",
      "ing techniques and Intelligent Scissors presented here lies not in the\n",
      "boundary deﬁning criteria per se´, but in themethod  of interaction.\n",
      "Namely, previous methods exhibit a pattern of boundary approxi-\n",
      "mation followed by boundary reﬁnement, whereas Intelligent Scis-\n",
      "sors  allow the user tointeractively select  the most suitable\n",
      "boundary from a set of all  optimal boundaries emanating from a\n",
      "seed point.  In addition, previous approaches do not incorporate on-\n",
      "the-ﬂy training or cooling, and are not as computationally efﬁcient.\n",
      "Finally, it appears that the problem of automated matching of spa-\n",
      "tial frequencies for digital image composition has not been\n",
      "addressed previously.\n",
      "3. Intelligent Scissors\n",
      "Boundary deﬁnition via dynamic programming can be formu-\n",
      "lated as a graph searching problem [10] where the goal is to ﬁnd the\n",
      "optimal path between a start node and a set of goal nodes.  As\n",
      "\n",
      "Document 34: 5. Snakes and Intelligent Scissors both require interaction as part of\n",
      "the boundary segmentation process.\n",
      "Dif ferences (compare with corresponding similarities above):\n",
      "1. The laplacian zero-crossing binary cost feature seems to have not\n",
      "been used previously in active contours models 1  (or DP bound-\n",
      "ary tracking methods for that matter).\n",
      "2. The active contour smoothing term is internal (i.e., based on the\n",
      "contour’s point positions) whereas the smoothing term for live-\n",
      "wire boundaries is computed from external image gradient direc-\n",
      "tions2(next page) .\n",
      "1. Kass et al. [8] did use a squared laplacian energy functional to show the rela-\n",
      "tionship of scale-space continuation to the Marr-Hildreth edge detection theory.  How-\n",
      "ever, the squared laplacian does not represent a binary condition, nor could it since the\n",
      "variational calculus minimization used in [8] required that all functionals be differen-\n",
      "tiable.\n",
      "******************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_set_df = testset.to_pandas()\n",
    "\n",
    "for index, row in enumerate(test_set_df.head(3).iterrows()):\n",
    "    print(f\"Question {index + 1}: {row[1]['question']}\")\n",
    "    print(f\"Reference answer: {row[1]['reference_answer']}\")\n",
    "    print(\"Reference context:\")\n",
    "    print(row[1]['reference_context'])\n",
    "    print(\"******************\", end=\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now save the test set to a file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.save(\"RAG-test-set.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the RAG Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a retriever from the Vector Store that will allow us to get the top similar documents to a given question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 0}, page_content='See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220720964\\nIntelligent scissors for image composition\\nConference Paper · January 1995\\nDOI: 10.1145/218380.218442\\xa0·\\xa0Source: DBLP\\nCITATIONS\\n812\\nREADS\\n2,545\\n2 authors, including:\\nEric N. Mortensen\\nLucidyne Technologies, Inc.\\n34 PUBLICATIONS\\xa0\\xa0\\xa03,134 CITATIONS\\xa0\\xa0\\xa0\\nSEE PROFILE\\nAll content following this page was uploaded by Eric N. Mortensen on 01 June 2014.\\nThe user has requested enhancement of the downloaded file.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 6}, page_content='2-D extension to previous optimal edge tracking methods rather\\nthan an improvement on active contours.\\n4. Image Composition with Intelligent Scissors\\nAs mentioned, composition artists need an intelligent, interactive\\ntool to facilitate image component boundary deﬁnition.  Since Intel-\\nligent Scissors can quickly segment object from an image, it serves\\nas a tool for cut and paste operations.  After object boundaries have\\nbeen extracted, object can be copied from the image and placed into\\na buffer (i.e., clipboard) for future processing and placement into\\nanother image, or the same image if desired.\\nThe cut object can be transformed--i.e., rotated, scaled, and trans-\\nlated, (RST)--before combination with the destination image.  This\\nis done using an interactive graphical tool with “handles” for RST\\ncontrol.  The tool speciﬁes a 2-D RST transformation matrix,M .\\nThe source image is then bilinearly interpolated through the matrix\\nto paste the cut object into the destination image.'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='Abstract\\nWe present a new, interactive tool called Intelligent Scissors\\nwhich we use for image segmentation and composition.  Fully auto-\\nmated segmentation is an unsolved problem, while manual tracing\\nis inaccurate and laboriously unacceptable.  However, Intelligent\\nScissors allow objects within digital images to be extracted quickly\\nand accurately using simple gesture motions with a mouse.  When\\nthe gestured mouse position comes in proximity to an object edge,\\nalive-wire boundary  “snaps” to, and wraps around the object of\\ninterest.\\nLive-wire boundary detection formulates discrete dynamic pro-\\ngramming (DP) as a two-dimensional graph searching problem.  DP\\nprovides mathematically optimal boundaries while greatly reducing\\nsensitivity to local noise or other intervening structures.  Robust-\\nness is further enhanced withon-the-ﬂy training  which causes the\\nboundary to adhere to the speciﬁc type of edge currently being fol-'),\n",
       " Document(metadata={'source': 'intelligent_scissor.pdf', 'page': 1}, page_content='control the segmentation process.\\nThis paper describes a new, interactive, digital image segmenta-\\ntion tool called “Intelligent Scissors” which allows rapid object\\nextraction from arbitrarily complex backgrounds. Intelligent Scis-\\nsors boundary detection formulates discrete dynamic programming\\n(DP) as a two-dimensional graph searching problem.  Presented as\\npart of this tool areboundary cooling  and on-the-ﬂy training , which\\nreduce user input and dynamically adapt the tool to speciﬁc types of\\nedges.  Finally, we presentlive-wire masking  and spatial frequency\\nequivalencing  for convincing image compositions.\\n2. Background\\nDigital image segmentation techniques are used to extract image\\ncomponents from their surrounding natural background.  However,\\ncurrently available computer based segmentation tools are typically\\nprimitive and often offer little more advantage than manual tracing.\\nRegion based magic wands, provided in many desktop applica-')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.get_relevant_documents(\"what is the intelligent scissor paper about?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from operator import itemgetter\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure the chain works by testing it with a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The intelligent scissor paper is about image composition and segmentation using a new interactive tool called Intelligent Scissors.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"question\": \"What is the intelligent scissor paper about?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Model on the Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to create a function that invokes the chain with a specific question and returns the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_fn(question, history=None):\n",
    "    return chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `evaluate()` function to evaluate the model on the test set. This function will compare the answers from the chain with the reference answers in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8cc69504ac41c0b387690607ae5c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Asking questions to the agent:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06163595103a46cd90ff86fc0cdc19d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "CorrectnessMetric evaluation:   0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from giskard.rag import evaluate\n",
    "\n",
    "report = evaluate(answer_fn, testset=testset, knowledge_base=knowledge_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let now display the report.\n",
    "\n",
    "Here are the five components of our RAG application:\n",
    "\n",
    "* **Generator**: This is the LLM used in the chain to generate the answers.\n",
    "* **Retriever**: This is the retriever that fetches relevant documents from the knowledge base according to a query.\n",
    "* **Rewriter**: This is a component that rewrites the user query to make it more relevant to the knowledge base or to account for chat history.\n",
    "* **Router**: This is a component that filters the query of the user based on his intentions.\n",
    "* **Knowledge Base**: This is the set of documents given to the RAG to generate the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "    <style>\n",
       "        .bk-notebook-logo {\n",
       "            display: block;\n",
       "            width: 20px;\n",
       "            height: 20px;\n",
       "            background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAYAAACNiR0NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAABx0RVh0U29mdHdhcmUAQWRvYmUgRmlyZXdvcmtzIENTNui8sowAAAOkSURBVDiNjZRtaJVlGMd/1/08zzln5zjP1LWcU9N0NkN8m2CYjpgQYQXqSs0I84OLIC0hkEKoPtiH3gmKoiJDU7QpLgoLjLIQCpEsNJ1vqUOdO7ppbuec5+V+rj4ctwzd8IIbbi6u+8f1539dt3A78eXC7QizUF7gyV1fD1Yqg4JWz84yffhm0qkFqBogB9rM8tZdtwVsPUhWhGcFJngGeWrPzHm5oaMmkfEg1usvLFyc8jLRqDOMru7AyC8saQr7GG7f5fvDeH7Ej8CM66nIF+8yngt6HWaKh7k49Soy9nXurCi1o3qUbS3zWfrYeQDTB/Qj6kX6Ybhw4B+bOYoLKCC9H3Nu/leUTZ1JdRWkkn2ldcCamzrcf47KKXdAJllSlxAOkRgyHsGC/zRday5Qld9DyoM4/q/rUoy/CXh3jzOu3bHUVZeU+DEn8FInkPBFlu3+nW3Nw0mk6vCDiWg8CeJaxEwuHS3+z5RgY+YBR6V1Z1nxSOfoaPa4LASWxxdNp+VWTk7+4vzaou8v8PN+xo+KY2xsw6une2frhw05CTYOmQvsEhjhWjn0bmXPjpE1+kplmmkP3suftwTubK9Vq22qKmrBhpY4jvd5afdRA3wGjFAgcnTK2s4hY0/GPNIb0nErGMCRxWOOX64Z8RAC4oCXdklmEvcL8o0BfkNK4lUg9HTl+oPlQxdNo3Mg4Nv175e/1LDGzZen30MEjRUtmXSfiTVu1kK8W4txyV6BMKlbgk3lMwYCiusNy9fVfvvwMxv8Ynl6vxoByANLTWplvuj/nF9m2+PDtt1eiHPBr1oIfhCChQMBw6Aw0UulqTKZdfVvfG7VcfIqLG9bcldL/+pdWTLxLUy8Qq38heUIjh4XlzZxzQm19lLFlr8vdQ97rjZVOLf8nclzckbcD4wxXMidpX30sFd37Fv/GtwwhzhxGVAprjbg0gCAEeIgwCZyTV2Z1REEW8O4py0wsjeloKoMr6iCY6dP92H6Vw/oTyICIthibxjm/DfN9lVz8IqtqKYLUXfoKVMVQVVJOElGjrnnUt9T9wbgp8AyYKaGlqingHZU/uG2NTZSVqwHQTWkx9hxjkpWDaCg6Ckj5qebgBVbT3V3NNXMSiWSDdGV3hrtzla7J+duwPOToIg42ChPQOQjspnSlp1V+Gjdged7+8UN5CRAV7a5EdFNwCjEaBR27b3W890TE7g24NAP/mMDXRWrGoFPQI9ls/MWO2dWFAar/xcOIImbbpA3zgAAAABJRU5ErkJggg==);\n",
       "        }\n",
       "    </style>\n",
       "    <div>\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-notebook-logo\"></a>\n",
       "        <span id=\"fdace654-6de1-47d7-b83c-2cd4ca5acefa\">Loading BokehJS ...</span>\n",
       "    </div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "'use strict';\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    function drop(id) {\n      const view = Bokeh.index.get_by_id(id)\n      if (view != null) {\n        view.model.document.clear()\n        Bokeh.index.delete(view)\n      }\n    }\n\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n\n    // Clean up Bokeh references\n    if (id != null) {\n      drop(id)\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim()\n            drop(id)\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded(error = null) {\n    const el = document.getElementById(\"fdace654-6de1-47d7-b83c-2cd4ca5acefa\");\n    if (el != null) {\n      const html = (() => {\n        if (typeof root.Bokeh === \"undefined\") {\n          if (error == null) {\n            return \"BokehJS is loading ...\";\n          } else {\n            return \"BokehJS failed to load.\";\n          }\n        } else {\n          const prefix = `BokehJS ${root.Bokeh.version}`;\n          if (error == null) {\n            return `${prefix} successfully loaded.`;\n          } else {\n            return `${prefix} <b>encountered errors</b> while loading and may not function as expected.`;\n          }\n        }\n      })();\n      el.innerHTML = html;\n\n      if (error != null) {\n        const wrapper = document.createElement(\"div\");\n        wrapper.style.overflow = \"auto\";\n        wrapper.style.height = \"5em\";\n        wrapper.style.resize = \"vertical\";\n        const content = document.createElement(\"div\");\n        content.style.fontFamily = \"monospace\";\n        content.style.whiteSpace = \"pre-wrap\";\n        content.style.backgroundColor = \"rgb(255, 221, 221)\";\n        content.textContent = error.stack ?? error.toString();\n        wrapper.append(content);\n        el.append(wrapper);\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(() => display_loaded(error), 100);\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n      try {\n            for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n\n      } catch (error) {display_loaded(error);throw error;\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"fdace654-6de1-47d7-b83c-2cd4ca5acefa\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<!DOCTYPE html>\n",
       "\n",
       "\n",
       "<style>\n",
       "    body{\n",
       "  background: #18181B;\n",
       "}\n",
       "\n",
       ".main{\n",
       "  font-family: \"Noto Sans\", ui-sans-serif, system-ui, -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif, \"Apple Color Emoji\", \"Segoe UI Emoji\", \"Segoe UI Symbol\", \"Noto Color Emoji\";\n",
       "  color: #FDFDFD;\n",
       "}\n",
       "\n",
       "h1 {\n",
       "  font-size: 2.5rem;\n",
       "  color: white;\n",
       "}\n",
       "\n",
       "h3 {\n",
       "  font-size: 1.5rem;\n",
       "  background: #0c087c;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border: 1px solid #6b7280;}\n",
       "\n",
       ".extended-title{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#gsk-overview{\n",
       "  display:flex;\n",
       "}\n",
       "\n",
       "h4 {\n",
       "  font-size: 1rem;\n",
       "  background: #27272A;\n",
       "  padding: 10px;\n",
       "  margin: 0px;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "h2 {\n",
       "  font-size: 1.5rem;\n",
       "  margin-top: 3px;\n",
       "  color:#000000;\n",
       "}\n",
       "\n",
       ".header{\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       ".header > * {\n",
       "  margin-inline: 20px;\n",
       "}\n",
       "\n",
       ".flex-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding:10px;\n",
       "  border: 1px solid #27272A;\n",
       "}\n",
       "\n",
       ".flex-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "progress[value] {\n",
       "  --background: #6D6D6D;\n",
       "  -webkit-appearance: none;\n",
       "  -moz-appearance: none;\n",
       "  appearance: none;\n",
       "  border: none;\n",
       "  height: 4px;\n",
       "  margin: 0 10px;\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--background);\n",
       "}\n",
       "\n",
       "progress[value]::-webkit-progress-value {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "progress[value]::-moz-progress-bar {\n",
       "  border-radius: 10em;\n",
       "  background: var(--color);\n",
       "}\n",
       "\n",
       "label {\n",
       "  font-size: 20px;\n",
       "  font-weight: bold;\n",
       "  display: block;\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".tab {\n",
       "  overflow: hidden;\n",
       "  border: 1px solid #27272A;\n",
       "  background-color: #27272A;\n",
       "}\n",
       "\n",
       ".tab button {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size:1.2rem;\n",
       "}\n",
       "\n",
       ".tab div {\n",
       "  background-color: inherit;\n",
       "  float: left;\n",
       "  border: none;\n",
       "  outline: none;\n",
       "  cursor: pointer;\n",
       "  padding: 14px 16px;\n",
       "  transition: 0.3s;\n",
       "  color: #ffffff;\n",
       "  font-size: 1.2rem;\n",
       "}\n",
       "\n",
       ".tab button:hover {\n",
       "  background-color: #18181B;\n",
       "}\n",
       "\n",
       ".tab-title{\n",
       "  font-size: 1.5rem;\n",
       "  font-weight: bold;\n",
       "  margin-bottom:-5px;\n",
       "}\n",
       "\n",
       ".tab button.active {\n",
       "  background-color: #18181B;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  border-bottom: 1px solid #18181B;\n",
       "  border-left: 1px solid #6b7280;\n",
       "  border-right: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       ".tabcontent {\n",
       "  display: none;\n",
       "  padding: 6px 12px;\n",
       "  background: #18181B;\n",
       "  border: 1px solid #27272A;\n",
       "  border-top: 1px solid #6b7280;\n",
       "  margin-top: -2px;\n",
       "}\n",
       "\n",
       "#gsk-advice {\n",
       "  display: flex;\n",
       "  justify-content: center;\n",
       "}\n",
       "\n",
       "#gsk-metrics{\n",
       "  width:100%;\n",
       "}\n",
       "\n",
       "#recommendation {\n",
       "  margin-top: 20px;\n",
       "  padding: 20px;\n",
       "  border-radius: 10px;\n",
       "  background-color: #e1ce86;\n",
       "  color: #27272A;\n",
       "  width:95%;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  font-size: 12pt;\n",
       "}\n",
       "\n",
       ".separator {\n",
       "  margin: 20px 0;\n",
       "}\n",
       "\n",
       ".separator-border {\n",
       "  margin: 20px 0;\n",
       "  border-bottom: 1px solid #6b7280;\n",
       "}\n",
       "\n",
       "#gsk-rag{\n",
       "  margin: 32px 28px;\n",
       "  padding: 12px 24px;\n",
       "  background-color: #111516;\n",
       "}\n",
       "\n",
       ".section-container {\n",
       "  margin-bottom: 32px;\n",
       "}\n",
       "\n",
       "  .components-container {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    align-items: flex-start;\n",
       "    gap: 0 32px;\n",
       "  }\n",
       "\n",
       "    .component-card {\n",
       "      background-color: #14191B;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .component-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #B1B1B1;\n",
       "      padding-bottom: 8px;\n",
       "    }\n",
       "\n",
       "    .component-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "    }\n",
       "      \n",
       "      .text-green {\n",
       "        color: #04B543;\n",
       "      }\n",
       "      \n",
       "      .text-orange {\n",
       "        color: #E76E0F;\n",
       "      }\n",
       "      \n",
       "      .text-red {\n",
       "        color: #EA3829;\n",
       "      }\n",
       "\n",
       "      .tooltip {\n",
       "        position: relative;\n",
       "        display: inline-block;\n",
       "      }\n",
       "\n",
       "      .tooltip .tooltiptext {\n",
       "        visibility: hidden;\n",
       "        width: 120px;\n",
       "        background-color: #464646;\n",
       "        color: #E6E6E6;\n",
       "        text-align: center;\n",
       "        border-radius: 6px;\n",
       "        position: absolute;\n",
       "        z-index: 1;\n",
       "        top: 150%;\n",
       "        left: 50%;\n",
       "        margin-left: -60px;\n",
       "        font-size: 12px;\n",
       "        padding: 12px;\n",
       "      }\n",
       "      \n",
       "      .tooltip .tooltiptext::after {\n",
       "        content: \"\";\n",
       "        position: absolute;\n",
       "        bottom: 100%;\n",
       "        left: 50%;\n",
       "        margin-left: -5px;\n",
       "        border-width: 5px;\n",
       "        border-style: solid;\n",
       "        border-color: transparent transparent black transparent;\n",
       "      }\n",
       "      \n",
       "      .tooltip:hover .tooltiptext {\n",
       "        visibility: visible;\n",
       "      }\n",
       "\n",
       "    .overall-card {\n",
       "      background-color: #026836;\n",
       "      border-radius: 16px;\n",
       "      padding: 28px 32px 32px 32px;\n",
       "      display: flex;\n",
       "      flex-flow: column;\n",
       "      align-items: center;\n",
       "      justify-content: center;\n",
       "      margin-top: 32px;\n",
       "      flex-grow: 1;\n",
       "    }\n",
       "  \n",
       "    .overall-title {\n",
       "      font-size: 12px;\n",
       "      font-weight: 500;\n",
       "      color: #E6E6E6;\n",
       "      padding: 14px 0 8px 0;\n",
       "      text-transform: uppercase;\n",
       "     }\n",
       "  \n",
       "    .overall-value {\n",
       "      font-size: 32px;\n",
       "      font-weight: 500;\n",
       "      padding-bottom: 12px;\n",
       "      color: #E6E6E6;\n",
       "    }\n",
       "\n",
       ".section-title {\n",
       "  font-size: 12px;\n",
       "  color: #B1B1B1;\n",
       "  margin-bottom: 20px;\n",
       "  text-align: left;\n",
       "  width: 100%;\n",
       "}\n",
       "\n",
       ".section-content {\n",
       "  color: #E6E6E6;\n",
       "  font-size: 20px;\n",
       "  line-height: 1.5;\n",
       "}\n",
       "\n",
       ".section-card {\n",
       "  background-color: #14191B;\n",
       "  border-radius: 16px;\n",
       "  padding: 28px 32px 32px 32px;\n",
       "  display: flex;\n",
       "  flex-flow: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       ".correctness-indicator{\n",
       "  padding: 20px;\n",
       "  border-radius: 50px;\n",
       "  font-size: 16pt;\n",
       "  box-shadow: 0 4px 8px 0 #000000, 0 6px 20px 0 #000000;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".metric-title{\n",
       "  margin: -2px;\n",
       "  border-bottom: none;\n",
       "}\n",
       "\n",
       ".hist-row {\n",
       "  display: flex;\n",
       "  flex-direction: row;\n",
       "  padding: 10px;\n",
       "  justify-content: space-around;\n",
       "  width: 85%;\n",
       "}\n",
       "\n",
       ".hist-row>div {\n",
       "  flex: auto;\n",
       "  box-sizing: border-box;\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  justify-content: center;\n",
       "  align-items: center;\n",
       "  padding-left: 1%;\n",
       "  padding-right: 1%;\n",
       "}\n",
       "\n",
       ".tab-row{\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "}\n",
       "\n",
       "#component-table{\n",
       "  width:50%;\n",
       "  margin-top: 10px;\n",
       "}\n",
       "\n",
       ".green{\n",
       "  background-color: #0a980a;\n",
       "}\n",
       "\n",
       ".orange {\n",
       "  background-color: #e5b62a;\n",
       "}\n",
       "\n",
       ".red {\n",
       "  background-color: #ba0e0e;\n",
       "}\n",
       "\n",
       ".progress-green {\n",
       "  --color: #04B543;\n",
       "}\n",
       "\n",
       ".progress-orange {\n",
       "  --color: #E76E0F;\n",
       "}\n",
       "\n",
       ".progress-red {\n",
       "  --color: #EA3829;\n",
       "}\n",
       "\n",
       ".corr-plot{\n",
       "  flex: 1;\n",
       "  padding-left: 2%;\n",
       "}\n",
       "\n",
       ".tooltip-text {\n",
       "  position: absolute;\n",
       "  display: none;\n",
       "  visibility: hidden;\n",
       "  z-index: 1;\n",
       "  top: 100%;\n",
       "  left: 0%;\n",
       "  width: 100%;\n",
       "  color: white;\n",
       "  font-size: 12px;\n",
       "  background-color: #2d3d4c;\n",
       "  border-radius: 10px;\n",
       "  padding: 10px 15px 10px 15px;\n",
       "}\n",
       "\n",
       "#fade {\n",
       "  opacity: 1;\n",
       "  transition: opacity 0.5s;\n",
       "}\n",
       "\n",
       "#delay {\n",
       "  opacity: 0;\n",
       "  transition: opacity 0.2s;\n",
       "  transition-delay: 1s;\n",
       "}\n",
       "\n",
       "td {\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "tr:hover .tooltip-text {\n",
       "  display: block;\n",
       "  visibility: visible;\n",
       "}\n",
       "\n",
       ".tr:hover #fade {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".tr:hover #delay {\n",
       "  opacity: 1;\n",
       "}\n",
       "\n",
       ".callout {\n",
       "  padding: 0.5rem 1rem 0.5rem 3rem;\n",
       "  background: #D9EDF9;\n",
       "  border: 3px solid #0088D1;\n",
       "  color: #272eb5;\n",
       "  position: relative;\n",
       "  max-width: 40rem;\n",
       "  border-radius: 10px;\n",
       "  margin-top: 10%;\n",
       "  font-size: 11pt;\n",
       "}\n",
       "\n",
       ".callout-icon {\n",
       "  content: \"\";\n",
       "\n",
       "  /* SVG via a data URI! */\n",
       "  background-size: cover;\n",
       "  width: 1.5rem;\n",
       "  height: 1.5rem;\n",
       "  display: block;\n",
       "  position: absolute;\n",
       "  left: 0.9rem;\n",
       "  top: 1.1rem;\n",
       "}\n",
       "\n",
       ".callout-icon svg{\n",
       "  fill: #016ca7;\n",
       "}\n",
       ".callout p+p {\n",
       "  margin-top: 1em;\n",
       "}\n",
       "\n",
       ".callout a {\n",
       "  color: #272eb5;\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       "#gsk-logo {\n",
       "  padding-top: 10px;\n",
       "}\n",
       "</style>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-3.3.4.min.js\" integrity=\"sha384-5QIrjQuyo4I/x6DK/Sau33lcA3hT2TCZGr9vbk+2ebd7Da6FnR1amdM+9B5xOrSf\" crossorigin=\"anonymous\"></script>\n",
       "<script src=\"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.3.4.min.js\" integrity=\"sha384-tXTWPp/bAKa+K9RPuXh7DNvye0Mv+P+6y4rAMVy+pWapsnXg9UG7g20WZ0N4i28A\" crossorigin=\"anonymous\"></script>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class=\"main\">\n",
       "    <div id=\"gsk-rag\" class=\"dark:text-white dark:bg-zinc-800 rounded border border-gray-500\">\n",
       "        <div class=\"header border-b border-b-gray-500\">\n",
       "            \n",
       "                <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"60\" height=\"30\" viewBox=\"0 0 30 15\" fill=\"none\" id=\"gsk-logo\">\n",
       "                    <path fill=\"#fff\" fill-rule=\"evenodd\"\n",
       "                        d=\"M22.504 1.549a4.196 4.196 0 0 1 2.573-.887v.002a3.783 3.783 0 0 1 2.706 1.086 3.783 3.783 0 0 1 1.126 2.69 3.771 3.771 0 0 1-1.126 2.69 3.77 3.77 0 0 1-2.706 1.085l-4.794.011-2.533 3.467L8.203 15l2.881-3.335a9.829 9.829 0 0 1-4.663-1.68H3.185L0 7.163h3.934C4.263 3.165 8.187 0 12.96 0c2.24 0 4.489.696 6.175 1.909a7.423 7.423 0 0 1 1.882 1.919 4.194 4.194 0 0 1 1.487-2.28ZM7.05 3.249l3.91 3.915h1.505L7.89 2.584a7.773 7.773 0 0 0-.84.665Zm4.079-2.008 5.923 5.923h1.503l-6.086-6.087c-.45.023-.898.078-1.34.164ZM4.574 8.226h-1.77l.784.693h1.584a8.454 8.454 0 0 1-.598-.693Zm9.479 0H5.984c1.469 1.477 3.656 2.377 5.977 2.422l2.092-2.422Zm-2.458 4.472 5.492-1.902 1.878-2.569h-3.508l-3.862 4.47Zm10.361-5.552h3.265a2.714 2.714 0 0 0 1.747-4.648 2.711 2.711 0 0 0-1.888-.773 3.127 3.127 0 0 0-3.123 3.124v2.297Zm3.659-3.73a.677.677 0 1 1-.134 1.348.677.677 0 0 1 .134-1.348Z\"\n",
       "                        clip-rule=\"evenodd\" />\n",
       "                </svg>\n",
       "            <h1>RAG Evaluation Toolkit</h1>\n",
       "        </div>\n",
       "        \n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"components-container\">\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">GENERATOR</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        66.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Generator is the LLM inside the RAG to generate the answers.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=66.0 class=\" progress-orange \">66.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">RETRIEVER</div>\n",
       "                    <div class=\"component-value tooltip  text-orange \">\n",
       "                        55.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Retriever fetches relevant documents from the knowledge base according to a user query.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=55.00000000000001 class=\" progress-orange \">55.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">REWRITER</div>\n",
       "                    <div class=\"component-value tooltip  text-red \">\n",
       "                        26.67%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Rewriter modifies the user query to match a predefined format or to include the context from the chat history.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=26.666666666666668 class=\" progress-red \">26.67%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">ROUTING</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        100.0%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The Router filters the query of the user based on his intentions (intentions detection).</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=100.0 class=\" progress-green \">100.0%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"component-card\">\n",
       "                    <div class=\"component-title\">KNOWLEDGE_BASE</div>\n",
       "                    <div class=\"component-value tooltip  text-green \">\n",
       "                        81.52%\n",
       "                            <span class=\"tooltiptext\" id=\"fade\">The knowledge base is the set of documents given to the RAG to generate the answers. Its scores is computed differently from the other components: it is the difference between the maximum and minimum correctness score across all the topics of the knowledge base.</span>\n",
       "                    </div>\n",
       "                    <div class=\"component-bar\">\n",
       "                        <progress max=\"100\" value=81.52173913043478 class=\" progress-green \">81.52%</progress>\n",
       "                    </div>\n",
       "                </div>\n",
       "                \n",
       "                <div class=\"overall-card\">\n",
       "                    <div class=\"overall-title\">Overall Correctness Score</div>\n",
       "                    <div class=\"overall-value\">62%</div>\n",
       "                </div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">RECOMMENDATION</div>\n",
       "                <span class=\"section-content\">To improve your RAG system, focus on improving the Rewriter and the Router components, as they are significantly underperforming in handling conversational and distracting-type questions. Additionally, enhance the retriever&#39;s performance for image composition techniques for better topical coverage.</span>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">CORRECTNESS BY TOPIC</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"72bcf379-aaac-4eb6-acef-f2fc7c588817\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1190\",\"attributes\":{\"height\":350,\"width_policy\":\"max\",\"x_range\":{\"type\":\"object\",\"name\":\"DataRange1d\",\"id\":\"p1192\",\"attributes\":{\"start\":0}},\"y_range\":{\"type\":\"object\",\"name\":\"FactorRange\",\"id\":\"p1199\",\"attributes\":{\"factors\":[\"Image Composition Techniques\",\"Image Segmentation Techniques\",\"Others\"]}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1200\"},\"y_scale\":{\"type\":\"object\",\"name\":\"CategoricalScale\",\"id\":\"p1201\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1197\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1219\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1187\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1188\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1189\"},\"data\":{\"type\":\"map\",\"entries\":[[\"correctness\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"ZCELWchCTEAAAAAAAMBSQIqd2ImdWFBA\"},\"shape\":[3],\"dtype\":\"float64\",\"order\":\"little\"}],[\"metadata_values\",[\"Image Composition Techniques\",\"Image Segmentation Techniques\",\"Others\"]],[\"colors\",[\"#a50026\",\"#006837\",\"#fef8b3\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1220\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1221\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1216\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1217\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1218\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"#1f77b4\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"#14191B\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1228\",\"attributes\":{\"data_source\":{\"id\":\"p1187\"},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1229\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1230\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1225\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1226\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"HBar\",\"id\":\"p1227\",\"attributes\":{\"y\":{\"type\":\"field\",\"field\":\"metadata_values\"},\"height\":{\"type\":\"value\",\"value\":0.85},\"right\":{\"type\":\"field\",\"field\":\"correctness\"},\"line_color\":{\"type\":\"value\",\"value\":\"white\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"value\",\"value\":\"#78BBFA\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1238\",\"attributes\":{\"visible\":false,\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1232\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1233\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1234\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",[0]],[\"y\",[0]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1239\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1240\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1235\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1236\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.1,\"line_width\":2,\"line_dash\":[6]}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Line\",\"id\":\"p1237\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"line_color\":\"#EA3829\",\"line_alpha\":0.2,\"line_width\":2,\"line_dash\":[6]}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1198\",\"attributes\":{\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1212\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":[[\"topic\",\"@metadata_values\"],[\"Correctness\",\"@correctness{0.00}\"]]}}]}},\"toolbar_location\":null,\"left\":[{\"type\":\"object\",\"name\":\"CategoricalAxis\",\"id\":\"p1207\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"CategoricalTicker\",\"id\":\"p1208\"},\"formatter\":{\"type\":\"object\",\"name\":\"CategoricalTickFormatter\",\"id\":\"p1209\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1210\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1202\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1203\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1204\"},\"axis_label\":\"Correctness (%)\",\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1205\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1206\",\"attributes\":{\"axis\":{\"id\":\"p1202\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1211\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1207\"},\"grid_line_color\":\"#E0E0E0\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Span\",\"id\":\"p1231\",\"attributes\":{\"location\":62.264150943396224,\"dimension\":\"height\",\"line_color\":\"#EA3829\",\"line_width\":2,\"line_dash\":[6]}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1241\",\"attributes\":{\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1242\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Correctness on the entire Testset\"},\"renderers\":[{\"id\":\"p1238\"}]}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"72bcf379-aaac-4eb6-acef-f2fc7c588817\",\"roots\":{\"p1190\":\"cfe02272-070c-4ea3-9adb-15aca4b3a73f\"},\"root_ids\":[\"p1190\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"cfe02272-070c-4ea3-9adb-15aca4b3a73f\" data-root-id=\"p1190\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "                <div class=\"section-title\">KNOWLEDGE BASE OVERVIEW</div>\n",
       "                    <script type=\"text/javascript\">\n",
       "        (function() {\n",
       "  const fn = function() {\n",
       "    Bokeh.safely(function() {\n",
       "      (function(root) {\n",
       "        function embed_document(root) {\n",
       "        const docs_json = '{\"f58e476c-bf07-4d6a-842b-80cfc5e4570f\":{\"version\":\"3.4.3\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"Tabs\",\"id\":\"p1121\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"tabs\":[{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1051\",\"attributes\":{\"title\":\"Topic exploration\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1004\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1013\",\"attributes\":{\"start\":19.41671133041382,\"end\":27.821502685546875}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1014\",\"attributes\":{\"start\":-6.514929533004761,\"end\":1.4252317324280739}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1015\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1016\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1011\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1044\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1001\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1002\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1003\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"V9u6QZrRr0EMq7dBqpq4QSedtkFcCqdBUaqqQZuiukEyG75B/1+zQdrZsUFTTrdBhGq0QVMjukET07hBIOS+QdCssEGuoKxBBTetQfODp0EPk6RBpPujQXy1o0HUM6BBU8ChQRmyp0FJyrNBCF+dQSE1nkGRNapB0M2rQRI9rEHJ4a1BoGCpQSi1rEGUdaNBxfunQf7zskE9abZB1qu/QYtMwEFgZcVBKR/GQWjyv0FZk7FBTDytQa3Es0E4ErJBXTy3QYCdvkHO48BBIpe4QQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"zc59wA87e8CuF2jANFyXwBGwgMD1l0zAUwVRwO9oUcCysCHAD4NWwL2wmL8iV7a/i8sgvRXLsr65WIa/Y9Qpv10TF8CD5s+/1D8tvwNT3r4NIdC//Pp5v8DCH8BZP0TAqAwNwBnmHsCzJNO+swofwC7dEcCeWPK/BaLQPfNKi7/C3ay+Z7SNwA2XaMBXVIPALaJ7wOlTmMBWIabAtFeQwJWOgMDMBGbAfe4ywHV6WcCUBEPAob0uwFiF7b8HEYnAGYsTwM0F3b8FWu+/x+0EwA==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Segmentation Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Image Composition Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\"]],[\"id\",[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51]],[\"content\",[\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220720964\\\\nIntelligent scissors for image composition\\\\nConference Paper \\\\u00b7 January 1995\\\\nDOI: 10.1145/218380.218442\\\\u00a0\\\\u00b7\\\\u00a0Source: DBLP\\\\nCITATIONS\\\\n812\\\\nREADS\\\\n2,545\\\\n2 authors, including:\\\\nEric N. Mortensen\\\\nLucidyne Technologies, Inc.\\\\n34 PUBLICATIONS\\\\u00a0\\\\u00a0\\\\u00a03,134 CITATIONS\\\\u00a0\\\\u00a0\\\\u00a0\\\\nSEE PROFILE\\\\nAll content following this page was uploaded by Eric N. Mortensen on 01 June 2014.\\\\nThe user has requested enhancement of...\",\"Abstract\\\\nWe present a new, interactive tool called Intelligent Scissors\\\\nwhich we use for image segmentation and composition.  Fully auto-\\\\nmated segmentation is an unsolved problem, while manual tracing\\\\nis inaccurate and laboriously unacceptable.  However, Intelligent\\\\nScissors allow objects within digital images to be extracted quickly\\\\nand accurately using simple gesture motions with a mouse.  When\\\\nthe gestured mouse position comes in proximity to an object edge,\\\\nalive-wire boundary  \\\\u201csnaps\\\\u201d to, ...\",\"lowed, rather than simply the strongest edge in the neighborhood.\\\\nBoundary cooling automatically freezes unchanging segments and\\\\nautomates input of additional seed points. Cooling also allows the\\\\nuser to be much more free with the gesture path, thereby increasing\\\\nthe ef\\\\ufb01ciency and \\\\ufb01nesse with which boundaries can be extracted.\\\\nExtracted objects can be scaled, rotated, and composited using\\\\nlive-wire masks and spatial frequency equivalencing .  Frequency\\\\nequivalencing is performed by applying a Bu...\",\"manipulation techniques, has also been used to realistically blend\\\\nold \\\\ufb01lm into a new script.  The goal of image composition is to com-\\\\nbine objects or regions from various still photographs or movie\\\\nframes to create a seamless, believable, image or image sequence\\\\nwhich appears convincing and real.  Fig. 9(d) shows a believable\\\\ncomposition created by combining objects extracted from three\\\\nimages, Fig. 9(a-c).  These objects were digitally extracted and\\\\ncombined in a few minutes using a new, inte...\",\"control the segmentation process.\\\\nThis paper describes a new, interactive, digital image segmenta-\\\\ntion tool called \\\\u201cIntelligent Scissors\\\\u201d which allows rapid object\\\\nextraction from arbitrarily complex backgrounds. Intelligent Scis-\\\\nsors boundary detection formulates discrete dynamic programming\\\\n(DP) as a two-dimensional graph searching problem.  Presented as\\\\npart of this tool areboundary cooling  and on-the-\\\\ufb02y training , which\\\\nreduce user input and dynamically adapt the tool to speci\\\\ufb01c types of\\\\n...\",\"tions, use an interactively selected seed point to \\\\u201cgrow\\\\u201d a region by\\\\nadding adjacent neighboring pixels.  Since this type of region grow-\\\\ning does not provide interactive visual feedback, resulting region\\\\nboundaries must usually be edited or modi\\\\ufb01ed.\\\\nOther popular boundary de\\\\ufb01nition methods use active contours\\\\nor snakes[1, 5, 8, 15] to improve a manually entered rough approx-\\\\nimation.  After being initialized with a rough boundary approxima-\\\\ntion, snakes iteratively adjust the boundary points i...\",\"boundary will look like when the rough approximation is input. If\\\\nthe resulting boundary is not satisfactory, the process must be\\\\nrepeated or the boundary must be manually edited.  W e provide a\\\\ndetailed comparison of snakes and Intelligent Scissors in section\\\\n3.6.\\\\nAnother class of image segmentation techniques use a graph\\\\nsearching formulation of DP (or similar concepts) to \\\\ufb01nd globally\\\\noptimal boundaries [2, 4, 10, 11, 14].  These techniques differ from\\\\nsnakes in that boundary points are gener...\",\"of freedom within a window about the two-dimensional boundary\\\\ntemplate.  Thus, boundary extraction using previous graph search-\\\\ning techniques is non-interactive (beyond template speci\\\\ufb01cation),\\\\nlosing the bene\\\\ufb01ts of further human guidance and expertise.\\\\nIntelligent Scissors for Image Composition\\\\nEric N. Mortensen 1       William A. Barrett 2\\\\nBrigham Y oung University\\\\n1 enm @ cs.byu.edu, Dept. of Comp. Sci., BYU, Provo, UT  84602 (801)378-7605\\\\n2 barrett @ cs.byu.edu, Dept. of Comp. Sci., BYU, Pro...\",\"permission and/or a fee. \\\\n\\\\u00a91995  ACM-0-89791-701-4/95/008\\\\u0085$3.50\",\"The most important difference between previous boundary \\\\ufb01nd-\\\\ning techniques and Intelligent Scissors presented here lies not in the\\\\nboundary de\\\\ufb01ning criteria per se\\\\u00b4, but in themethod  of interaction.\\\\nNamely, previous methods exhibit a pattern of boundary approxi-\\\\nmation followed by boundary re\\\\ufb01nement, whereas Intelligent Scis-\\\\nsors  allow the user tointeractively select  the most suitable\\\\nboundary from a set of all  optimal boundaries emanating from a\\\\nseed point.  In addition, previous approach...\",\"applied to image boundary \\\\ufb01nding, the graph search consists of\\\\n\\\\ufb01nding the globally optimal path from a start pixel to a goal pixel--\\\\nin particular, pixels represent nodes and edges are created between\\\\neach pixel and its 8 neighbors.  For this paper, optimality is de\\\\ufb01ned\\\\nas the minimum cumulative cost path from a start pixel to a goal\\\\npixel where the cumulative cost of a path is the sum of the local\\\\nedge (or link) costs on the path.\\\\n3.1. Local Costs\\\\nSince a minimum cost path should correspond to ...\",\"(1)\\\\nwhere each \\\\u03c9  is the weight of the corresponding feature function.\\\\n(Empirically, weights of \\\\u03c9 Z  = 0.43, \\\\u03c9 D  = 0.43, and \\\\u03c9 G  = 0.14 seem\\\\nto work well in a wide range of images.)\\\\nThe laplacian zero-crossing is a binary edge feature used for edge\\\\nlocalization [7, 9]. Convolution of an image with a laplacian kernel\\\\napproximates the 2nd  partial derivative of the image.  The laplacian\\\\nimage zero-crossing corresponds to points of maximal (or minimal)\\\\ngradient magnitude.  Thus, laplacian zero-cr...\",\"single-pixel wide cost \\\\u201ccanyons\\\\u201d used for boundary localization.\\\\nImage Feature Formulation\\\\nLaplacian Zero-Crossing fZ\\\\nGradient Magnitude fG\\\\nGradient Direction fD\\\\nl pq ,()\\\\u03c9 Z f Z q()\\\\u22c5\\\\u03c9 D f D pq ,()\\\\u22c5\\\\u03c9 G f G q()\\\\u22c5++=\\\\nf Z q()\\\\n0 ;\\\\u0011\\\\u0011\\\\u0011if\\\\u0011 I L q() 0=\\\\n1 ;\\\\u0011\\\\u0011\\\\u0011if\\\\u0011 I L q() 0\\\\u2260{=\\\\nSince the laplacian zero-crossing creates a binary feature, fZ ( q )\\\\ndoes not distinguish between strong, high gradient edges and weak,\\\\nlow gradient edges.  However, gradient magnitude provides a direct\\\\ncorrelation between edge strengt...\",\"imum gradient at unity,fG ( q ) is scaled by 1 if q  is a diagonal neigh-\\\\nbor to p  and by 1/ \\\\u221a 2 if q  is a horizontal or vertical neighbor.\\\\nThe gradient direction adds a smoothness constraint to the\\\\nboundary by associating a high cost for sharp changes in boundary\\\\ndirection.  The gradient direction is the unit vector de\\\\ufb01ned byI x  and\\\\nI y .  Letting D ( p ) be the unit vector perpendicular (rotated 90 degrees\\\\nclockwise) to the gradient direction at point p  (i.e., for D ( p ) = ( I y ( p ),\\\\n-I...\",\"two pixels that have similar gradient directions but are perpendicu-\\\\nlar, or near perpendicular, to the link between them.  Therefore, the\\\\ndirection feature cost is low when the gradient direction of the two\\\\npixels are similar to each other and the link between them.\\\\n3.2. Two-Dimensional Dynamic Programming\\\\nAs mentioned, dynamic programming can be formulated as a\\\\ndirected graph search for an optimal path.  This paper utilizes an\\\\noptimal graph search similar to that presented by Dijkstra [6] and\\\\n...\",\"nates the directed sampling and searching restrictions of previous\\\\nimplementations, thereby allowing boundaries of arbitrary com-\\\\nGI x\\\\n2 I y\\\\n2+=\\\\nf G\\\\nma x G() G\\\\u2212\\\\nma x G()\\\\n1\\\\nG\\\\nma x G()\\\\u2212==\\\\nf D pq ,()\\\\n1\\\\n\\\\u03c0\\\\nd p pq ,()[]cos 1\\\\u2212 d q pq ,()[]cos 1\\\\u2212+{}=\\\\nd p pq ,() D &#x27; p() Lp q ,()\\\\u22c5=\\\\nd q pq ,() Lp q ,() D &#x27; q()\\\\u22c5=\\\\nLp q ,()\\\\nqp \\\\u0011\\\\u0011\\\\u0011if\\\\u0011 D &#x27; p() qp \\\\u2212() 0\\\\u2265\\\\u22c5;\\\\u2212\\\\npq \\\\u0011\\\\u0011\\\\u0011if\\\\u0011 D &#x27; p() qp \\\\u2212() 0&lt;\\\\u22c5;\\\\u2212{=\",\"plexity to be extracted.  Second, the edge features used here are\\\\nmore robust and comprehensive than previous implementations: we\\\\nmaximize over different gradient kernels sizes to encompass the\\\\nvarious edge types and scales while simultaneously attempting to\\\\nbalance edge detail with noise suppression [7], and we use the lapla-\\\\ncian zero-crossing for boundary localization and \\\\ufb01ne detail live-\\\\nwire \\\\u201csnapping\\\\u201d.  Third, the discrete, bounded nature of the local\\\\nedge costs permit the use of a special...\",\"Algorithm:  Live-Wire 2-D DP graph search.\\\\nInput:\\\\ns {Start (or seed) pixel.}\\\\nl( q , r ) {Local cost function for link between pixels q and r.}\\\\nData Structures:\\\\nL {List of active pixels sorted by total cost (initially empty).}\\\\nN( q ) {Neighborhood set of q (contains 8 neighbors of pixel).}\\\\ne( q ) {Boolean function indicating if q has been expanded/processed.}\\\\ng( q ) {Total cost function from seed point to q.}\\\\nOutput:\\\\np {Pointers from each pixel indicating the minimum cost path.}\\\\nAlgorithm:\\\\ng( s )...\",\"g( r ) \\\\u2190 g tmp ; {  assign neighbor\\\\u2019s total cost, }\\\\np( r ) \\\\u2190 q ; {  set (or reset) back pointer, }\\\\nL \\\\u2190 r ; {  and place on (or return to) }\\\\nend {  active list.}\\\\nend\\\\nend\\\\nNotice that since the active list is sorted, when a new , lower cumu-\\\\nlative cost is computed for a pixel already on the list then that point\\\\nmust be removed from the list in order to be added back to the list\\\\nwith the new lower cost.  Similar to adding a point to the sorted list,\\\\nthis operation is also performed in constant time...\",\"pointer map after expanding the seed point (with a cumulative cost\\\\nof zero).  Notice how the diagonal local costs have been scaled by\\\\nEuclidean distance (consistent with the gradient magnitude cost\\\\nfeature described previously).  Though complicating the example,\\\\nweighing by Euclidean distance is necessary to demonstrate that the\\\\ncumulative costs to points currently on the active list can change if\\\\neven lower cumulative costs are computed from as yet unexpanded\\\\nneighbors.  This is demonstrated in...\",\"\\\\u201cwavefront\\\\u201d of active points emanating from the initial start point,\\\\ncalled the seed point, and that the wavefront grows out faster where\\\\nthere are lower costs.\\\\n3.3. Interactive \\\\u201cLive-Wire\\\\u201d Segmentation T ool\\\\nOnce the optimal path pointers are generated, a desired boundary\\\\nsegment can be chosen dynamically via a \\\\u201cfree\\\\u201d point.  Interactive\\\\nmovement of the free point by the mouse cursor causes the bound-\\\\nary to behave like a live-wire as it adapts to the new minimum cost\\\\npath by following the opti...\",\"1 1 5283457259\\\\n1 2 421563248 1 2\\\\n1 0 975985378 1 5\\\\n(a)\\\\n(e)\\\\n(f)\\\\nFigure 1:  (a) Initial local cost matrix.  (b) Seed point (shaded)\\\\nexpanded.  (c) 2 points (shaded) expanded.  (d) 5 points (shaded)\\\\nexpanded.  (e) 47 points expanded.  (f) Finished total cost and path\\\\nmatrix with two of many paths (free points shaded) indicated.\\\\n(c) (d)\\\\n41 35 31 29 35\\\\n38 29 23 22 24 29\\\\n28 18 16 21 28 37\\\\n18 12 16 27 38\\\\n1 4 8 1 32 02 93 55 2 3 52 83 2\\\\n14 6 6 12 14 22 28 35 27 25 31\\\\n1 8 72959 1 4 2 1 1 8 2 3 3 2\\\\n1 6 40...\",\"to the seed point.  By constraining the seed point and free points to\\\\nlie near a given edge, the user is able to interactively \\\\u201csnap\\\\u201d and\\\\n\\\\u201cwrap\\\\u201d the live-wire boundary around the object of interest.  Figure\\\\n2 demonstrates how a live-wire boundary segment adapts to\\\\nchanges in the free point (cursor position) by latching onto more\\\\nand more of an object boundary.  Speci\\\\ufb01cally, note the live-wire\\\\nsegments corresponding to user-speci\\\\ufb01ed free point positions at\\\\ntimest0 , t1 , and t2 .  Although Fig. 2...\",\"new seed point while effectively \\\\u201ctieing off\\\\u201d the boundary com-\\\\nputed up to the new seed point.\\\\nNote again that optimal paths are computed from the seed point\\\\nto all  points in the image (since the 2-D DP graph search produces\\\\na minimum cost spanning tree of the image [6]).  Thus, by selecting\\\\na free point with the mouse cursor, the interactive live-wire tool is\\\\nsimply selecting an optimal boundary segment from a large collec-\\\\ntion of optimal paths.\\\\nSince each pixel (or free point) de\\\\ufb01nes only o...\",\"and tedious.  If a seed point is not localized to an object edge then\\\\nspikes results on the segmented boundary at those seed points (since\\\\nFigure 2:  Image demonstrating how the live-wire segment adapts and\\\\nsnaps to an object boundary as the free point moves (via cursor move-\\\\nment).  The path of the free point is shown in white.  Live-wire segments\\\\nfrom previous free point positions (t0 , t 1 , and t 2 ) are shown in green.\\\\n(a) (b)\\\\nFigure 3:  Comparison of live-wire without (a) and with (b) cool...\",\"from 1\\\\u00d7 1 (resulting in no cursor snap) to 15 \\\\u00d7 15 (where the cursor\\\\ncan snap as much as 7 pixels in both x  and y ).  Thus, as the mouse\\\\ncursor is moved by the user, it snaps or jumps to a neighborhood\\\\npixel representing a \\\\u201cgood\\\\u201d static edge point.\\\\n3.4. Path Cooling\\\\nGenerating closed boundaries around objects of interest can\\\\nrequire as few as two seed points (for reasons given previously).\\\\nSimple objects typically require two to \\\\ufb01ve seed points but complex\\\\nobjects may require many more.  Even w...\",\"shows that without training, high gradients are favored since they map\\\\nto low costs.  However , with training, the dynamic cost map (d) favors\\\\ngradients similar to those sampled from the previous boundary segment.\\\\nM G\\\\nn G\\\\n0\\\\n0 Cost\\\\nGradient Magnitude\\\\nM G\\\\nn G\\\\n0\\\\n0 Cost\\\\nGradient Magnitude\\\\nStatic Cost Map Dynamic Cost Map\\\\n(c) (d)\",\"Automatic seed point generation relieves the user from precise\\\\nmanual placement of seed points by automatically selecting a pixel\\\\non the current active boundary segment to be a new seed point.\\\\nSelection is based on \\\\u201cpath cooling\\\\u201d which in turn relies on path\\\\ncoalescence. Though a single minimum cost path exists from each\\\\npixel to a given seed point, many paths \\\\u201ccoalesce\\\\u201d and share por-\\\\ntions of their optimal path with paths from other pixels.  Due to\\\\nBellman\\\\u2019s Principle of Optimality [3], if any...\",\"and only change local to the free point.  As the free point moves far-\\\\nther and farther away from the seed point, the portion of the active\\\\nlive-wire boundary segment that does not change becomes longer.\\\\nNew seed points are generated at the end of a stable segment (i.e.,\\\\nthat has not changed recently).  Stability is measured by time (in\\\\nmilliseconds) on the active boundary and path coalescence (number\\\\nof times the path has been redrawn from distinct free points).\\\\nThis measure of stability provid...\",\"generated automatically via cooling.\\\\n3.5. Interactive Dynamic Training\\\\nOn occasion, a section of the desired object boundary may have\\\\na weak gradient magnitude relative to a nearby strong gradient\\\\nedge.  Since the nearby strong edge has a relatively lower cost, the\\\\nlive-wire segment snaps to the strong edge rather than the desired\\\\nweaker edge.  This can be seen in Fig. 4(a).  The desired boundary\\\\nis the woman\\\\u2019s (Harriet\\\\u2019s) cheek.  However, since part of it is so\\\\nclose to the high contrast should...\",\"separate training phase and allows the trained feature cost functions\\\\nto adaptwithin  the object being segmented as well as between\\\\nobjects in the image.  Fig. 4(b) demonstrates how a trained live-wire\\\\nsegment latches onto the edge that is similar to the previous training\\\\nsegment rather that the nearby stronger edge.\\\\nTo facilitate training and trained cost computation, a gradient\\\\nmagnitude feature map or image is precomputed by scaling the min-\\\\nimized gradient magnitude image,G &#x27;, into an intege...\",\"interactively using the live-wire tool.  To allow training to adapt to\\\\nslow (or smooth) changes in edge characteristics, the trained gradi-\\\\nent magnitude cost function is based only on the most recent or\\\\nclosest portion of the current de\\\\ufb01ned object boundary.  A training\\\\nlength, t, speci\\\\ufb01es how many of the most recent boundary pixels are\\\\nused to generate the training statistics.  A monotonically decreasing\\\\nweight function (either linearly or Gaussian based) determines the\\\\ncontribution from each o...\",\"precomputed feature maps along the closestt pixels of the edge seg-\\\\nment and increments the feature histogram element by the corre-\\\\nsponding pixel weight to generate a histogram for each feature\\\\ninvolved in training.\\\\nAfter sampling and smoothing, each feature histogram is then\\\\nscaled and inverted (by subtracting the scaled histogram values\\\\nfrom its maximum value) to create the feature cost map needed to\\\\nconvert feature values to trained cost functions.\\\\nSince training is based on learned edge cha...\",\"ary so that it can be used (if needed) in a section of the boundary\\\\nwith similar edge characteristics and then turned off before a drastic\\\\nchange occurs.\\\\n3.6 Comparison with Snakes\\\\nDue to the recent popularity of snakes and other active contours\\\\nmodels and since the interactive boundary wrapping of the live-\\\\nwire may seem similar to the \\\\u201cwiggling\\\\u201d of snakes, we highlight\\\\nwhat we feel are the similarities and their corresponding differences\\\\nbetween snakes and Intelligent Scissors.\\\\nSimilarities (c...\",\"5. Snakes and Intelligent Scissors both require interaction as part of\\\\nthe boundary segmentation process.\\\\nDif ferences (compare with corresponding similarities above):\\\\n1. The laplacian zero-crossing binary cost feature seems to have not\\\\nbeen used previously in active contours models 1  (or DP bound-\\\\nary tracking methods for that matter).\\\\n2. The active contour smoothing term is internal (i.e., based on the\\\\ncontour\\\\u2019s point positions) whereas the smoothing term for live-\\\\nwire boundaries is computed...\",\"3. Snakes are typically attracted to edge features only within the\\\\ngravity of an edge\\\\u2019s gradient energy valley whereas the live-wire\\\\nboundary can snap to strong edge features from arbitrary dis-\\\\ntances (since the 2-D DP\\\\u2019s search window is the entire image).\\\\n4. Snakes are globally optimal over the entire contour whereas live-\\\\nwire boundaries are piece-wise optimal (i.e., optimal between\\\\nseed points).  We feel this creates a desirable balance between\\\\nglobal optimality and local control.  This piec...\",\"and all previous techniques.  Snakes are interactively initialized\\\\nwith an approximate boundary contour (often requiring several\\\\nmanually placed points); this single contour is then iteratively\\\\nadjusted in an attempt to minimize an energy functional.  The live-\\\\nwire tool, on the other hand, is interactively initialized with just a\\\\nsingle seed point and it then generates, at interactive speeds, all pos-\\\\nsible optimal paths from the seed point toevery  other point in the\\\\nimage, thus, allowing the ...\",\"2-D extension to previous optimal edge tracking methods rather\\\\nthan an improvement on active contours.\\\\n4. Image Composition with Intelligent Scissors\\\\nAs mentioned, composition artists need an intelligent, interactive\\\\ntool to facilitate image component boundary de\\\\ufb01nition.  Since Intel-\\\\nligent Scissors can quickly segment object from an image, it serves\\\\nas a tool for cut and paste operations.  After object boundaries have\\\\nbeen extracted, object can be copied from the image and placed into\\\\na buffer...\",\"Image composition often requires blending an object from one\\\\nimage into another image such that the cut-out object is not in the\\\\nforeground.  This requires the composition artist to \\\\u201cslip\\\\u201d the cut-\\\\nout object behind some scene components while leaving it in front\\\\nof other components. This operation can again be performed using\\\\nthe live-wire tool to create a composition mask1 . Scene components\\\\ncan be cut out of an image to create a mask such that any additions\\\\nor changes to the scene will not af...\",\"ef\\\\ufb01ciency.\\\\n1. Similar in concept to an optical mask used in motion picture special effects.\\\\nlution of the image.  This may produce jaggies along object bound-\\\\naries in a resulting composition.  However, subpixel accuracy can\\\\nbe obtained by exploiting the signed output of the laplacian opera-\\\\ntor.  That is, the position of the object edge can be estimated to sub-\\\\npixel accuracy by using a (linearly) weighted combination of the\\\\nlaplacian pixel values on either side of the zero-crossings.\\\\nSince the...\",\"ing of the image around the pasted object boundary .  However, this\\\\ndoes not account for the fact that the edge pixels of the cut object\\\\nvery likely contain some background information from the original\\\\nimage. This is most noticeable when an object is composited into a\\\\nscene with a different background color than the object\\\\u2019s original\\\\nbackground color.  A more general solution would determine how\\\\nmuch of each edge pixel corresponds to the actual object color and\\\\nweight them accordingly when comb...\",\"often have differing spatial frequencies or contrast due to differ-\\\\nences in focus or lighting when the images were acquired.  Thus,\\\\nequivalencing of spatial frequencies and normalization of contrast\\\\nis sometimes desirable in order to produce a convincing composi-\\\\ntion.\\\\nEquivalencing of spatial frequencies is performed by matching\\\\nthe spectral content of the cut piece and the destination image in the\\\\nvicinity where it is to be pasted.  Convincing composition often\\\\nrequires the spectra of the obj...\",\"where d 0  is the distance of the cutoff frequency from the origin and\\\\nn  is the \\\\ufb01lter order.\\\\nEquivalencing of spatial frequencies is performed by \\\\ufb01rst com-\\\\nputing the fourier transforms, S ( u , v ) and I ( u , v ) of the source image\\\\nS ( x , y ) and the destination image I ( x , y ).  We then compute the log\\\\npower spectra s ( u , v ) and i( u , v ):\\\\nBy varying the two parameters d 0  and n , a least squares \\\\ufb01t can be\\\\nused to create a normalized Butterworth \\\\ufb01lter B ( u , v , d 0 &#x27;, n &#x27;) (where\\\\n...\",\"d 0 &#x27; and n &#x27; are the \\\\ufb01t parameters) matched to the spatial frequency\\\\ncharacteristics of i( u , v ).  If i( u , v ) demonstrates lower spatial fre-\\\\nquency content than s ( u , v ), the spatial frequencies between the two\\\\nimages can be equivalenced by \\\\ufb01tting B ( u , v , d 0 &#x27;, n &#x27;) to s ( u , v ).  The\\\\nequivalenced result using the inverse Fourier transform\\\\nis then pasted onto I ( x , y ).  Prior to pasting, the colors in the source\\\\nimage are scaled to the range of I ( x , y ) to account for diff...\",\"times (for a trained user) for each displayed object boundary are\\\\ngiven in the caption of each respective \\\\ufb01gure.\\\\nFigure 8 graphically compares the live-wire boundary de\\\\ufb01nition\\\\ntimes and boundary accuracy with manual tracing.  These results\\\\nshow the average time and accuracy from a study where 8 untrained\\\\nusers1  were asked to de\\\\ufb01ne the boundaries of \\\\ufb01ve objects (the two\\\\nobjects in Fig. 5, the paper clip holder and pocket knife in Fig. 6,\\\\nand the outer boundary of the spinal vertebrae in Fig. 7)....\",\"3 \\\\u00d7 3, 5 \\\\u00d7 5, 7 \\\\u00d7 7, and 9 \\\\u00d7 9 kernels), a gradient orientation calculation,\\\\na maximum gradient neighborhood search, and creation of a local\\\\n1.   Each user spent a few minutes becoming familiar with the live-wire tool as\\\\nwell as a manual tracing tool and then were asked to de\\\\ufb01ne the boundary of 5 objects\\\\nEach boundary was de\\\\ufb01ned multiple times by each user with both Intelligent Scissors\\\\nand manual tracing (to also measure intra- and inter-user reproducibility--not shown).\\\\nS \\\\u2032 xy ,() F 1\\\\u2212 Bu v d ...\",\"Hand Traced\\\\nPercentage (of Pixels \\\\u2264  Distance from \\\\u201cIdeal\\\\u201d Boundary)\\\\nAccuracy\\\\ncost map.  For color images, we maximize feature values over the\\\\nthree color bands rather than averaging.\\\\nPreviously, dynamic programming approaches to boundary\\\\ndetection were typically computationally expensive.  However, by\\\\nformulating DP as a graph search and restricting the local costs to\\\\ninteger values within a range, the 2-D DP algorithm can take advan-\\\\ntage of an O(N ) sort for N  points.  As mentioned, adding p...\",\"in 8N  cumulative cost computations for N  pixels.  Obviously, not\\\\nevery point can be expanded after all of its neighbors have. Except\\\\nfor the seed point, every point that has a cumulative cost must have\\\\nat least one neighboring point that has already been expanded.  Thus\\\\nthe cumulative cost is not recomputed for those neighbors.  In short,\\\\nit can be shown that at most only 4N  cumulative cost computations\\\\nare performed, resulting in an O( N ) algorithm.\\\\n6.Conclusions and Future Work\\\\nIntelligent...\",\"(1) making use of the weighted zero-crossings in the Laplacian to\\\\nperform subpixel edge \\\\ufb01ltering and anti-aliasing,  (2) use of multi-\\\\nple layered (multiplane) masks,  (3) making spatial frequency\\\\nequivalencing locally adaptive,  (4) varying the light source over\\\\nthe object using directional gradient shading (arti\\\\ufb01cial or borrowed)\\\\nto provide consistent lighting in the composition, and, most impor-\\\\ntantly  (5) extension of the 2-D DP graph search and application of\\\\nthe live-wire snap and trainin...\",\"Princeton, NJ: Princeton University Press, 1962.\\\\n[4]  Y . P . Chien and K. S. Fu, \\\\u201cA Decision Function Method for\\\\nBoundary Detection,\\\\u201d Computer Graphics and Image Pro-\\\\ncessing , vol. 3, no. 2, pp. 125-140, June 1974.\\\\n[5] D. Daneels, et al., \\\\u201cInteractive Outlining: An Improved\\\\nApproach Using Active Contours,\\\\u201d in SPIE Proceedings of\\\\nStorage and Retrieval for Image and V ideo Databases , vol\\\\n1908, pp. 226-233, Feb. 1993.\\\\n[6] E. W. Dijkstra, \\\\u201cA Note on Two Problems in Connexion with\\\\nGraphs,\\\\u201d Numeris...\",\"Contour Models,\\\\u201d in Proceedings of the First International\\\\nConference on Computer Vision , London, England, pp. 259-\\\\n68, June 1987.\\\\n[9] D. Marr and E. Hildreth, \\\\u201cA Theory of Edge Detection,\\\\u201d in\\\\nProceedings of the Royal Society of London--Series B: Biolog-\\\\nical Sciences, vol. 207, no. 1167, pp. 187-217, Feb. 1980.\\\\n[10] A. Martelli, \\\\u201cAn Application of Heuristic Search Methods to\\\\nEdge and Contour Detection,\\\\u201d Communications of the ACM ,\\\\nvol. 19, no. 2, pp. 73-83, Feb. 1976.\\\\n[11] U. Montanari, \\\\u201cOn th...\",\"\\\\\"Dynamic Search Algorithms in Left V entricular Border Rec-\\\\nognition and Analysis of Coronary Arteries,\\\\\" inIEEE Pro-\\\\nceedings of Computers in Cardiology , pp. 71-75, Sept. 1984.\\\\n[15] D. J. Williams and M. Shah, \\\\u201cA Fast Algorithm for Active\\\\nContours and Curvature Estimation,\\\\u201d CVGIP: Image Under-\\\\nstanding , vol. 55, no. 1, pp. 14-26, Jan. 1992.\\\\nFigure 5:  Test image exhibiting edge blurring and point noise.  Bound-\\\\nary de\\\\ufb01nition times--polygon: 4.3 sec and curve: 8.3 sec.\\\\nFigure 6:  Desktop scene....\"]],[\"color\",{\"type\":\"ndarray\",\"array\":[\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#aec7e8\",\"#1f77b4\",\"#999\",\"#999\",\"#999\",\"#999\",\"#1f77b4\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#aec7e8\",\"#aec7e8\",\"#999\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#1f77b4\",\"#999\",\"#999\",\"#1f77b4\"],\"shape\":[52],\"dtype\":\"object\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1045\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1046\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1041\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1042\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1043\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1012\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1027\",\"attributes\":{\"renderers\":\"auto\",\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Document Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}},{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1028\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1029\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1030\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1031\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1036\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1037\"}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1022\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1023\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1024\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1025\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1017\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1018\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1019\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1020\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1021\",\"attributes\":{\"axis\":{\"id\":\"p1017\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1026\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1022\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1047\",\"attributes\":{\"title\":\"Knowledge Base Tospics\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1048\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Image Composition Techniques\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1049\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Image Segmentation Techniques\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":10}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1050\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"Others\"},\"renderers\":[{\"id\":\"p1044\"}],\"index\":5}}]}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}},{\"type\":\"object\",\"name\":\"TabPanel\",\"id\":\"p1120\",\"attributes\":{\"title\":\"Failures\",\"child\":{\"type\":\"object\",\"name\":\"Figure\",\"id\":\"p1056\",\"attributes\":{\"sizing_mode\":\"stretch_width\",\"x_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1065\",\"attributes\":{\"start\":19.41671133041382,\"end\":27.821502685546875}},\"y_range\":{\"type\":\"object\",\"name\":\"Range1d\",\"id\":\"p1066\",\"attributes\":{\"start\":-6.514929533004761,\"end\":1.4252317324280739}},\"x_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1067\"},\"y_scale\":{\"type\":\"object\",\"name\":\"LinearScale\",\"id\":\"p1068\"},\"title\":{\"type\":\"object\",\"name\":\"Title\",\"id\":\"p1063\",\"attributes\":{\"text_color\":\"#E0E0E0\",\"text_font\":\"Helvetica\",\"text_font_size\":\"14pt\"}},\"outline_line_color\":\"#E0E0E0\",\"outline_line_alpha\":0.25,\"renderers\":[{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1095\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1052\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1053\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1054\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"yeGtQdQzoEFRqqpBITWeQRI9rEFJyrNBlHWjQWBlxUFZk7FBqpq4QZuiukFJyrNBV9u6QYtMwEHWq79BzuPAQf7zskGEarRBgJ2+QSi1rEFZk7FBhGq0QTIbvkFTI7pBXTy3Qdarv0Ga0a9BU063QZuiukFgZcVBYGXFQdarv0EyG75BlHWjQWjyv0FTwKFBzuPAQZrRr0E9abZBV9u6QSkfxkET07hBWZOxQSi1rEHzg6dB1DOgQa3Es0GAnb5BXAqnQVNOt0HQrLBBWZOxQdarv0E=\"},\"shape\":[53],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"wt2svlk/RMBTBVHALt0RwPNKi7+zJNO+V1SDwMwEZsCUBEPANFyXwO9oUcCzJNO+zc59wJWOgMC0V5DABVrvv+lTmMCLyyC9zQXdvw2XaMCUBEPAi8sgvbKwIcAVy7K+GYsTwLRXkMAPO3vAIle2v+9oUcDMBGbAzARmwLRXkMCysCHAV1SDwHV6WcCoDA3ABVrvvw87e8BWIabAzc59wH3uMsC5WIa/lARDwA2XaMADU96+WT9EwFiF7b/NBd2/9ZdMwCJXtr9dExfAlARDwLRXkMA=\"},\"shape\":[53],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Image Segmentation Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Segmentation Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Others\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Segmentation Techniques\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Others\",\"Others\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Image Composition Techniques\",\"Others\"]],[\"correctness\",[false,true,true,true,true,true,true,true,false,true,true,true,true,true,true,true,true,true,true,true,false,true,false,false,false,false,false,true,false,true,true,false,false,true,false,true,true,true,true,true,false,true,false,true,true,false,false,false,true,true,false,false,false]],[\"questions\",[\"What are the limitations of the training based on learned edge characteristics?\",\"What is the role of the seed points in the intelligent scissor tool?\",\"What is the key difference between Intelligent Scissors and previous boundary finding techniques?\",\"What is the benefit of path cooling in the live-wire segment?\",\"What is the role of the training length, t, in the live-wire tool?\",\"What is the difference between a static cost map and a dynamic cost map?\",\"What are the key differences between Snakes and Intelligent Scissors in terms of boundary segmentation?\",\"How is equivalencing of spatial frequencies achieved in image composition?\",\"What does preprocessing require for color images in the Intelligent Scissors application?\",\"What is the purpose of the Intelligent Scissors tool?\",\"Could you list the individuals who are credited with the creation of the Intelligent Scissors for Image Composition?\",\"How does the dynamic cost map in relation to gradient magnitudes evolve when subjected to a training process, as compared to when it is not trained?\",\"In which month and year was the conference paper titled &#x27;Intelligent scissors for image composition&#x27; first made public?\",\"What potential problems might be encountered when an object, originally from a different background color, is composited into a new scene?\",\"What methods can be implemented to achieve subpixel accuracy in the process of image composition, and how does the laplacian operator contribute to this?\",\"Could you tell me the name of the piece written by U. Montanari that was made public in the month of May in the year 1971?\",\"Could you elaborate on the purpose and functionality of the Intelligent Scissors tool in the context of image segmentation and composition?\",\"Could you specify the weights assigned to each feature function in the image feature formulation, as mentioned in the document?\",\"Could you identify the authors who contributed to the writing of the research paper named &#x27;A Decision Function Method for Boundary Detection&#x27;?\",\"Could you elaborate on the main variances between the mechanisms of Snakes and Intelligent Scissors in the context of boundary segmentation, particularly focusing on their use of laplacian zero-crossing binary cost feature, active contour smoothing term, and their approach to edge features attraction and global optimality?\",\"What was the objective of the study conducted with eight untrained users in relation to Intelligent Scissors, considering the issue of the live-wire snapping to a strong edge rather than a desired weaker edge?\",\"Considering the necessity of Euclidean distance weighting in the computation of cumulative costs, how does the Laplacian zero-crossing contribute to the image feature formulation?\",\"What is the copyright year and fee for the material provided on the 2-D extension to previous optimal edge tracking methods and the Image Composition with Intelligent Scissors?\",\"In the context of the Live-Wire 2-D DP graph search algorithm used in the intelligent scissor paper, how does the gradient direction function to ensure smoothness in the boundary direction?\",\"Considering the context of image composition, could you enumerate the five primary techniques employed in the process, and explain how they contribute to the overall efficacy?\",\"In the context of image composition using Intelligent Scissors, how can subpixel accuracy be achieved?\",\"Considering the wavefront of active points that grows out faster where there are lower costs, what is the key advantage of Intelligent Scissors over previous segmentation tools?\",\"How does the Laplacian zero-crossing contribute to defining boundaries in image types like the ones shown in Figures 5, 6, and 7 using Intelligent Scissors?\",\"Can you identify the authors of the Intelligent Scissors for Image Composition, a tool that allows for cursor snap and path cooling in image editing?\",\"In the context of image composition, such as in the creation of the final composition in Figure 9(d) where objects were scaled, rotated, and flipped, how is the equivalencing of spatial frequencies achieved?\",\"As a graphic design student, I&#x27;m trying to create a convincing composition with the intelligent scissor paper. Could you explain to me how the equivalencing of spatial frequencies is achieved in image composition to normalize contrast?\",\"As a graphic design student, I&#x27;m trying to understand the practical use of the intelligent scissor paper for object edge estimation in image processing. Could you explain how I can achieve subpixel accuracy in image composition?\",\"As a graphic design student working on a class project about the intelligent scissor paper, I need to know the copyright year and fee for the ACM-0-89791-701-4/95/008. Can you help me with this information?\",\"As an AI researcher, I&#x27;m trying to understand the differences and similarities between Intelligent Scissors and Snakes imaging techniques. Could you explain what the key difference is between Intelligent Scissors and previous techniques like Snakes?\",\"As a doctoral student in biomedical engineering, I&#x27;m researching new applications for Intelligent Scissors in medical imaging. Can you tell me what are the current applications of Intelligent Scissors as shown in the figures?\",\"As a digital artist, I&#x27;m curious about the intelligent scissor paper and how the live-wire segment adapts and snaps to object boundaries in images. Can you explain the purpose of the cursor snap feature in the live-wire tool?\",\"I&#x27;m currently exploring the application of heuristic search methods to edge and contour detection in images and came across the paper &#x27;Adaptive Boundary Dectection Using \\\\u2018Live-Wire\\\\u2019 Two-Dimensional Dynamic Programming&#x27;. Can you tell me who the authors of this paper are?\",\"As a digital artist, I&#x27;m always looking for ways to improve my workflow. Can you tell me how using Intelligent Scissors for image segmentation could benefit me in terms of speed and precision?\",\"As an indie game developer, I&#x27;m looking into using intelligent scissors for image blending in my game. Can you explain the purpose of creating a composition mask using the live-wire tool in image composition?\",\"Hey there, I&#x27;m a graphic designer and I&#x27;ve been exploring the use of intelligent scissors in image composition. Can you tell me when the conference paper &#x27;Intelligent scissors for image composition&#x27; was published?\",\"What is the process of equivalencing of spatial frequencies and what is the role of parameters d 0 and n in the Butterworth filter?\",\"In the context of two pixels, what is the direction feature cost and how does the technique presented in the paper extend previous boundary tracking methods?\",\"What does Figure 8 in the study represent and what does the preprocessing of color images for the Intelligent Scissors application require?\",\"What are some similarities and differences between snakes and Intelligent Scissors?\",\"Why is weighing by Euclidean distance necessary in the pointer map and what happens to the points diagonal to the seed point?\",\"What is the purpose of placing seed points in the intelligent scissor tool and what happens when the movement of the free point causes the boundary to digress from the desired object edge?\",\"What is the computational complexity of the 2-D DP algorithm and what is the worst case scenario when a pixel is removed from the active list?\",\"What is the title of the paper written by Y.P. Chien and K.S. Fu and what is the topic of the paper published by M. Kass, A. Witkin, and D. Terzopoulos?\",\"What are the limitations of active contours in boundary definition methods and what is the principle of region growing in the same context?\",\"What are the weights of the feature functions \\\\u03c9 Z, \\\\u03c9 D, and \\\\u03c9 G, and what is the role of the laplacian zero-crossing in image processing?\",\"What are its features?\",\"What does it require?\",\"What is one approach to reduce them?\"]],[\"agent_answer\",[\"The limitations of the training based on learned edge characteristics include the potential for the trained features to be too subject to old edge characteristics, as well as the need for adaptive training with local dependence to prevent this issue.\",\"The seed point in the intelligent scissor tool is interactively initialized and it generates all possible optimal paths from the seed point to every other point in the image. This allows the user to interactively select the desired optimal boundary segment.\",\"The key difference is that Intelligent Scissors allow the user to interactively select the most suitable boundary from a set of all optimal boundaries emanating from a seed point, whereas previous methods exhibit a pattern of boundary approximation followed by boundary refinement.\",\"The benefit of path cooling in the live-wire segment is that it allows for the automatic generation of new seed points as the live-wire segment freezes, reducing the need for manual placement of seed points.\",\"The role of the training length, t, in the live-wire tool is to specify how many of the most recent boundary pixels are used to generate the training statistics.\",\"The difference between a static cost map and a dynamic cost map is that without training, high gradients are favored in a static cost map, while in a dynamic cost map, gradients similar to those sampled from the previous boundary segment are favored after training.\",\"1. The laplacian zero-crossing binary cost feature has not been used previously in active contours models or DP boundary tracking methods for Snakes, while it is used in Intelligent Scissors.\\\\n2. The active contour smoothing term for Snakes is internal based on the contour&#x27;s point positions, while for Intelligent Scissors, the smoothing term is computed from external image gradient directions.\",\"The equivalencing of spatial frequencies in image composition is achieved by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. This involves low-pass filtering the spectrum with higher frequency content to match that of the other image, determined by parameter fitting of a Butterworth low-pass filter to both spectra. Parameters corresponding to the spectrum with lower frequency content are then used to low-pass filter the spectrum of the other image.\",\"I don&#x27;t know.\",\"The purpose of the Intelligent Scissors tool is for image segmentation and composition, allowing objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse.\",\"Eric N. Mortensen and William A. Barrett\",\"When subjected to a training process, the dynamic cost map favors gradients similar to those sampled from the previous boundary segment. On the other hand, without training, high gradients are favored since they map to low costs.\",\"The conference paper titled &#x27;Intelligent scissors for image composition&#x27; was first made public in January 1995.\",\"One potential problem that might be encountered is that the edge pixels of the cut object may contain background information from the original image, which can be noticeable when the object is composited into a scene with a different background color. This can make it difficult to seamlessly blend the object into the new scene.\",\"To achieve subpixel accuracy in the process of image composition, the following methods can be implemented:\\\\n1. Making use of weighted zero-crossings in the Laplacian for subpixel edge filtering and anti-aliasing.\\\\n2. Using multi-layered (multiplane) masks.\\\\n3. Making spatial frequency equivalencing locally adaptive.\\\\n4. Varying the light source over the object using directional gradient shading.\\\\n5. Extending the 2-D DP graph search and applying live-wire snap and training tools to moving objects and multiplane masks for image sequence composition.\\\\n\\\\nThe Laplacian operator contributes to achieving subpixel accuracy by allowing the estimation of the position of object edges to subpixel accuracy through a weighted combination of Laplacian pixel values on either side of zero-crossings. This helps in reducing jaggies along object boundaries in the resulting composition.\",\"The piece written by U. Montanari that was made public in May 1971 is titled \\\\\"On the Optimal Detection of Curves in Noisy Pictures.\\\\\"\",\"The Intelligent Scissors tool is designed to facilitate image component boundary definition for composition artists. It allows for quick and accurate segmentation of objects within digital images by using simple gestures with a mouse. The tool utilizes live-wire boundary detection, which formulates discrete dynamic programming as a two-dimensional graph searching problem to provide optimal boundaries while reducing sensitivity to local noise. Additionally, the tool features boundary cooling, on-the-fly training, live-wire masking, and spatial frequency equivalencing to enhance robustness and adapt to specific types of edges. The cut objects can be copied, transformed (rotated, scaled, translated), and pasted into another image using an interactive graphical tool with handles for control. The tool specifies a 2-D RST transformation matrix for pasting the cut object into the destination image through bilinear interpolation.\",\"The weights assigned to each feature function in the image feature formulation are as follows: \\\\u03c9 Z  = 0.43, \\\\u03c9 D  = 0.43, and \\\\u03c9 G  = 0.14.\",\"The authors who contributed to the writing of the research paper named &#x27;A Decision Function Method for Boundary Detection&#x27; are Y. P. Chien and K. S. Fu.\",\"Based on the context provided, the main variances between Snakes and Intelligent Scissors in the context of boundary segmentation are as follows:\\\\n\\\\n1. Laplacian zero-crossing binary cost feature:\\\\n- Snakes do not seem to have used the laplacian zero-crossing binary cost feature previously in active contours models or DP boundary tracking methods.\\\\n- Intelligent Scissors, on the other hand, utilize the laplacian zero-crossing binary cost feature.\\\\n\\\\n2. Active contour smoothing term:\\\\n- The active contour smoothing term in Snakes is internal, based on the contour&#x27;s point positions.\\\\n- In contrast, the smoothing term for live-wire boundaries in Intelligent Scissors is computed from external image gradient directions.\\\\n\\\\n3. Approach to edge features attraction and global optimality:\\\\n- Snakes are typically attracted to edge features only within the gravity of an edge&#x27;s gradient energy valley.\\\\n- Snakes are globally optimal over the entire contour.\\\\n- Intelligent Scissors, on the other hand, can snap to strong edge features from arbitrary distances and are piece-wise optimal between seed points.\\\\n- This piece-wise optimality in Intelligent Scissors creates a balance between global optimality and local control, allowing for path cooling, intra-object on-the-fly training, and interactive optimal 2-D path selection.\\\\n\\\\nTherefore, the main differences between Snakes and Intelligent Scissors in terms of the laplacian zero-crossing binary cost feature, active contour smoothing term, and approach to edge features attraction and global optimality lie in their specific mechanisms and techniques used for boundary segmentation.\",\"The objective of the study conducted with eight untrained users in relation to Intelligent Scissors was to investigate how the live-wire snapping to a strong edge rather than a desired weaker edge could affect the segmentation process.\",\"The Laplacian zero-crossing contributes to the image feature formulation by creating a binary feature that represents good edge properties, resulting in a low local cost.\",\"I don&#x27;t know.\",\"The gradient direction function ensures smoothness in the boundary direction by providing mathematically optimal boundaries while greatly reducing sensitivity to local noise or other intervening structures.\",\"I don&#x27;t know.\",\"In the context of image composition using Intelligent Scissors, subpixel accuracy can be achieved by using live-wire boundaries which formulate discrete dynamic programming as a two-dimensional graph searching problem. This approach provides mathematically optimal boundaries while greatly reducing sensitivity to local noise or other intervening structures. Additionally, on-the-fly training can further enhance robustness, causing the boundary to adhere to the specific type of edge currently being followed.\",\"The key advantage of Intelligent Scissors over previous segmentation tools is that it allows the user to interactively select the most suitable boundary from a set of all optimal boundaries emanating from a seed point.\",\"The Laplacian zero-crossing contributes to defining boundaries by representing points of maximal (or minimal) gradient magnitude, which are considered \\\\\"good\\\\\" edge properties with low local cost. This feature helps in accurately localizing edges in images, including those with gaussian edge blurring and point noise, as shown in Figures 5, 6, and 7 using Intelligent Scissors.\",\"Yes, the authors of the Intelligent Scissors for Image Composition are Eric N. Mortensen and another author from Lucidyne Technologies, Inc.\",\"The equivalencing of spatial frequencies is achieved by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. This is done by low-pass filtering the spectrum with higher frequency content to match that of the other image, using parameter fitting of a Butterworth low-pass filter (BLPF) to both spectra.\",\"To achieve equivalencing of spatial frequencies in image composition with the intelligent scissor paper, you need to match the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. This involves low-pass filtering the spectrum with higher frequency content to match that of the other image. The process includes parameter fitting of a Butterworth low-pass filter to both spectra, using parameters corresponding to the spectrum with lower frequency content to filter the spectrum of the other image. This helps to ensure that the spatial frequencies and contrast are normalized, resulting in a convincing composition.\",\"I don&#x27;t know.\",\"I don&#x27;t know.\",\"The key difference between Intelligent Scissors and previous techniques like Snakes is in the method of interaction. Previous methods involve a pattern of boundary approximation followed by boundary refinement, while Intelligent Scissors allow the user to interactively select the most suitable boundary from a set of all optimal boundaries emanating from a seed point. Additionally, previous approaches do not incorporate on-the-fly training or cooling, and are not as computationally efficient.\",\"I don&#x27;t know.\",\"The cursor snap feature in the live-wire tool allows objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse. When the gestured mouse position comes in proximity to an object edge, the live-wire boundary \\\\\"snaps\\\\\" to and wraps around the object of interest. This feature helps in quickly and accurately selecting object boundaries in images, making image segmentation and composition tasks easier for digital artists.\",\"The authors of the paper &#x27;Adaptive Boundary Detection Using \\\\u2018Live-Wire\\\\u2019 Two-Dimensional Dynamic Programming&#x27; are E. N. Mortensen, B. S. Morse, W. A. Barrett, and J. K. Udupa.\",\"Using Intelligent Scissors for image segmentation can benefit you as a digital artist by allowing you to quickly extract objects from images with simple mouse gestures, thus improving your workflow in terms of speed. Additionally, the tool provides accurate boundary detection through live-wire boundary snapping, which enhances precision in object extraction.\",\"Creating a composition mask using the live-wire tool allows the composition artist to blend an object from one image into another image by slipping the cut-out object behind some scene components while leaving it in front of other components. This helps in seamlessly integrating the cut-out object into the new image without affecting the masked pixels, making the blending process more accurate and visually appealing.\",\"The conference paper &#x27;Intelligent scissors for image composition&#x27; was published in January 1995.\",\"The process of equivalencing of spatial frequencies involves matching the spectral content of the cut piece and the destination image by low-pass filtering the spectrum with higher frequency content to match that of the other. The parameters d 0 and n in the Butterworth filter are used for fitting the filter to both spectra. If the destination image demonstrates lower spatial frequency content than the source image, the spatial frequencies between the two images can be equivalenced by fitting the Butterworth filter to the source image.\",\"In the context of two pixels, the direction feature cost is low when the gradient direction of the two pixels are similar to each other and the link between them.\\\\n\\\\nThe technique presented in the paper extends previous boundary tracking methods by:\\\\n1. Imposing no directional sampling or searching constraints.\\\\n2. Utilizing a new set of edge features and costs: laplacian zero-crossing, multiple gradient kernels.\\\\n3. Sorting the active list with an O(N) sort for N nodes/pixels.\\\\n4. Not specifying a priori goal nodes/pixels.\",\"Figure 8 in the study represents a graphical comparison between live-wire boundary definition times and boundary accuracy with manual tracing. The preprocessing of color images for the Intelligent Scissors application requires 36 convolutions.\",\"Some similarities between snakes and Intelligent Scissors are:\\\\n1. Both use a gradient magnitude cost/edge energy functional.\\\\n2. Both employ a smoothing term to minimize the effects of noise.\\\\n3. Both are attracted towards strong edge features.\\\\n4. Both aim to find globally optimal boundaries.\\\\n\\\\nSome differences between snakes and Intelligent Scissors are:\\\\n1. Snakes do not use laplacian zero-crossing binary cost feature.\\\\n2. The smoothing term in snakes is internal, while in Intelligent Scissors it is computed from external image gradient directions.\\\\n3. Snakes are interactively initialized with an approximate boundary contour and iteratively adjusted, while Intelligent Scissors are initialized with a single seed point and generate optimal paths from there.\\\\n4. Snakes are globally optimal over the entire contour, while live-wire boundaries are piece-wise optimal between seed points.\",\"Weighing by Euclidean distance in the pointer map is necessary to demonstrate that the cumulative costs to points currently on the active list can change if even lower cumulative costs are computed from as yet unexpanded neighbors. The points diagonal to the seed point change cumulative cost and direction pointers because the Euclidean weighting between the seed and diagonal points makes them more costly than non-diagonal paths.\",\"The purpose of placing seed points in the intelligent scissor tool is to ensure a closed object boundary. When the movement of the free point causes the boundary to digress from the desired object edge, spikes may result on the segmented boundary at those seed points.\",\"The computational complexity of the 2-D DP algorithm is O(N), where N represents the number of image pixels. In the worst case scenario when a pixel is removed from the active list, it is expanded by computing the cumulative cost to all of its neighbors that have not already been expanded.\",\"The title of the paper written by Y.P. Chien and K.S. Fu is \\\\\"A Decision Function Method for Boundary Detection.\\\\\" The topic of the paper published by M. Kass, A. Witkin, and D. Terzopoulos is \\\\\"Snakes: Active.\\\\\"\",\"The limitations of active contours in boundary definition methods are that they follow a pattern of initialization followed by energy minimization, which means the user does not know the final result. The principle of region growing is to use an interactively selected seed point to \\\\\"grow\\\\\" a region by adding adjacent neighboring pixels.\",\"The weights of the feature functions are as follows: \\\\u03c9 Z = 0.43, \\\\u03c9 D = 0.43, and \\\\u03c9 G = 0.14. The role of the laplacian zero-crossing in image processing is to represent \\\\\"good\\\\\" edge properties and should therefore have a low local cost.\",\"The features mentioned in the context are:\\\\n1. Laplacian zero-crossing binary cost feature\\\\n2. Active contour smoothing term\\\\n3. Gradient magnitude cost\\\\n4. Smoothing term\\\\n5. Attraction towards strong edge features\\\\n6. Finding globally optimal boundaries\",\"It requires blending an object from one image into another image.\",\"One approach to reduce jaggies is to perform edge filtering, similar to anti-aliasing.\"]],[\"reference_answer\",[\"Training is most effective for those objects with edge properties that are relatively consistent along the object boundary. Training can be counter-productive for objects with sudden and/or dramatic changes in edge features. However, training can be turned on and off interactively throughout the definition of an object boundary.\",\"Seed points are used to compute optimal paths from the seed point to all points in the image. A minimum of two seed points must be placed to ensure a closed object boundary. The path map from the first seed point of every object is maintained during the course of an object\\\\u2019s boundary definition to provide a closing boundary path from the free point. If a new seed point is placed, it reinitiates the 2-D DP boundary detection, causing potential paths to be recomputed.\",\"The key difference between Intelligent Scissors and previous boundary finding techniques lies in the method of interaction. Previous methods exhibit a pattern of boundary approximation followed by boundary refinement, whereas Intelligent Scissors allow the user to interactively select the most suitable boundary from a set of all optimal boundaries emanating from a seed point.\",\"Path cooling in the live-wire segment allows for automatic generation of new seed points as the live-wire segment freezes. This eliminates the need for manual placement of each seed point on the object boundary, making the process less tedious.\",\"The training length, t, specifies how many of the most recent boundary pixels are used to generate the training statistics. It is used in the trained gradient magnitude cost function and helps the tool adapt to slow or smooth changes in edge characteristics.\",\"Without training, high gradients are favored as they map to low costs, this is represented by the static cost map. However, with training, the dynamic cost map favors gradients similar to those sampled from the previous boundary segment.\",\"1. The laplacian zero-crossing binary cost feature seems to have not been used previously in active contours models or DP boundary tracking methods. 2. The active contour smoothing term is internal (i.e., based on the contour\\\\u2019s point positions) whereas the smoothing term for live-wire boundaries is computed from external image gradient directions.\",\"Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. This is accomplished by low-pass filtering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter fitting of a Butterworth low-pass filter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass filter the spectrum of the other image.\",\"Preprocessing requires 36 convolutions for color images, a gradient orientation calculation, a maximum gradient neighborhood search, and creation of a local.\",\"The Intelligent Scissors tool is used to digitally extract objects from images. This can be used to combine objects or regions from various still photographs or movie frames to create a seamless, believable, image or image sequence which appears convincing and real.\",\"The authors of the Intelligent Scissors for Image Composition are Eric N. Mortensen and William A. Barrett from Brigham Young University.\",\"With training, the dynamic cost map favors gradients similar to those sampled from the previous boundary segment.\",\"The conference paper on Intelligent scissors for image composition was published in January 1995.\",\"The edge pixels of the cut object likely contain some background information from the original image. This becomes noticeable when an object is composited into a scene with a different background color than the object\\\\u2019s original background color.\",\"Subpixel accuracy can be obtained by exploiting the signed output of the laplacian operator. The position of the object edge can be estimated to subpixel accuracy by using a (linearly) weighted combination of the laplacian pixel values on either side of the zero-crossings.\",\"On the Optimal Detection of Curves in Noisy Pictures\",\"Intelligent Scissors is an interactive tool used for image segmentation and composition. It allows objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse. The extracted object can then be copied and placed into a buffer for future processing and placement into another image.\",\"The weights are \\\\u03c9 Z = 0.43, \\\\u03c9 D = 0.43, and \\\\u03c9 G = 0.14.\",\"The paper titled &#x27;A Decision Function Method for Boundary Detection&#x27; was written by Y. P. Chien and K. S. Fu.\",\"1. The laplacian zero-crossing binary cost feature seems to have not been used previously in active contours models or DP boundary tracking methods. 2. The active contour smoothing term is internal whereas the smoothing term for live-wire boundaries is computed from external image gradient directions. 3. Snakes are typically attracted to edge features only within the gravity of an edge\\\\u2019s gradient energy valley whereas the live-wire boundary can snap to strong edge features from arbitrary distances. 4. Snakes are globally optimal over the entire contour whereas live-wire boundaries are piece-wise optimal. 5. Snakes refine a single rough boundary approximation where the live-wire tool interactively selects an optimal boundary segment from potentially all possible minimum cost paths.\",\"The study with eight untrained users was conducted to compare the live-wire boundary definition times and boundary accuracy of Intelligent Scissors with manual tracing. The users were asked to define the boundaries of five objects multiple times with both Intelligent Scissors and manual tracing.\",\"The laplacian zero-crossing is a binary edge feature used for edge localization. Convolution of an image with a laplacian kernel approximates the 2nd partial derivative of the image. The laplacian image zero-crossing corresponds to points of maximal (or minimal) gradient magnitude. Thus, laplacian zero-crossings represent \\\\u201cgood\\\\u201d edge properties and should therefore have a low local cost.\",\"The copyright year is 1995 and the fee is $3.50.\",\"The gradient direction adds a smoothness constraint to the boundary by associating a high cost for sharp changes in boundary direction. It is the unit vector defined by Ix and Iy.\",\"The five key methods are: (1) making use of the weighted zero-crossings in the Laplacian to perform subpixel edge filtering and anti-aliasing, (2) use of multiple layered (multiplane) masks, (3) making spatial frequency equivalencing locally adaptive, (4) varying the light source over the object using directional gradient shading (artificial or borrowed) to provide consistent lighting in the composition, and, most importantly (5) extension of the 2-D DP graph search and application of the live-wire snap and training tools to moving objects and moving, multiplane masks for composition of image sequences.\",\"Subpixel accuracy can be obtained by exploiting the signed output of the laplacian operator. The position of the object edge can be estimated to subpixel accuracy by using a (linearly) weighted combination of the laplacian pixel values on either side of the zero-crossings.\",\"Intelligent Scissors require less time and effort to segment an object than it takes to manually input an initial approximation to the object boundary. It allows objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse.\",\"The Laplacian zero-crossing is a binary edge feature used for edge localization. Convolution of an image with a Laplacian kernel approximates the 2nd partial derivative of the image. The Laplacian image zero-crossing corresponds to points of maximal (or minimal) gradient magnitude. Thus, Laplacian zero-crossings represent \\\\u201cgood\\\\u201d edge properties and should therefore have a low local cost.\",\"The authors of the Intelligent Scissors for Image Composition are Eric N. Mortensen and William A. Barrett from Brigham Young University.\",\"Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. This is accomplished by low-pass filtering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter fitting of a Butterworth low-pass filter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass filter the spectrum of the other image.\",\"Equivalencing of spatial frequencies is performed by matching the spectral content of the cut piece and the destination image in the vicinity where it is to be pasted. This is accomplished by low-pass filtering the spectrum with the higher frequency content to match that of the other. The spectrum with the higher frequency content is determined by parameter fitting of a Butterworth low-pass filter (BLPF) to both spectra. Parameters corresponding to the spectrum with the lower frequency content are used to low-pass filter the spectrum of the other image.\",\"Subpixel accuracy can be obtained by exploiting the signed output of the laplacian operator. The position of the object edge can be estimated to subpixel accuracy by using a (linearly) weighted combination of the laplacian pixel values on either side of the zero-crossings.\",\"The copyright year is 1995 and the fee is $3.50.\",\"The key difference between Intelligent Scissors and all previous techniques is the interactive optimal 2-D path selection. The live-wire tool is interactively initialized with just a single seed point and it then generates, at interactive speeds, all possible optimal paths from the seed point to every other point in the image, thus, allowing the user to interactively select the desired optimal boundary segment.\",\"The Intelligent Scissors are applied in a variety of image types including an artificial test image that exhibits gaussian edge blurring and point noise typical of some imaging hardware, a desktop scene, and a CT image of a lumbar spine which demonstrates the live-wire\\\\u2019s application to medical imaging.\",\"The cursor snap feature in the live-wire tool is used to facilitate seed point placement. It forces the mouse pointer to the maximum gradient magnitude pixel within a user-specified neighborhood. This allows the user to interactively &#x27;snap&#x27; and &#x27;wrap&#x27; the live-wire boundary around the object of interest.\",\"The authors of the paper &#x27;Adaptive Boundary Dectection Using \\\\u2018Live-Wire\\\\u2019 Two-Dimensional Dynamic Programming&#x27; are E. N. Mortensen, B. S. Morse, W . A. Barrett, and J. K. Udupa.\",\"Intelligent Scissors allow objects within digital images to be extracted quickly and accurately using simple gesture motions with a mouse. When the gestured mouse position comes in proximity to an object edge, a live-wire boundary &#x27;snaps&#x27; to, and wraps around the object of interest. This typically requires less time and effort to segment an object than it takes to manually input an initial approximation to the object boundary.\",\"The composition mask created using the live-wire tool allows an artist to &#x27;slip&#x27; the cut-out object behind some scene components while leaving it in front of others. Scene components can be cut out of an image to create a mask such that any additions or changes to the scene will not affect masked pixels.\",\"The conference paper &#x27;Intelligent scissors for image composition&#x27; was published in January 1995.\",\"Equivalencing of spatial frequencies is performed by first computing the fourier transforms, S ( u , v ) and I ( u , v ) of the source image S ( x , y ) and the destination image I ( x , y ). We then compute the log power spectra s ( u , v ) and i( u , v ). By varying the two parameters d 0 and n, a least squares fit can be used to create a normalized Butterworth filter B ( u , v , d 0 &#x27;, n &#x27;)\",\"The direction feature cost is low when the gradient direction of the two pixels are similar to each other and the link between them. The technique presented in the paper extends previous boundary tracking methods by 1. Imposing no directional sampling or searching constraints. 2. Utilizing a new set of edge features and costs: laplacian zero-crossing, multiple gradient kernels. 3. Sorting the active list with an O( N ) sort for N  nodes/pixels. 4. Not specifying any a priori goal nodes/pixels.\",\"Figure 8 graphically compares the live-wire boundary definition times and boundary accuracy with manual tracing. The preprocessing of color images for the Intelligent Scissors application requires 36 convolutions, a gradient orientation calculation, a maximum gradient neighborhood search, and creation of a local.\",\"The gradient magnitude cost in Intelligent Scissors is similar to the edge energy functional used in snakes. Both methods employ a smoothing term to minimize the effects of noise in the boundary. Snakes and live-wire boundaries are both attracted towards strong edge features. Both techniques attempt to find globally optimal boundaries to try to overcome the effects of noise and edge dropout. However, snakes are typically attracted to edge features only within the gravity of an edge\\\\u2019s gradient energy valley whereas the live-wire boundary can snap to strong edge features from arbitrary distances. Snakes are globally optimal over the entire contour whereas live-wire boundaries are piece-wise optimal. Finally, snakes refine a single rough boundary approximation where the live-wire tool interactively selects an optimal boundary segment from potentially all possible minimum cost paths.\",\"Weighing by Euclidean distance is necessary in the pointer map to demonstrate that the cumulative costs to points currently on the active list can change if even lower cumulative costs are computed from as yet unexpanded neighbors. The points diagonal to the seed point have changed cumulative cost and direction pointers, making them more costly than non-diagonal paths due to the Euclidean weighting.\",\"Seed points are placed to ensure a closed object boundary and are points from which optimal paths are computed to all points in the image. When the movement of the free point causes the boundary to digress from the desired object edge, interactive input of a new seed point prior to the point of departure reinitiates the 2-D DP boundary detection, causing potential paths to be recomputed from the new seed point.\",\"The computational complexity for N image pixels is O(N) and in the worst case, a pixel has its cumulative cost computed by all of its 8 neighbors.\",\"The title of the paper written by Y.P. Chien and K.S. Fu is &#x27;A Decision Function Method for Boundary Detection&#x27;, and the topic of the paper published by M. Kass, A. Witkin, and D. Terzopoulos is not provided in the given context.\",\"Active contours follow a pattern of initialization followed by energy minimization, hence the user does not know what the final result will be and they can only track frame-to-frame boundary motion provided the boundary hasn\\\\u2019t moved drastically. On the other hand, region growing methods use an interactively selected seed point to &#x27;grow&#x27; a region by adding adjacent neighboring pixels, but since this type of region growing does not provide interactive visual feedback, resulting region boundaries must usually be edited or modified.\",\"The weights of the feature functions are \\\\u03c9 Z = 0.43, \\\\u03c9 D = 0.43, and \\\\u03c9 G = 0.14. The laplacian zero-crossing is a binary edge feature used for edge localization, representing &#x27;good&#x27; edge properties and should therefore have a low local cost.\",\"The Live-Wire 2-D dynamic programming graph search algorithm has several features. It maximizes over different gradient kernels sizes to balance edge detail with noise suppression and to encompass various edge types and scales. It uses the Laplacian zero-crossing for boundary localization and fine detail live-wire &#x27;snapping&#x27;. It also uses a specialized sorting algorithm that inserts points into a sorted list in constant time. Lastly, the live-wire tool is free to define a goal pixel interactively, at any &#x27;free&#x27; point in the image, after minimum cost paths are computed to all pixels.\",\"Preprocessing for color images in the Intelligent Scissors application requires 36 convolutions (from 3 \\\\u00d7 3, 5 \\\\u00d7 5, 7 \\\\u00d7 7, and 9 \\\\u00d7 9 kernels), a gradient orientation calculation, a maximum gradient neighborhood search, and creation of a local.\",\"One approach to reduce &#x27;jaggies&#x27; is by exploiting the signed output of the laplacian operator. The position of the object edge can be estimated to subpixel accuracy by using a (linearly) weighted combination of the laplacian pixel values on either side of the zero-crossings. Another approach is through appropriate edge filtering, similar to anti-aliasing.\"]],[\"id\",[32,23,6,28,31,26,35,41,44,3,7,26,0,40,39,50,37,12,49,34,44,12,8,13,48,39,1,11,7,41,41,39,8,35,43,24,50,1,38,0,42,14,44,34,19,23,46,49,5,11,16,44,39]],[\"content\",[\"precomputed feature maps along the closestt pixels of the edge seg-\\\\nment and increments the feature histogram element by the corre-\\\\nsponding pixel weight to generate a histogram for each feature\\\\ninvolved in training.\\\\nAfter sampling and smoothing, each feature histogram is then\\\\nscaled and inverted (by subtracting the scaled histogram values\\\\nfrom its maximum value) to create the feature cost map needed to\\\\nconvert feature values to trained cost functions.\\\\nSince training is based on learned edge cha...\",\"new seed point while effectively \\\\u201ctieing off\\\\u201d the boundary com-\\\\nputed up to the new seed point.\\\\nNote again that optimal paths are computed from the seed point\\\\nto all  points in the image (since the 2-D DP graph search produces\\\\na minimum cost spanning tree of the image [6]).  Thus, by selecting\\\\na free point with the mouse cursor, the interactive live-wire tool is\\\\nsimply selecting an optimal boundary segment from a large collec-\\\\ntion of optimal paths.\\\\nSince each pixel (or free point) de\\\\ufb01nes only o...\",\"boundary will look like when the rough approximation is input. If\\\\nthe resulting boundary is not satisfactory, the process must be\\\\nrepeated or the boundary must be manually edited.  W e provide a\\\\ndetailed comparison of snakes and Intelligent Scissors in section\\\\n3.6.\\\\nAnother class of image segmentation techniques use a graph\\\\nsearching formulation of DP (or similar concepts) to \\\\ufb01nd globally\\\\noptimal boundaries [2, 4, 10, 11, 14].  These techniques differ from\\\\nsnakes in that boundary points are gener...\",\"and only change local to the free point.  As the free point moves far-\\\\nther and farther away from the seed point, the portion of the active\\\\nlive-wire boundary segment that does not change becomes longer.\\\\nNew seed points are generated at the end of a stable segment (i.e.,\\\\nthat has not changed recently).  Stability is measured by time (in\\\\nmilliseconds) on the active boundary and path coalescence (number\\\\nof times the path has been redrawn from distinct free points).\\\\nThis measure of stability provid...\",\"interactively using the live-wire tool.  To allow training to adapt to\\\\nslow (or smooth) changes in edge characteristics, the trained gradi-\\\\nent magnitude cost function is based only on the most recent or\\\\nclosest portion of the current de\\\\ufb01ned object boundary.  A training\\\\nlength, t, speci\\\\ufb01es how many of the most recent boundary pixels are\\\\nused to generate the training statistics.  A monotonically decreasing\\\\nweight function (either linearly or Gaussian based) determines the\\\\ncontribution from each o...\",\"shows that without training, high gradients are favored since they map\\\\nto low costs.  However , with training, the dynamic cost map (d) favors\\\\ngradients similar to those sampled from the previous boundary segment.\\\\nM G\\\\nn G\\\\n0\\\\n0 Cost\\\\nGradient Magnitude\\\\nM G\\\\nn G\\\\n0\\\\n0 Cost\\\\nGradient Magnitude\\\\nStatic Cost Map Dynamic Cost Map\\\\n(c) (d)\",\"3. Snakes are typically attracted to edge features only within the\\\\ngravity of an edge\\\\u2019s gradient energy valley whereas the live-wire\\\\nboundary can snap to strong edge features from arbitrary dis-\\\\ntances (since the 2-D DP\\\\u2019s search window is the entire image).\\\\n4. Snakes are globally optimal over the entire contour whereas live-\\\\nwire boundaries are piece-wise optimal (i.e., optimal between\\\\nseed points).  We feel this creates a desirable balance between\\\\nglobal optimality and local control.  This piec...\",\"often have differing spatial frequencies or contrast due to differ-\\\\nences in focus or lighting when the images were acquired.  Thus,\\\\nequivalencing of spatial frequencies and normalization of contrast\\\\nis sometimes desirable in order to produce a convincing composi-\\\\ntion.\\\\nEquivalencing of spatial frequencies is performed by matching\\\\nthe spectral content of the cut piece and the destination image in the\\\\nvicinity where it is to be pasted.  Convincing composition often\\\\nrequires the spectra of the obj...\",\"times (for a trained user) for each displayed object boundary are\\\\ngiven in the caption of each respective \\\\ufb01gure.\\\\nFigure 8 graphically compares the live-wire boundary de\\\\ufb01nition\\\\ntimes and boundary accuracy with manual tracing.  These results\\\\nshow the average time and accuracy from a study where 8 untrained\\\\nusers1  were asked to de\\\\ufb01ne the boundaries of \\\\ufb01ve objects (the two\\\\nobjects in Fig. 5, the paper clip holder and pocket knife in Fig. 6,\\\\nand the outer boundary of the spinal vertebrae in Fig. 7)....\",\"manipulation techniques, has also been used to realistically blend\\\\nold \\\\ufb01lm into a new script.  The goal of image composition is to com-\\\\nbine objects or regions from various still photographs or movie\\\\nframes to create a seamless, believable, image or image sequence\\\\nwhich appears convincing and real.  Fig. 9(d) shows a believable\\\\ncomposition created by combining objects extracted from three\\\\nimages, Fig. 9(a-c).  These objects were digitally extracted and\\\\ncombined in a few minutes using a new, inte...\",\"of freedom within a window about the two-dimensional boundary\\\\ntemplate.  Thus, boundary extraction using previous graph search-\\\\ning techniques is non-interactive (beyond template speci\\\\ufb01cation),\\\\nlosing the bene\\\\ufb01ts of further human guidance and expertise.\\\\nIntelligent Scissors for Image Composition\\\\nEric N. Mortensen 1       William A. Barrett 2\\\\nBrigham Y oung University\\\\n1 enm @ cs.byu.edu, Dept. of Comp. Sci., BYU, Provo, UT  84602 (801)378-7605\\\\n2 barrett @ cs.byu.edu, Dept. of Comp. Sci., BYU, Pro...\",\"shows that without training, high gradients are favored since they map\\\\nto low costs.  However , with training, the dynamic cost map (d) favors\\\\ngradients similar to those sampled from the previous boundary segment.\\\\nM G\\\\nn G\\\\n0\\\\n0 Cost\\\\nGradient Magnitude\\\\nM G\\\\nn G\\\\n0\\\\n0 Cost\\\\nGradient Magnitude\\\\nStatic Cost Map Dynamic Cost Map\\\\n(c) (d)\",\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220720964\\\\nIntelligent scissors for image composition\\\\nConference Paper \\\\u00b7 January 1995\\\\nDOI: 10.1145/218380.218442\\\\u00a0\\\\u00b7\\\\u00a0Source: DBLP\\\\nCITATIONS\\\\n812\\\\nREADS\\\\n2,545\\\\n2 authors, including:\\\\nEric N. Mortensen\\\\nLucidyne Technologies, Inc.\\\\n34 PUBLICATIONS\\\\u00a0\\\\u00a0\\\\u00a03,134 CITATIONS\\\\u00a0\\\\u00a0\\\\u00a0\\\\nSEE PROFILE\\\\nAll content following this page was uploaded by Eric N. Mortensen on 01 June 2014.\\\\nThe user has requested enhancement of...\",\"ing of the image around the pasted object boundary .  However, this\\\\ndoes not account for the fact that the edge pixels of the cut object\\\\nvery likely contain some background information from the original\\\\nimage. This is most noticeable when an object is composited into a\\\\nscene with a different background color than the object\\\\u2019s original\\\\nbackground color.  A more general solution would determine how\\\\nmuch of each edge pixel corresponds to the actual object color and\\\\nweight them accordingly when comb...\",\"ef\\\\ufb01ciency.\\\\n1. Similar in concept to an optical mask used in motion picture special effects.\\\\nlution of the image.  This may produce jaggies along object bound-\\\\naries in a resulting composition.  However, subpixel accuracy can\\\\nbe obtained by exploiting the signed output of the laplacian opera-\\\\ntor.  That is, the position of the object edge can be estimated to sub-\\\\npixel accuracy by using a (linearly) weighted combination of the\\\\nlaplacian pixel values on either side of the zero-crossings.\\\\nSince the...\",\"Contour Models,\\\\u201d in Proceedings of the First International\\\\nConference on Computer Vision , London, England, pp. 259-\\\\n68, June 1987.\\\\n[9] D. Marr and E. Hildreth, \\\\u201cA Theory of Edge Detection,\\\\u201d in\\\\nProceedings of the Royal Society of London--Series B: Biolog-\\\\nical Sciences, vol. 207, no. 1167, pp. 187-217, Feb. 1980.\\\\n[10] A. Martelli, \\\\u201cAn Application of Heuristic Search Methods to\\\\nEdge and Contour Detection,\\\\u201d Communications of the ACM ,\\\\nvol. 19, no. 2, pp. 73-83, Feb. 1976.\\\\n[11] U. Montanari, \\\\u201cOn th...\",\"2-D extension to previous optimal edge tracking methods rather\\\\nthan an improvement on active contours.\\\\n4. Image Composition with Intelligent Scissors\\\\nAs mentioned, composition artists need an intelligent, interactive\\\\ntool to facilitate image component boundary de\\\\ufb01nition.  Since Intel-\\\\nligent Scissors can quickly segment object from an image, it serves\\\\nas a tool for cut and paste operations.  After object boundaries have\\\\nbeen extracted, object can be copied from the image and placed into\\\\na buffer...\",\"single-pixel wide cost \\\\u201ccanyons\\\\u201d used for boundary localization.\\\\nImage Feature Formulation\\\\nLaplacian Zero-Crossing fZ\\\\nGradient Magnitude fG\\\\nGradient Direction fD\\\\nl pq ,()\\\\u03c9 Z f Z q()\\\\u22c5\\\\u03c9 D f D pq ,()\\\\u22c5\\\\u03c9 G f G q()\\\\u22c5++=\\\\nf Z q()\\\\n0 ;\\\\u0011\\\\u0011\\\\u0011if\\\\u0011 I L q() 0=\\\\n1 ;\\\\u0011\\\\u0011\\\\u0011if\\\\u0011 I L q() 0\\\\u2260{=\\\\nSince the laplacian zero-crossing creates a binary feature, fZ ( q )\\\\ndoes not distinguish between strong, high gradient edges and weak,\\\\nlow gradient edges.  However, gradient magnitude provides a direct\\\\ncorrelation between edge strengt...\",\"Princeton, NJ: Princeton University Press, 1962.\\\\n[4]  Y . P . Chien and K. S. Fu, \\\\u201cA Decision Function Method for\\\\nBoundary Detection,\\\\u201d Computer Graphics and Image Pro-\\\\ncessing , vol. 3, no. 2, pp. 125-140, June 1974.\\\\n[5] D. Daneels, et al., \\\\u201cInteractive Outlining: An Improved\\\\nApproach Using Active Contours,\\\\u201d in SPIE Proceedings of\\\\nStorage and Retrieval for Image and V ideo Databases , vol\\\\n1908, pp. 226-233, Feb. 1993.\\\\n[6] E. W. Dijkstra, \\\\u201cA Note on Two Problems in Connexion with\\\\nGraphs,\\\\u201d Numeris...\",\"5. Snakes and Intelligent Scissors both require interaction as part of\\\\nthe boundary segmentation process.\\\\nDif ferences (compare with corresponding similarities above):\\\\n1. The laplacian zero-crossing binary cost feature seems to have not\\\\nbeen used previously in active contours models 1  (or DP bound-\\\\nary tracking methods for that matter).\\\\n2. The active contour smoothing term is internal (i.e., based on the\\\\ncontour\\\\u2019s point positions) whereas the smoothing term for live-\\\\nwire boundaries is computed...\",\"times (for a trained user) for each displayed object boundary are\\\\ngiven in the caption of each respective \\\\ufb01gure.\\\\nFigure 8 graphically compares the live-wire boundary de\\\\ufb01nition\\\\ntimes and boundary accuracy with manual tracing.  These results\\\\nshow the average time and accuracy from a study where 8 untrained\\\\nusers1  were asked to de\\\\ufb01ne the boundaries of \\\\ufb01ve objects (the two\\\\nobjects in Fig. 5, the paper clip holder and pocket knife in Fig. 6,\\\\nand the outer boundary of the spinal vertebrae in Fig. 7)....\",\"single-pixel wide cost \\\\u201ccanyons\\\\u201d used for boundary localization.\\\\nImage Feature Formulation\\\\nLaplacian Zero-Crossing fZ\\\\nGradient Magnitude fG\\\\nGradient Direction fD\\\\nl pq ,()\\\\u03c9 Z f Z q()\\\\u22c5\\\\u03c9 D f D pq ,()\\\\u22c5\\\\u03c9 G f G q()\\\\u22c5++=\\\\nf Z q()\\\\n0 ;\\\\u0011\\\\u0011\\\\u0011if\\\\u0011 I L q() 0=\\\\n1 ;\\\\u0011\\\\u0011\\\\u0011if\\\\u0011 I L q() 0\\\\u2260{=\\\\nSince the laplacian zero-crossing creates a binary feature, fZ ( q )\\\\ndoes not distinguish between strong, high gradient edges and weak,\\\\nlow gradient edges.  However, gradient magnitude provides a direct\\\\ncorrelation between edge strengt...\",\"permission and/or a fee. \\\\n\\\\u00a91995  ACM-0-89791-701-4/95/008\\\\u0085$3.50\",\"imum gradient at unity,fG ( q ) is scaled by 1 if q  is a diagonal neigh-\\\\nbor to p  and by 1/ \\\\u221a 2 if q  is a horizontal or vertical neighbor.\\\\nThe gradient direction adds a smoothness constraint to the\\\\nboundary by associating a high cost for sharp changes in boundary\\\\ndirection.  The gradient direction is the unit vector de\\\\ufb01ned byI x  and\\\\nI y .  Letting D ( p ) be the unit vector perpendicular (rotated 90 degrees\\\\nclockwise) to the gradient direction at point p  (i.e., for D ( p ) = ( I y ( p ),\\\\n-I...\",\"(1) making use of the weighted zero-crossings in the Laplacian to\\\\nperform subpixel edge \\\\ufb01ltering and anti-aliasing,  (2) use of multi-\\\\nple layered (multiplane) masks,  (3) making spatial frequency\\\\nequivalencing locally adaptive,  (4) varying the light source over\\\\nthe object using directional gradient shading (arti\\\\ufb01cial or borrowed)\\\\nto provide consistent lighting in the composition, and, most impor-\\\\ntantly  (5) extension of the 2-D DP graph search and application of\\\\nthe live-wire snap and trainin...\",\"ef\\\\ufb01ciency.\\\\n1. Similar in concept to an optical mask used in motion picture special effects.\\\\nlution of the image.  This may produce jaggies along object bound-\\\\naries in a resulting composition.  However, subpixel accuracy can\\\\nbe obtained by exploiting the signed output of the laplacian opera-\\\\ntor.  That is, the position of the object edge can be estimated to sub-\\\\npixel accuracy by using a (linearly) weighted combination of the\\\\nlaplacian pixel values on either side of the zero-crossings.\\\\nSince the...\",\"Abstract\\\\nWe present a new, interactive tool called Intelligent Scissors\\\\nwhich we use for image segmentation and composition.  Fully auto-\\\\nmated segmentation is an unsolved problem, while manual tracing\\\\nis inaccurate and laboriously unacceptable.  However, Intelligent\\\\nScissors allow objects within digital images to be extracted quickly\\\\nand accurately using simple gesture motions with a mouse.  When\\\\nthe gestured mouse position comes in proximity to an object edge,\\\\nalive-wire boundary  \\\\u201csnaps\\\\u201d to, ...\",\"(1)\\\\nwhere each \\\\u03c9  is the weight of the corresponding feature function.\\\\n(Empirically, weights of \\\\u03c9 Z  = 0.43, \\\\u03c9 D  = 0.43, and \\\\u03c9 G  = 0.14 seem\\\\nto work well in a wide range of images.)\\\\nThe laplacian zero-crossing is a binary edge feature used for edge\\\\nlocalization [7, 9]. Convolution of an image with a laplacian kernel\\\\napproximates the 2nd  partial derivative of the image.  The laplacian\\\\nimage zero-crossing corresponds to points of maximal (or minimal)\\\\ngradient magnitude.  Thus, laplacian zero-cr...\",\"of freedom within a window about the two-dimensional boundary\\\\ntemplate.  Thus, boundary extraction using previous graph search-\\\\ning techniques is non-interactive (beyond template speci\\\\ufb01cation),\\\\nlosing the bene\\\\ufb01ts of further human guidance and expertise.\\\\nIntelligent Scissors for Image Composition\\\\nEric N. Mortensen 1       William A. Barrett 2\\\\nBrigham Y oung University\\\\n1 enm @ cs.byu.edu, Dept. of Comp. Sci., BYU, Provo, UT  84602 (801)378-7605\\\\n2 barrett @ cs.byu.edu, Dept. of Comp. Sci., BYU, Pro...\",\"often have differing spatial frequencies or contrast due to differ-\\\\nences in focus or lighting when the images were acquired.  Thus,\\\\nequivalencing of spatial frequencies and normalization of contrast\\\\nis sometimes desirable in order to produce a convincing composi-\\\\ntion.\\\\nEquivalencing of spatial frequencies is performed by matching\\\\nthe spectral content of the cut piece and the destination image in the\\\\nvicinity where it is to be pasted.  Convincing composition often\\\\nrequires the spectra of the obj...\",\"often have differing spatial frequencies or contrast due to differ-\\\\nences in focus or lighting when the images were acquired.  Thus,\\\\nequivalencing of spatial frequencies and normalization of contrast\\\\nis sometimes desirable in order to produce a convincing composi-\\\\ntion.\\\\nEquivalencing of spatial frequencies is performed by matching\\\\nthe spectral content of the cut piece and the destination image in the\\\\nvicinity where it is to be pasted.  Convincing composition often\\\\nrequires the spectra of the obj...\",\"ef\\\\ufb01ciency.\\\\n1. Similar in concept to an optical mask used in motion picture special effects.\\\\nlution of the image.  This may produce jaggies along object bound-\\\\naries in a resulting composition.  However, subpixel accuracy can\\\\nbe obtained by exploiting the signed output of the laplacian opera-\\\\ntor.  That is, the position of the object edge can be estimated to sub-\\\\npixel accuracy by using a (linearly) weighted combination of the\\\\nlaplacian pixel values on either side of the zero-crossings.\\\\nSince the...\",\"permission and/or a fee. \\\\n\\\\u00a91995  ACM-0-89791-701-4/95/008\\\\u0085$3.50\",\"3. Snakes are typically attracted to edge features only within the\\\\ngravity of an edge\\\\u2019s gradient energy valley whereas the live-wire\\\\nboundary can snap to strong edge features from arbitrary dis-\\\\ntances (since the 2-D DP\\\\u2019s search window is the entire image).\\\\n4. Snakes are globally optimal over the entire contour whereas live-\\\\nwire boundaries are piece-wise optimal (i.e., optimal between\\\\nseed points).  We feel this creates a desirable balance between\\\\nglobal optimality and local control.  This piec...\",\"d 0 &#x27; and n &#x27; are the \\\\ufb01t parameters) matched to the spatial frequency\\\\ncharacteristics of i( u , v ).  If i( u , v ) demonstrates lower spatial fre-\\\\nquency content than s ( u , v ), the spatial frequencies between the two\\\\nimages can be equivalenced by \\\\ufb01tting B ( u , v , d 0 &#x27;, n &#x27;) to s ( u , v ).  The\\\\nequivalenced result using the inverse Fourier transform\\\\nis then pasted onto I ( x , y ).  Prior to pasting, the colors in the source\\\\nimage are scaled to the range of I ( x , y ) to account for diff...\",\"and tedious.  If a seed point is not localized to an object edge then\\\\nspikes results on the segmented boundary at those seed points (since\\\\nFigure 2:  Image demonstrating how the live-wire segment adapts and\\\\nsnaps to an object boundary as the free point moves (via cursor move-\\\\nment).  The path of the free point is shown in white.  Live-wire segments\\\\nfrom previous free point positions (t0 , t 1 , and t 2 ) are shown in green.\\\\n(a) (b)\\\\nFigure 3:  Comparison of live-wire without (a) and with (b) cool...\",\"Contour Models,\\\\u201d in Proceedings of the First International\\\\nConference on Computer Vision , London, England, pp. 259-\\\\n68, June 1987.\\\\n[9] D. Marr and E. Hildreth, \\\\u201cA Theory of Edge Detection,\\\\u201d in\\\\nProceedings of the Royal Society of London--Series B: Biolog-\\\\nical Sciences, vol. 207, no. 1167, pp. 187-217, Feb. 1980.\\\\n[10] A. Martelli, \\\\u201cAn Application of Heuristic Search Methods to\\\\nEdge and Contour Detection,\\\\u201d Communications of the ACM ,\\\\nvol. 19, no. 2, pp. 73-83, Feb. 1976.\\\\n[11] U. Montanari, \\\\u201cOn th...\",\"Abstract\\\\nWe present a new, interactive tool called Intelligent Scissors\\\\nwhich we use for image segmentation and composition.  Fully auto-\\\\nmated segmentation is an unsolved problem, while manual tracing\\\\nis inaccurate and laboriously unacceptable.  However, Intelligent\\\\nScissors allow objects within digital images to be extracted quickly\\\\nand accurately using simple gesture motions with a mouse.  When\\\\nthe gestured mouse position comes in proximity to an object edge,\\\\nalive-wire boundary  \\\\u201csnaps\\\\u201d to, ...\",\"Image composition often requires blending an object from one\\\\nimage into another image such that the cut-out object is not in the\\\\nforeground.  This requires the composition artist to \\\\u201cslip\\\\u201d the cut-\\\\nout object behind some scene components while leaving it in front\\\\nof other components. This operation can again be performed using\\\\nthe live-wire tool to create a composition mask1 . Scene components\\\\ncan be cut out of an image to create a mask such that any additions\\\\nor changes to the scene will not af...\",\"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/220720964\\\\nIntelligent scissors for image composition\\\\nConference Paper \\\\u00b7 January 1995\\\\nDOI: 10.1145/218380.218442\\\\u00a0\\\\u00b7\\\\u00a0Source: DBLP\\\\nCITATIONS\\\\n812\\\\nREADS\\\\n2,545\\\\n2 authors, including:\\\\nEric N. Mortensen\\\\nLucidyne Technologies, Inc.\\\\n34 PUBLICATIONS\\\\u00a0\\\\u00a0\\\\u00a03,134 CITATIONS\\\\u00a0\\\\u00a0\\\\u00a0\\\\nSEE PROFILE\\\\nAll content following this page was uploaded by Eric N. Mortensen on 01 June 2014.\\\\nThe user has requested enhancement of...\",\"where d 0  is the distance of the cutoff frequency from the origin and\\\\nn  is the \\\\ufb01lter order.\\\\nEquivalencing of spatial frequencies is performed by \\\\ufb01rst com-\\\\nputing the fourier transforms, S ( u , v ) and I ( u , v ) of the source image\\\\nS ( x , y ) and the destination image I ( x , y ).  We then compute the log\\\\npower spectra s ( u , v ) and i( u , v ):\\\\nBy varying the two parameters d 0  and n , a least squares \\\\ufb01t can be\\\\nused to create a normalized Butterworth \\\\ufb01lter B ( u , v , d 0 &#x27;, n &#x27;) (where\\\\n...\",\"two pixels that have similar gradient directions but are perpendicu-\\\\nlar, or near perpendicular, to the link between them.  Therefore, the\\\\ndirection feature cost is low when the gradient direction of the two\\\\npixels are similar to each other and the link between them.\\\\n3.2. Two-Dimensional Dynamic Programming\\\\nAs mentioned, dynamic programming can be formulated as a\\\\ndirected graph search for an optimal path.  This paper utilizes an\\\\noptimal graph search similar to that presented by Dijkstra [6] and\\\\n...\",\"times (for a trained user) for each displayed object boundary are\\\\ngiven in the caption of each respective \\\\ufb01gure.\\\\nFigure 8 graphically compares the live-wire boundary de\\\\ufb01nition\\\\ntimes and boundary accuracy with manual tracing.  These results\\\\nshow the average time and accuracy from a study where 8 untrained\\\\nusers1  were asked to de\\\\ufb01ne the boundaries of \\\\ufb01ve objects (the two\\\\nobjects in Fig. 5, the paper clip holder and pocket knife in Fig. 6,\\\\nand the outer boundary of the spinal vertebrae in Fig. 7)....\",\"5. Snakes and Intelligent Scissors both require interaction as part of\\\\nthe boundary segmentation process.\\\\nDif ferences (compare with corresponding similarities above):\\\\n1. The laplacian zero-crossing binary cost feature seems to have not\\\\nbeen used previously in active contours models 1  (or DP bound-\\\\nary tracking methods for that matter).\\\\n2. The active contour smoothing term is internal (i.e., based on the\\\\ncontour\\\\u2019s point positions) whereas the smoothing term for live-\\\\nwire boundaries is computed...\",\"pointer map after expanding the seed point (with a cumulative cost\\\\nof zero).  Notice how the diagonal local costs have been scaled by\\\\nEuclidean distance (consistent with the gradient magnitude cost\\\\nfeature described previously).  Though complicating the example,\\\\nweighing by Euclidean distance is necessary to demonstrate that the\\\\ncumulative costs to points currently on the active list can change if\\\\neven lower cumulative costs are computed from as yet unexpanded\\\\nneighbors.  This is demonstrated in...\",\"new seed point while effectively \\\\u201ctieing off\\\\u201d the boundary com-\\\\nputed up to the new seed point.\\\\nNote again that optimal paths are computed from the seed point\\\\nto all  points in the image (since the 2-D DP graph search produces\\\\na minimum cost spanning tree of the image [6]).  Thus, by selecting\\\\na free point with the mouse cursor, the interactive live-wire tool is\\\\nsimply selecting an optimal boundary segment from a large collec-\\\\ntion of optimal paths.\\\\nSince each pixel (or free point) de\\\\ufb01nes only o...\",\"Hand Traced\\\\nPercentage (of Pixels \\\\u2264  Distance from \\\\u201cIdeal\\\\u201d Boundary)\\\\nAccuracy\\\\ncost map.  For color images, we maximize feature values over the\\\\nthree color bands rather than averaging.\\\\nPreviously, dynamic programming approaches to boundary\\\\ndetection were typically computationally expensive.  However, by\\\\nformulating DP as a graph search and restricting the local costs to\\\\ninteger values within a range, the 2-D DP algorithm can take advan-\\\\ntage of an O(N ) sort for N  points.  As mentioned, adding p...\",\"Princeton, NJ: Princeton University Press, 1962.\\\\n[4]  Y . P . Chien and K. S. Fu, \\\\u201cA Decision Function Method for\\\\nBoundary Detection,\\\\u201d Computer Graphics and Image Pro-\\\\ncessing , vol. 3, no. 2, pp. 125-140, June 1974.\\\\n[5] D. Daneels, et al., \\\\u201cInteractive Outlining: An Improved\\\\nApproach Using Active Contours,\\\\u201d in SPIE Proceedings of\\\\nStorage and Retrieval for Image and V ideo Databases , vol\\\\n1908, pp. 226-233, Feb. 1993.\\\\n[6] E. W. Dijkstra, \\\\u201cA Note on Two Problems in Connexion with\\\\nGraphs,\\\\u201d Numeris...\",\"tions, use an interactively selected seed point to \\\\u201cgrow\\\\u201d a region by\\\\nadding adjacent neighboring pixels.  Since this type of region grow-\\\\ning does not provide interactive visual feedback, resulting region\\\\nboundaries must usually be edited or modi\\\\ufb01ed.\\\\nOther popular boundary de\\\\ufb01nition methods use active contours\\\\nor snakes[1, 5, 8, 15] to improve a manually entered rough approx-\\\\nimation.  After being initialized with a rough boundary approxima-\\\\ntion, snakes iteratively adjust the boundary points i...\",\"(1)\\\\nwhere each \\\\u03c9  is the weight of the corresponding feature function.\\\\n(Empirically, weights of \\\\u03c9 Z  = 0.43, \\\\u03c9 D  = 0.43, and \\\\u03c9 G  = 0.14 seem\\\\nto work well in a wide range of images.)\\\\nThe laplacian zero-crossing is a binary edge feature used for edge\\\\nlocalization [7, 9]. Convolution of an image with a laplacian kernel\\\\napproximates the 2nd  partial derivative of the image.  The laplacian\\\\nimage zero-crossing corresponds to points of maximal (or minimal)\\\\ngradient magnitude.  Thus, laplacian zero-cr...\",\"plexity to be extracted.  Second, the edge features used here are\\\\nmore robust and comprehensive than previous implementations: we\\\\nmaximize over different gradient kernels sizes to encompass the\\\\nvarious edge types and scales while simultaneously attempting to\\\\nbalance edge detail with noise suppression [7], and we use the lapla-\\\\ncian zero-crossing for boundary localization and \\\\ufb01ne detail live-\\\\nwire \\\\u201csnapping\\\\u201d.  Third, the discrete, bounded nature of the local\\\\nedge costs permit the use of a special...\",\"times (for a trained user) for each displayed object boundary are\\\\ngiven in the caption of each respective \\\\ufb01gure.\\\\nFigure 8 graphically compares the live-wire boundary de\\\\ufb01nition\\\\ntimes and boundary accuracy with manual tracing.  These results\\\\nshow the average time and accuracy from a study where 8 untrained\\\\nusers1  were asked to de\\\\ufb01ne the boundaries of \\\\ufb01ve objects (the two\\\\nobjects in Fig. 5, the paper clip holder and pocket knife in Fig. 6,\\\\nand the outer boundary of the spinal vertebrae in Fig. 7)....\",\"ef\\\\ufb01ciency.\\\\n1. Similar in concept to an optical mask used in motion picture special effects.\\\\nlution of the image.  This may produce jaggies along object bound-\\\\naries in a resulting composition.  However, subpixel accuracy can\\\\nbe obtained by exploiting the signed output of the laplacian opera-\\\\ntor.  That is, the position of the object edge can be estimated to sub-\\\\npixel accuracy by using a (linearly) weighted combination of the\\\\nlaplacian pixel values on either side of the zero-crossings.\\\\nSince the...\"]],[\"color\",[\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\",\"#0a980a\",\"#0a980a\",\"#ba0e0e\",\"#ba0e0e\",\"#ba0e0e\"]]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1096\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1097\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1092\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.7},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.7}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1093\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1094\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"field\",\"field\":\"color\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"line_width\":{\"type\":\"value\",\"value\":2},\"fill_color\":{\"type\":\"field\",\"field\":\"color\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"field\",\"field\":\"color\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}},{\"type\":\"object\",\"name\":\"GlyphRenderer\",\"id\":\"p1110\",\"attributes\":{\"data_source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1101\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1102\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1103\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"V9u6QZrRr0EMq7dBqpq4QSedtkFcCqdBUaqqQZuiukEyG75B/1+zQdrZsUFTTrdBhGq0QVMjukET07hBIOS+QdCssEGuoKxBBTetQfODp0EPk6RBpPujQXy1o0HUM6BBU8ChQRmyp0FJyrNBCF+dQSE1nkGRNapB0M2rQRI9rEHJ4a1BoGCpQSi1rEGUdaNBxfunQf7zskE9abZB1qu/QYtMwEFgZcVBKR/GQWjyv0FZk7FBTDytQa3Es0E4ErJBXTy3QYCdvkHO48BBIpe4QQ==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"zc59wA87e8CuF2jANFyXwBGwgMD1l0zAUwVRwO9oUcCysCHAD4NWwL2wmL8iV7a/i8sgvRXLsr65WIa/Y9Qpv10TF8CD5s+/1D8tvwNT3r4NIdC//Pp5v8DCH8BZP0TAqAwNwBnmHsCzJNO+swofwC7dEcCeWPK/BaLQPfNKi7/C3ay+Z7SNwA2XaMBXVIPALaJ7wOlTmMBWIabAtFeQwJWOgMDMBGbAfe4ywHV6WcCUBEPAob0uwFiF7b8HEYnAGYsTwM0F3b8FWu+/x+0EwA==\"},\"shape\":[52],\"dtype\":\"float32\",\"order\":\"little\"}]]}}},\"view\":{\"type\":\"object\",\"name\":\"CDSView\",\"id\":\"p1111\",\"attributes\":{\"filter\":{\"type\":\"object\",\"name\":\"AllIndices\",\"id\":\"p1112\"}}},\"glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1107\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}},\"nonselection_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1108\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.1},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.1},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.1}}},\"muted_glyph\":{\"type\":\"object\",\"name\":\"Scatter\",\"id\":\"p1109\",\"attributes\":{\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"size\":{\"type\":\"value\",\"value\":6},\"line_color\":{\"type\":\"value\",\"value\":\"grey\"},\"line_alpha\":{\"type\":\"value\",\"value\":0.2},\"fill_color\":{\"type\":\"value\",\"value\":\"grey\"},\"fill_alpha\":{\"type\":\"value\",\"value\":0.2},\"hatch_color\":{\"type\":\"value\",\"value\":\"grey\"},\"hatch_alpha\":{\"type\":\"value\",\"value\":0.2}}}}}],\"toolbar\":{\"type\":\"object\",\"name\":\"Toolbar\",\"id\":\"p1064\",\"attributes\":{\"logo\":\"grey\",\"tools\":[{\"type\":\"object\",\"name\":\"PanTool\",\"id\":\"p1079\"},{\"type\":\"object\",\"name\":\"WheelZoomTool\",\"id\":\"p1080\",\"attributes\":{\"renderers\":\"auto\"}},{\"type\":\"object\",\"name\":\"BoxZoomTool\",\"id\":\"p1081\",\"attributes\":{\"overlay\":{\"type\":\"object\",\"name\":\"BoxAnnotation\",\"id\":\"p1082\",\"attributes\":{\"syncable\":false,\"level\":\"overlay\",\"visible\":false,\"left\":{\"type\":\"number\",\"value\":\"nan\"},\"right\":{\"type\":\"number\",\"value\":\"nan\"},\"top\":{\"type\":\"number\",\"value\":\"nan\"},\"bottom\":{\"type\":\"number\",\"value\":\"nan\"},\"left_units\":\"canvas\",\"right_units\":\"canvas\",\"top_units\":\"canvas\",\"bottom_units\":\"canvas\",\"line_color\":\"black\",\"line_alpha\":1.0,\"line_width\":2,\"line_dash\":[4,4],\"fill_color\":\"lightgrey\",\"fill_alpha\":0.5}}}},{\"type\":\"object\",\"name\":\"ResetTool\",\"id\":\"p1087\"},{\"type\":\"object\",\"name\":\"SaveTool\",\"id\":\"p1088\"},{\"type\":\"object\",\"name\":\"HoverTool\",\"id\":\"p1055\",\"attributes\":{\"renderers\":[{\"id\":\"p1095\"}],\"tooltips\":\"\\\\n    &lt;div style=\\\\\"width:400px;\\\\\"&gt;\\\\n    &lt;b&gt;Document id:&lt;/b&gt; @id &lt;br&gt;\\\\n    &lt;b&gt;Topic:&lt;/b&gt; @topic &lt;br&gt;\\\\n    &lt;b&gt;Question:&lt;/b&gt; @questions &lt;br&gt;\\\\n    &lt;b&gt;agent Answer:&lt;/b&gt; @agent_answer &lt;br&gt;\\\\n    &lt;b&gt;Reference Answer:&lt;/b&gt; @reference_answer &lt;br&gt;\\\\n    &lt;b&gt;Correctness:&lt;/b&gt; @correctness &lt;br&gt;\\\\n    &lt;b&gt;Content:&lt;/b&gt; @content\\\\n    &lt;/div&gt;\\\\n    \"}}]}},\"left\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1074\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1075\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1076\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1077\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"below\":[{\"type\":\"object\",\"name\":\"LinearAxis\",\"id\":\"p1069\",\"attributes\":{\"ticker\":{\"type\":\"object\",\"name\":\"BasicTicker\",\"id\":\"p1070\",\"attributes\":{\"mantissas\":[1,2,5]}},\"formatter\":{\"type\":\"object\",\"name\":\"BasicTickFormatter\",\"id\":\"p1071\"},\"axis_label_standoff\":10,\"axis_label_text_color\":\"#E0E0E0\",\"axis_label_text_font\":\"Helvetica\",\"axis_label_text_font_size\":\"1.25em\",\"axis_label_text_font_style\":\"normal\",\"major_label_policy\":{\"type\":\"object\",\"name\":\"AllLabels\",\"id\":\"p1072\"},\"major_label_text_color\":\"#E0E0E0\",\"major_label_text_font\":\"Helvetica\",\"major_label_text_font_size\":\"1.025em\",\"axis_line_color\":\"#E0E0E0\",\"axis_line_alpha\":0,\"major_tick_line_color\":\"#E0E0E0\",\"major_tick_line_alpha\":0,\"minor_tick_line_color\":\"#E0E0E0\",\"minor_tick_line_alpha\":0}}],\"center\":[{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1073\",\"attributes\":{\"axis\":{\"id\":\"p1069\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Grid\",\"id\":\"p1078\",\"attributes\":{\"dimension\":1,\"axis\":{\"id\":\"p1074\"},\"grid_line_color\":\"white\",\"grid_line_alpha\":0.25}},{\"type\":\"object\",\"name\":\"Legend\",\"id\":\"p1098\",\"attributes\":{\"title\":\"Question Correctness\",\"title_text_color\":\"#B1B1B1\",\"title_text_font_style\":\"bold\",\"border_line_alpha\":0,\"background_fill_color\":\"#111516\",\"background_fill_alpha\":0.5,\"label_text_color\":\"#E0E0E0\",\"label_text_font\":\"Helvetica\",\"label_text_font_size\":\"1.025em\",\"label_standoff\":8,\"glyph_width\":15,\"spacing\":8,\"items\":[{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1099\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"False\"},\"renderers\":[{\"id\":\"p1095\"}],\"index\":0}},{\"type\":\"object\",\"name\":\"LegendItem\",\"id\":\"p1100\",\"attributes\":{\"label\":{\"type\":\"value\",\"value\":\"True\"},\"renderers\":[{\"id\":\"p1095\"}],\"index\":1}}]}},{\"type\":\"object\",\"name\":\"LabelSet\",\"id\":\"p1116\",\"attributes\":{\"level\":\"glyph\",\"source\":{\"type\":\"object\",\"name\":\"ColumnDataSource\",\"id\":\"p1113\",\"attributes\":{\"selected\":{\"type\":\"object\",\"name\":\"Selection\",\"id\":\"p1114\",\"attributes\":{\"indices\":[],\"line_indices\":[]}},\"selection_policy\":{\"type\":\"object\",\"name\":\"UnionRenderers\",\"id\":\"p1115\"},\"data\":{\"type\":\"map\",\"entries\":[[\"x\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"PJe0QV58p0E=\"},\"shape\":[2],\"dtype\":\"float32\",\"order\":\"little\"}],[\"y\",{\"type\":\"ndarray\",\"array\":{\"type\":\"bytes\",\"data\":\"s7FawApX0r8=\"},\"shape\":[2],\"dtype\":\"float32\",\"order\":\"little\"}],[\"topic\",[\"Image Composition Techniques\",\"Image Segmentation Techniques\"]]]}}},\"x\":{\"type\":\"field\",\"field\":\"x\"},\"y\":{\"type\":\"field\",\"field\":\"y\"},\"text\":{\"type\":\"field\",\"field\":\"topic\"},\"text_color\":{\"type\":\"value\",\"value\":\"#B1B1B1\"},\"text_font_size\":{\"type\":\"value\",\"value\":\"12pt\"},\"text_font_style\":{\"type\":\"value\",\"value\":\"bold\"},\"text_align\":{\"type\":\"value\",\"value\":\"center\"}}}],\"background_fill_color\":\"#14191B\",\"border_fill_color\":\"#15191C\"}}}}],\"tabs_location\":\"below\"}}]}}';\n",
       "        const render_items = [{\"docid\":\"f58e476c-bf07-4d6a-842b-80cfc5e4570f\",\"roots\":{\"p1121\":\"e983b632-9ea2-4985-a150-c459a7a7c5f2\"},\"root_ids\":[\"p1121\"]}];\n",
       "        root.Bokeh.embed.embed_items(docs_json, render_items);\n",
       "        }\n",
       "        if (root.Bokeh !== undefined) {\n",
       "          embed_document(root);\n",
       "        } else {\n",
       "          let attempts = 0;\n",
       "          const timer = setInterval(function(root) {\n",
       "            if (root.Bokeh !== undefined) {\n",
       "              clearInterval(timer);\n",
       "              embed_document(root);\n",
       "            } else {\n",
       "              attempts++;\n",
       "              if (attempts > 100) {\n",
       "                clearInterval(timer);\n",
       "                console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "              }\n",
       "            }\n",
       "          }, 10, root)\n",
       "        }\n",
       "      })(window);\n",
       "    });\n",
       "  };\n",
       "  if (document.readyState != \"loading\") fn();\n",
       "  else document.addEventListener(\"DOMContentLoaded\", fn);\n",
       "})();\n",
       "    </script>\n",
       "\n",
       "                <div id=\"e983b632-9ea2-4985-a150-c459a7a7c5f2\" data-root-id=\"p1121\" style=\"display: contents;\"></div>\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        <div class=\"section-container\">\n",
       "            <div class=\"section-card\">\n",
       "\n",
       "                <div class=\"section-title\">SELECTED METRICS</div>\n",
       "\n",
       "                \n",
       "\n",
       "            </div>\n",
       "        </div>\n",
       "\n",
       "        \n",
       "    </div>\n",
       "</div>\n",
       "\n",
       "\n",
       "\n",
       "<script type=\"text/javascript\">\n",
       "    function opentab(evt, name) {\n",
       "    // Declare all variables\n",
       "    let i, tabcontent, tablinks;\n",
       "\n",
       "    // Get all elements with class=\"tabcontent\" and hide them\n",
       "    tabcontent = document.getElementsByClassName(\"tabcontent\");\n",
       "    for (i = 0; i < tabcontent.length; i++) {\n",
       "        tabcontent[i].style.display = \"none\";\n",
       "    }\n",
       "\n",
       "    // Get all elements with class=\"tablinks\" and remove the class \"active\"\n",
       "    tablinks = document.getElementsByClassName(\"tablinks\");\n",
       "    for (i = 0; i < tablinks.length; i++) {\n",
       "        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n",
       "    }\n",
       "\n",
       "    // Show the current tab, and add an \"active\" class to the button that opened the tab\n",
       "    document.getElementById(name).style.display = \"block\";\n",
       "    evt.currentTarget.className += \" active\";\n",
       "}\n",
       "</script>\n"
      ],
      "text/plain": [
       "<giskard.rag.report.RAGReport at 0x1efdf0c83b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report.to_html(\"report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the correctness results organized by question type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>correctness</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>question_type</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>complex</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>conversational</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distracting element</th>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>double</th>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>situational</th>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     correctness\n",
       "question_type                   \n",
       "complex                      1.0\n",
       "conversational               0.0\n",
       "distracting element          0.3\n",
       "double                       0.5\n",
       "simple                       0.8\n",
       "situational                  0.7"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.correctness_by_question_type()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also display the specific failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>reference_answer</th>\n",
       "      <th>reference_context</th>\n",
       "      <th>conversation_history</th>\n",
       "      <th>metadata</th>\n",
       "      <th>agent_answer</th>\n",
       "      <th>correctness</th>\n",
       "      <th>correctness_reason</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>336e5430-3023-42cf-a975-4b9da8b53c25</th>\n",
       "      <td>What are the limitations of the training based...</td>\n",
       "      <td>Training is most effective for those objects w...</td>\n",
       "      <td>Document 32: precomputed feature maps along th...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'simple', 'seed_document_id'...</td>\n",
       "      <td>The limitations of the training based on learn...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent's answer did not mention that traini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2590f4db-5a36-4e3d-86dc-10f1e59ae602</th>\n",
       "      <td>What does preprocessing require for color imag...</td>\n",
       "      <td>Preprocessing requires 36 convolutions for col...</td>\n",
       "      <td>Document 44: times (for a trained user) for ea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'simple', 'seed_document_id'...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that it doesn't know the answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e2ebb642-6503-4a09-a071-2574c81f2089</th>\n",
       "      <td>What was the objective of the study conducted ...</td>\n",
       "      <td>The study with eight untrained users was condu...</td>\n",
       "      <td>Document 44: times (for a trained user) for ea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>The objective of the study conducted with eigh...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that the objective of the stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a5b51156-fde2-4faf-86f3-9b983162591c</th>\n",
       "      <td>What is the copyright year and fee for the mat...</td>\n",
       "      <td>The copyright year is 1995 and the fee is $3.50.</td>\n",
       "      <td>Document 8: permission and/or a fee. \\n©1995  ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that it doesn't know the copy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4a9f105f-5bd2-4feb-b123-0f54187c3708</th>\n",
       "      <td>In the context of the Live-Wire 2-D DP graph s...</td>\n",
       "      <td>The gradient direction adds a smoothness const...</td>\n",
       "      <td>Document 13: imum gradient at unity,fG ( q ) i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>The gradient direction function ensures smooth...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent's answer was not specific enough. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c66dc718-27b0-4289-88b5-f0e0fae21b25</th>\n",
       "      <td>Considering the context of image composition, ...</td>\n",
       "      <td>The five key methods are: (1) making use of th...</td>\n",
       "      <td>Document 48: (1) making use of the weighted ze...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that it doesn't know the answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d097df72-9b9b-44ed-969c-55123f98df42</th>\n",
       "      <td>In the context of image composition using Inte...</td>\n",
       "      <td>Subpixel accuracy can be obtained by exploitin...</td>\n",
       "      <td>Document 39: efﬁciency.\\n1. Similar in concept...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>In the context of image composition using Inte...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided an answer about using live-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6a48b2d0-45a6-435b-a067-2180ea37150b</th>\n",
       "      <td>Considering the wavefront of active points tha...</td>\n",
       "      <td>Intelligent Scissors require less time and eff...</td>\n",
       "      <td>Document 1: Abstract\\nWe present a new, intera...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>The key advantage of Intelligent Scissors over...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that the advantage of Intelli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9db4f3f0-a35f-45bf-834d-0741d1ea4a83</th>\n",
       "      <td>Can you identify the authors of the Intelligen...</td>\n",
       "      <td>The authors of the Intelligent Scissors for Im...</td>\n",
       "      <td>Document 7: of freedom within a window about t...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'distracting element', 'seed...</td>\n",
       "      <td>Yes, the authors of the Intelligent Scissors f...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent incorrectly stated that the authors ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8bbd7515-3bf2-4415-9ea1-1ac781df82d9</th>\n",
       "      <td>As a graphic design student, I'm trying to und...</td>\n",
       "      <td>Subpixel accuracy can be obtained by exploitin...</td>\n",
       "      <td>Document 39: efﬁciency.\\n1. Similar in concept...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that it doesn't know, but sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8cac29ad-e168-4dfd-b1b0-bed9750c879b</th>\n",
       "      <td>As a graphic design student working on a class...</td>\n",
       "      <td>The copyright year is 1995 and the fee is $3.50.</td>\n",
       "      <td>Document 8: permission and/or a fee. \\n©1995  ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that it doesn't know the answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dc186be3-28b5-4f05-acb2-d2d068611ac9</th>\n",
       "      <td>As a doctoral student in biomedical engineerin...</td>\n",
       "      <td>The Intelligent Scissors are applied in a vari...</td>\n",
       "      <td>Document 43: d 0 ' and n ' are the ﬁt paramete...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'situational', 'seed_documen...</td>\n",
       "      <td>I don't know.</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that it doesn't know the answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cfba317e-cc08-413c-be65-50f2d38315f6</th>\n",
       "      <td>What is the process of equivalencing of spatia...</td>\n",
       "      <td>Equivalencing of spatial frequencies is perfor...</td>\n",
       "      <td>Document 42: where d 0  is the distance of the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>The process of equivalencing of spatial freque...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent's explanation of the process of equi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d289c85a-dbd4-4dd2-89c9-6f7e412acabb</th>\n",
       "      <td>What does Figure 8 in the study represent and ...</td>\n",
       "      <td>Figure 8 graphically compares the live-wire bo...</td>\n",
       "      <td>Document 44: times (for a trained user) for ea...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>Figure 8 in the study represents a graphical c...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent correctly described what Figure 8 re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b9fc0c20-98b8-4069-85be-f18fc765b3bc</th>\n",
       "      <td>What is the purpose of placing seed points in ...</td>\n",
       "      <td>Seed points are placed to ensure a closed obje...</td>\n",
       "      <td>Document 23: new seed point while effectively ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>The purpose of placing seed points in the inte...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent's answer was partially correct but m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e7b23101-1dc1-493d-aabe-9ee4e1cf71d4</th>\n",
       "      <td>What is the computational complexity of the 2-...</td>\n",
       "      <td>The computational complexity for N image pixel...</td>\n",
       "      <td>Document 46: Hand Traced\\nPercentage (of Pixel...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>The computational complexity of the 2-D DP alg...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that in the worst case scenar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613a9c4-4738-44d6-bbd0-493806d7c953</th>\n",
       "      <td>What is the title of the paper written by Y.P....</td>\n",
       "      <td>The title of the paper written by Y.P. Chien a...</td>\n",
       "      <td>Document 49: Princeton, NJ: Princeton Universi...</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'question_type': 'double', 'original_question...</td>\n",
       "      <td>The title of the paper written by Y.P. Chien a...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that the topic of the paper p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5df524ef-79d6-4748-a0a4-2783f32ea2cd</th>\n",
       "      <td>What are its features?</td>\n",
       "      <td>The Live-Wire 2-D dynamic programming graph se...</td>\n",
       "      <td>Document 16: plexity to be extracted.  Second,...</td>\n",
       "      <td>[{'role': 'user', 'content': 'I am interested ...</td>\n",
       "      <td>{'question_type': 'conversational', 'seed_docu...</td>\n",
       "      <td>The features mentioned in the context are:\\n1....</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a list of features that are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479</th>\n",
       "      <td>What does it require?</td>\n",
       "      <td>Preprocessing for color images in the Intellig...</td>\n",
       "      <td>Document 44: times (for a trained user) for ea...</td>\n",
       "      <td>[{'role': 'user', 'content': 'I'm curious abou...</td>\n",
       "      <td>{'question_type': 'conversational', 'seed_docu...</td>\n",
       "      <td>It requires blending an object from one image ...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent stated that preprocessing for color ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2aaf118b-bff2-4483-b10f-7ad9be49cbfe</th>\n",
       "      <td>What is one approach to reduce them?</td>\n",
       "      <td>One approach to reduce 'jaggies' is by exploit...</td>\n",
       "      <td>Document 39: efﬁciency.\\n1. Similar in concept...</td>\n",
       "      <td>[{'role': 'user', 'content': 'I'm dealing with...</td>\n",
       "      <td>{'question_type': 'conversational', 'seed_docu...</td>\n",
       "      <td>One approach to reduce jaggies is to perform e...</td>\n",
       "      <td>False</td>\n",
       "      <td>The agent provided a correct approach to reduc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                               question  \\\n",
       "id                                                                                        \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25  What are the limitations of the training based...   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602  What does preprocessing require for color imag...   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089  What was the objective of the study conducted ...   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c  What is the copyright year and fee for the mat...   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708  In the context of the Live-Wire 2-D DP graph s...   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25  Considering the context of image composition, ...   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42  In the context of image composition using Inte...   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b  Considering the wavefront of active points tha...   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83  Can you identify the authors of the Intelligen...   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9  As a graphic design student, I'm trying to und...   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b  As a graphic design student working on a class...   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9  As a doctoral student in biomedical engineerin...   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6  What is the process of equivalencing of spatia...   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb  What does Figure 8 in the study represent and ...   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc  What is the purpose of placing seed points in ...   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4  What is the computational complexity of the 2-...   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953  What is the title of the paper written by Y.P....   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd                             What are its features?   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479                              What does it require?   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe               What is one approach to reduce them?   \n",
       "\n",
       "                                                                       reference_answer  \\\n",
       "id                                                                                        \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25  Training is most effective for those objects w...   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602  Preprocessing requires 36 convolutions for col...   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089  The study with eight untrained users was condu...   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c   The copyright year is 1995 and the fee is $3.50.   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708  The gradient direction adds a smoothness const...   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25  The five key methods are: (1) making use of th...   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42  Subpixel accuracy can be obtained by exploitin...   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b  Intelligent Scissors require less time and eff...   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83  The authors of the Intelligent Scissors for Im...   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9  Subpixel accuracy can be obtained by exploitin...   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b   The copyright year is 1995 and the fee is $3.50.   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9  The Intelligent Scissors are applied in a vari...   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6  Equivalencing of spatial frequencies is perfor...   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb  Figure 8 graphically compares the live-wire bo...   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc  Seed points are placed to ensure a closed obje...   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4  The computational complexity for N image pixel...   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953  The title of the paper written by Y.P. Chien a...   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd  The Live-Wire 2-D dynamic programming graph se...   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479  Preprocessing for color images in the Intellig...   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe  One approach to reduce 'jaggies' is by exploit...   \n",
       "\n",
       "                                                                      reference_context  \\\n",
       "id                                                                                        \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25  Document 32: precomputed feature maps along th...   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602  Document 44: times (for a trained user) for ea...   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089  Document 44: times (for a trained user) for ea...   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c  Document 8: permission and/or a fee. \\n©1995  ...   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708  Document 13: imum gradient at unity,fG ( q ) i...   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25  Document 48: (1) making use of the weighted ze...   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42  Document 39: efﬁciency.\\n1. Similar in concept...   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b  Document 1: Abstract\\nWe present a new, intera...   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83  Document 7: of freedom within a window about t...   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9  Document 39: efﬁciency.\\n1. Similar in concept...   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b  Document 8: permission and/or a fee. \\n©1995  ...   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9  Document 43: d 0 ' and n ' are the ﬁt paramete...   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6  Document 42: where d 0  is the distance of the...   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb  Document 44: times (for a trained user) for ea...   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc  Document 23: new seed point while effectively ...   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4  Document 46: Hand Traced\\nPercentage (of Pixel...   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953  Document 49: Princeton, NJ: Princeton Universi...   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd  Document 16: plexity to be extracted.  Second,...   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479  Document 44: times (for a trained user) for ea...   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe  Document 39: efﬁciency.\\n1. Similar in concept...   \n",
       "\n",
       "                                                                   conversation_history  \\\n",
       "id                                                                                        \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25                                                 []   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602                                                 []   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089                                                 []   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c                                                 []   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708                                                 []   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25                                                 []   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42                                                 []   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b                                                 []   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83                                                 []   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9                                                 []   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b                                                 []   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9                                                 []   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6                                                 []   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb                                                 []   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc                                                 []   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4                                                 []   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953                                                 []   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd  [{'role': 'user', 'content': 'I am interested ...   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479  [{'role': 'user', 'content': 'I'm curious abou...   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe  [{'role': 'user', 'content': 'I'm dealing with...   \n",
       "\n",
       "                                                                               metadata  \\\n",
       "id                                                                                        \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25  {'question_type': 'simple', 'seed_document_id'...   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602  {'question_type': 'simple', 'seed_document_id'...   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089  {'question_type': 'distracting element', 'seed...   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c  {'question_type': 'distracting element', 'seed...   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708  {'question_type': 'distracting element', 'seed...   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25  {'question_type': 'distracting element', 'seed...   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42  {'question_type': 'distracting element', 'seed...   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b  {'question_type': 'distracting element', 'seed...   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83  {'question_type': 'distracting element', 'seed...   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9  {'question_type': 'situational', 'seed_documen...   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b  {'question_type': 'situational', 'seed_documen...   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9  {'question_type': 'situational', 'seed_documen...   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6  {'question_type': 'double', 'original_question...   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb  {'question_type': 'double', 'original_question...   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc  {'question_type': 'double', 'original_question...   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4  {'question_type': 'double', 'original_question...   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953  {'question_type': 'double', 'original_question...   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd  {'question_type': 'conversational', 'seed_docu...   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479  {'question_type': 'conversational', 'seed_docu...   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe  {'question_type': 'conversational', 'seed_docu...   \n",
       "\n",
       "                                                                           agent_answer  \\\n",
       "id                                                                                        \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25  The limitations of the training based on learn...   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602                                      I don't know.   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089  The objective of the study conducted with eigh...   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c                                      I don't know.   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708  The gradient direction function ensures smooth...   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25                                      I don't know.   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42  In the context of image composition using Inte...   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b  The key advantage of Intelligent Scissors over...   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83  Yes, the authors of the Intelligent Scissors f...   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9                                      I don't know.   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b                                      I don't know.   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9                                      I don't know.   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6  The process of equivalencing of spatial freque...   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb  Figure 8 in the study represents a graphical c...   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc  The purpose of placing seed points in the inte...   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4  The computational complexity of the 2-D DP alg...   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953  The title of the paper written by Y.P. Chien a...   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd  The features mentioned in the context are:\\n1....   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479  It requires blending an object from one image ...   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe  One approach to reduce jaggies is to perform e...   \n",
       "\n",
       "                                      correctness  \\\n",
       "id                                                  \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25        False   \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602        False   \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089        False   \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c        False   \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708        False   \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25        False   \n",
       "d097df72-9b9b-44ed-969c-55123f98df42        False   \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b        False   \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83        False   \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9        False   \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b        False   \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9        False   \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6        False   \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb        False   \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc        False   \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4        False   \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953        False   \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd        False   \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479        False   \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe        False   \n",
       "\n",
       "                                                                     correctness_reason  \n",
       "id                                                                                       \n",
       "336e5430-3023-42cf-a975-4b9da8b53c25  The agent's answer did not mention that traini...  \n",
       "2590f4db-5a36-4e3d-86dc-10f1e59ae602  The agent stated that it doesn't know the answ...  \n",
       "e2ebb642-6503-4a09-a071-2574c81f2089  The agent stated that the objective of the stu...  \n",
       "a5b51156-fde2-4faf-86f3-9b983162591c  The agent stated that it doesn't know the copy...  \n",
       "4a9f105f-5bd2-4feb-b123-0f54187c3708  The agent's answer was not specific enough. Th...  \n",
       "c66dc718-27b0-4289-88b5-f0e0fae21b25  The agent stated that it doesn't know the answ...  \n",
       "d097df72-9b9b-44ed-969c-55123f98df42  The agent provided an answer about using live-...  \n",
       "6a48b2d0-45a6-435b-a067-2180ea37150b  The agent stated that the advantage of Intelli...  \n",
       "9db4f3f0-a35f-45bf-834d-0741d1ea4a83  The agent incorrectly stated that the authors ...  \n",
       "8bbd7515-3bf2-4415-9ea1-1ac781df82d9  The agent stated that it doesn't know, but sho...  \n",
       "8cac29ad-e168-4dfd-b1b0-bed9750c879b  The agent stated that it doesn't know the answ...  \n",
       "dc186be3-28b5-4f05-acb2-d2d068611ac9  The agent stated that it doesn't know the answ...  \n",
       "cfba317e-cc08-413c-be65-50f2d38315f6  The agent's explanation of the process of equi...  \n",
       "d289c85a-dbd4-4dd2-89c9-6f7e412acabb  The agent correctly described what Figure 8 re...  \n",
       "b9fc0c20-98b8-4069-85be-f18fc765b3bc  The agent's answer was partially correct but m...  \n",
       "e7b23101-1dc1-493d-aabe-9ee4e1cf71d4  The agent stated that in the worst case scenar...  \n",
       "7613a9c4-4738-44d6-bbd0-493806d7c953  The agent stated that the topic of the paper p...  \n",
       "5df524ef-79d6-4748-a0a4-2783f32ea2cd  The agent provided a list of features that are...  \n",
       "2bfd4ae4-6c14-4e4b-bcf7-0bd98b08d479  The agent stated that preprocessing for color ...  \n",
       "2aaf118b-bff2-4483-b10f-7ad9be49cbfe  The agent provided a correct approach to reduc...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report.get_failures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Test Suite\n",
    "\n",
    "We can create a test suite and use it to compare different models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the test set from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from giskard.rag import QATestset\n",
    "\n",
    "testset = QATestset.load(\"RAG-test-set.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Test Suite from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_suite = testset.to_test_suite(\"Machine Learning School Test Suite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a function that takes a DataFrame of questions, invokes the chain with each question, and returns the answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import giskard\n",
    "\n",
    "\n",
    "def batch_prediction_fn(df: pd.DataFrame):\n",
    "    return chain.batch([{\"question\": q} for q in df[\"question\"].values])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a Giskard Model object to run our test suite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-17 14:09:24,203 pid:3908 MainThread giskard.models.automodel INFO     Your 'prediction_function' is successfully wrapped by Giskard's 'PredictionFunctionModel' wrapper class.\n"
     ]
    }
   ],
   "source": [
    "giskard_model = giskard.Model(\n",
    "    model=batch_prediction_fn,\n",
    "    model_type=\"text_generation\",\n",
    "    name=\"Machine Learning School Question and Answer Model\",\n",
    "    description=\"This model answers questions about the Machine Learning School website.\",\n",
    "    feature_names=[\"question\"], \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now run the test suite using the model we created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-11-17 14:09:28,728 pid:3908 MainThread giskard.datasets.base INFO     Casting dataframe columns from {'question': 'object'} to {'question': 'object'}\n",
      "2024-11-17 14:09:33,388 pid:3908 MainThread giskard.utils.logging_utils INFO     Predicted dataset with shape (53, 5) executed in 0:00:04.659229\n",
      "2024-11-17 14:11:00,798 pid:3908 MainThread root         ERROR    An error happened during test execution for test: TestsetCorrectnessTest\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\core\\suite.py\", line 522, in run\n",
      "    result = test_partial.giskard_test(**test_params).execute()\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\registry\\giskard_test.py\", line 195, in execute\n",
      "    return configured_validate_arguments(self.test_fn)(*self.args, **self.kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\validate_call_decorator.py\", line 60, in wrapper_function\n",
      "    return validate_call_wrapper(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pydantic\\_internal\\_validate_call.py\", line 96, in __call__\n",
      "    res = self.__pydantic_validator__.validate_python(pydantic_core.ArgsKwargs(args, kwargs))\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\zaodu\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\giskard\\testing\\tests\\llm\\correctness.py\", line 50, in test_llm_correctness\n",
      "    details=eval_result.details,\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: 'EvaluationResult' object has no attribute 'details'\n",
      "2024-11-17 14:11:00,798 pid:3908 MainThread giskard.core.suite INFO     Executed test suite 'Machine Learning School Test Suite'\n",
      "2024-11-17 14:11:00,798 pid:3908 MainThread giskard.core.suite INFO     result: failed\n",
      "2024-11-17 14:11:00,798 pid:3908 MainThread giskard.core.suite INFO     TestsetCorrectnessTest ({'model': <giskard.models.function.PredictionFunctionModel object at 0x000001EFDF44D040>, 'dataset': <giskard.datasets.base.Dataset object at 0x000001EFDEFE4CB0>}): {failed, metric=None}\n"
     ]
    }
   ],
   "source": [
    "test_suite_results = test_suite.run(model=giskard_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .alert {\n",
       "        padding: 16px;\n",
       "        border-radius: 4px;\n",
       "        display: flex;\n",
       "        align-items: center;\n",
       "        flex-wrap: wrap;\n",
       "        gap: 4px;\n",
       "        margin-bottom: 16px;\n",
       "    }\n",
       "\n",
       "    .test-result {\n",
       "        border-radius: 4px;\n",
       "        padding: 0 12px;\n",
       "        display: flex;\n",
       "        gap: 4px;\n",
       "        align-items: center;\n",
       "    }\n",
       "\n",
       "    .alert-error, .test-card-failed .test-result {\n",
       "        color: rgb(255, 82, 82);\n",
       "        background: rgb(243, 226, 226);\n",
       "        fill: currentcolor;\n",
       "    }\n",
       "\n",
       "    .test-card-error .test-result {\n",
       "        color: #856404;\n",
       "        background: #fff3cd;\n",
       "        fill: currentcolor;\n",
       "    }\n",
       "\n",
       "    .alert-success, .test-card-passed .test-result {\n",
       "        color: rgb(76, 175, 80);\n",
       "        background: rgb(226, 243, 226);\n",
       "        fill: currentcolor;\n",
       "    }\n",
       "\n",
       "    .alert svg {\n",
       "        height: 24px;\n",
       "        width: 24px;\n",
       "        border-radius: 50%;\n",
       "        background: rgba(0, 0, 0, 10%);\n",
       "    }\n",
       "\n",
       "    .test-result svg {\n",
       "        height: 16px;\n",
       "        width: 16px;\n",
       "    }\n",
       "\n",
       "    .card-container {\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "        gap: 16px;\n",
       "    }\n",
       "\n",
       "    .test-card {\n",
       "        border-radius: 4px;\n",
       "        border: 1px solid #e0e0e0;\n",
       "        color: rgb(98, 98, 98);\n",
       "        background: #fff;\n",
       "    }\n",
       "\n",
       "    .param {\n",
       "        display: flex;\n",
       "        flex-direction: column;\n",
       "    }\n",
       "\n",
       "    .test-card .test-name, .test-card .param-value {\n",
       "        color: #000\n",
       "    }\n",
       "\n",
       "    .test-card-row {\n",
       "        padding: 10px;\n",
       "        display: flex;\n",
       "        gap: 20px;\n",
       "        align-items: center;\n",
       "        flex-wrap: wrap;\n",
       "    }\n",
       "\n",
       "    .test-card-row:last-child {\n",
       "        border-top: 1px solid #dee2e6;\n",
       "    }\n",
       "\n",
       "    .spacer {\n",
       "        flex-grow: 1;\n",
       "    }\n",
       "\n",
       "    .test-card-failed .metric {\n",
       "        color: #b71c1c;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "<div class='dark'>\n",
       "    <div id='gsk-suite'>\n",
       "        <div class=\"alert alert-error\">\n",
       "  \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 24 24\">\n",
       "    <title>close</title>\n",
       "    <path\n",
       "      d=\"M19,6.41L17.59,5L12,10.59L6.41,5L5,6.41L10.59,12L5,17.59L6.41,19L12,13.41L17.59,19L19,17.59L13.41,12L19,6.41Z\"\n",
       "    />\n",
       "  </svg>\n",
       "  <strong>Test suite failed.</strong>\n",
       "  \n",
       "</div>\n",
       "        <div class='card-container'>\n",
       "  \n",
       "  <div\n",
       "    class='test-card test-card-error'>\n",
       "    <div class='test-card-row'>\n",
       "      <span class='test-name'>Test TestsetCorrectnessTest</span>\n",
       "      <div class='spacer'></div>\n",
       "      \n",
       "      <span class='test-result'>\n",
       "      \n",
       "        <svg xmlns='http://www.w3.org/2000/svg' viewBox='0 0 24 24'><title>alert-outline</title>\n",
       "          <path d='M12,2L1,21H23M12,6L19.53,19H4.47M11,10V14H13V10M11,16V18H13V16' />\n",
       "        </svg>\n",
       "        Error\n",
       "      \n",
       "    </span>\n",
       "    </div>\n",
       "    <div class='test-card-row'>\n",
       "      \n",
       "      <div class='param'>\n",
       "        <span>model</span>\n",
       "        <span class='param-value'>Machine Learning School Question and Answer Model</span>\n",
       "      </div>\n",
       "      \n",
       "      <div class='param'>\n",
       "        <span>dataset</span>\n",
       "        <span class='param-value'>QA Testset</span>\n",
       "      </div>\n",
       "      \n",
       "    </div>\n",
       "  </div>\n",
       "  \n",
       "</div>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<TestSuiteResult (failed)>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(test_suite_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating with Pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now integrate our test suite with Pytest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%ipytest\n",
    "\n",
    "import pytest\n",
    "from giskard.rag import QATestset\n",
    "from giskard.testing.tests.llm import test_llm_correctness\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def dataset():\n",
    "    testset = QATestset.load(\"test-set.jsonl\")\n",
    "    return testset.to_dataset()\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def model():\n",
    "    return giskard_model\n",
    "\n",
    "\n",
    "def test_chain(dataset, model):\n",
    "    test_llm_correctness(model=model, dataset=dataset, threshold=0.5).assert_()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
